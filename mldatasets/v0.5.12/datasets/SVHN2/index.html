<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/'</script><script charset="utf-8" src="../../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>SVHN format 2 · MLDatasets.jl</title><script data-outdated-warner="" src="../../assets/warner.js"></script><link href="nothing/mldatasets/stable/datasets/SVHN2/" rel="canonical"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link nav-item" href="../../../../flux/">Flux</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Building Blocks</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a><a class="nav-link nav-item" href="../../../../functors/">Functors</a></ul></div><div class="nav-mega-column"><div class="column-header">Automatic differentiation libraries</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../zygote/">Zygote</a></ul></div><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Training</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Data Wrangling</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mlutils/">MLUtils</a><a class="nav-link nav-item" href="../../../../onehotarrays/">OneHotArrays</a></ul></div><div class="nav-mega-column"><div class="column-header">Data Augmentation</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../dataaugmentation/">DataAugmentation</a></ul></div><div class="nav-mega-column"><div class="column-header">Datasets</div><ul class="column-content"><a class="nav-link active nav-item" href="../../../">MLDatasets</a></ul></div><div class="nav-mega-column"><div class="column-header">Schedulers</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../paramschedulers/dev/">ParameterSchedulers</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Models</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Computer Vision</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../metalhead/">Metalhead</a></ul></div><div class="nav-mega-column"><div class="column-header">Natural Language Processing</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../transformers/">Transformers</a></ul></div></div></div><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="MLDatasets.jl logo" src="../../assets/logo.png"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">MLDatasets.jl</a></span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Available Datasets</span><ul><li><span class="tocitem">Vision</span><ul><li><a class="tocitem" href="../MNIST/">MNIST</a></li><li><a class="tocitem" href="../FashionMNIST/">FashionMNIST</a></li><li><a class="tocitem" href="../CIFAR10/">CIFAR-10</a></li><li><a class="tocitem" href="../CIFAR100/">CIFAR-100</a></li><li class="is-active"><a class="tocitem" href="">SVHN format 2</a><ul class="internal"><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#API-Documentation"><span>API Documentation</span></a></li></ul></li></ul></li><li><span class="tocitem">Miscellaneous</span><ul><li><a class="tocitem" href="../Iris/">Iris</a></li><li><a class="tocitem" href="../BostonHousing/">Boston Housing</a></li><li><a class="tocitem" href="../Mutagenesis/">Mutagenesis</a></li></ul></li><li><span class="tocitem">Text</span><ul><li><a class="tocitem" href="../PTBLM/">PTBLM</a></li><li><a class="tocitem" href="../UD_English/">UD_English</a></li></ul></li><li><span class="tocitem">Graphs</span><ul><li><a class="tocitem" href="../CiteSeer/">CiteSeer</a></li><li><a class="tocitem" href="../Cora/">Cora</a></li><li><a class="tocitem" href="../PubMed/">PubMed</a></li><li><a class="tocitem" href="../TUDataset/">TUDataset</a></li></ul></li></ul></li><li><a class="tocitem" href="../../utils/">Utils</a></li><li><a class="tocitem" href="../../LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Available Datasets</a></li><li><a class="is-disabled">Vision</a></li><li class="is-active"><a href="">SVHN format 2</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">SVHN format 2</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaML/MLDatasets.jl/blob/master/docs/src/datasets/SVHN2.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="SVHN2"><a class="docs-heading-anchor" href="#SVHN2">The Street View House Numbers (SVHN) Dataset</a><a id="SVHN2-1"></a><a class="docs-heading-anchor-permalink" href="#SVHN2" title="Permalink"></a></h1><p>Description from the <a href="http://ufldl.stanford.edu/housenumbers/">official website</a>:</p><blockquote><p>SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.</p></blockquote><p>About Format 2 (Cropped Digits):</p><blockquote><p>All digits have been resized to a fixed resolution of 32-by-32 pixels. The original character bounding boxes are extended in the appropriate dimension to become square windows, so that resizing them to 32-by-32 pixels does not introduce aspect ratio distortions. Nevertheless this preprocessing introduces some distracting digits to the sides of the digit of interest.</p></blockquote><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>For non-commercial use only</p></div></div><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><ul><li><a href="#SVHN2">The Street View House Numbers (SVHN) Dataset</a></li><li class="no-marker"><ul><li><a href="#Contents">Contents</a></li><li><a href="#Overview">Overview</a></li><li><a href="#API-Documentation">API Documentation</a></li><li class="no-marker"><ul><li><a href="#Trainingset">Trainingset</a></li><li><a href="#Testset">Testset</a></li><li><a href="#Extraset">Extraset</a></li><li><a href="#Utilities">Utilities</a></li><li><a href="#References">References</a></li></ul></li></ul></li></ul><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>The <code>MLDatasets.SVHN2</code> sub-module provides a programmatic interface to download, load, and work with the SVHN2 dataset of handwritten digits.</p><pre><code class="language-julia hljs">using MLDatasets

# load full training set
train_x, train_y = SVHN2.traindata()

# load full test set
test_x,  test_y  = SVHN2.testdata()

# load additional train set
extra_x, extra_y = SVHN2.extradata()</code></pre><p>The provided functions also allow for optional arguments, such as the directory <code>dir</code> where the dataset is located, or the specific observation <code>indices</code> that one wants to work with. For more information on the interface take a look at the documentation (e.g. <code>?SVHN2.traindata</code>).</p><table><tbody><tr><th style="text-align: right">Function</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.download"><code>download([dir])</code></a></td><td style="text-align: right">Trigger interactive download of the dataset</td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.classnames"><code>classnames()</code></a></td><td style="text-align: right">Return the class names as a vector of strings</td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.traintensor"><code>traintensor([T], [indices]; [dir])</code></a></td><td style="text-align: right">Load the training images as an array of eltype <code>T</code></td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.trainlabels"><code>trainlabels([indices]; [dir])</code></a></td><td style="text-align: right">Load the labels for the training images</td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.traindata"><code>traindata([T], [indices]; [dir])</code></a></td><td style="text-align: right">Load images and labels of the training data</td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.testtensor"><code>testtensor([T], [indices]; [dir])</code></a></td><td style="text-align: right">Load the test images as an array of eltype <code>T</code></td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.testlabels"><code>testlabels([indices]; [dir])</code></a></td><td style="text-align: right">Load the labels for the test images</td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.testdata"><code>testdata([T], [indices]; [dir])</code></a></td><td style="text-align: right">Load images and labels of the test data</td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.extratensor"><code>extratensor([T], [indices]; [dir])</code></a></td><td style="text-align: right">Load the extra images as an array of eltype <code>T</code></td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.extralabels"><code>extralabels([indices]; [dir])</code></a></td><td style="text-align: right">Load the labels for the extra training images</td></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.extradata"><code>extradata([T], [indices]; [dir])</code></a></td><td style="text-align: right">Load images and labels of the extra training data</td></tr></tbody></table><p>This module also provides utility functions to make working with the SVHN (format 2) dataset in Julia more convenient.</p><table><tbody><tr><th style="text-align: right">Function</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><a href="#MLDatasets.SVHN2.convert2image"><code>convert2image(array)</code></a></td><td style="text-align: right">Convert the SVHN tensor/matrix to a colorant array</td></tr></tbody></table><p>To visualize an image or a prediction we provide the function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert the given SVHN2 horizontal-major tensor (or feature matrix) to a vertical-major <code>Colorant</code> array.</p><pre><code class="language-julia hljs">julia&gt; SVHN2.convert2image(SVHN2.traindata(1)[1]) # first training image
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><h2 id="API-Documentation"><a class="docs-heading-anchor" href="#API-Documentation">API Documentation</a><a id="API-Documentation-1"></a><a class="docs-heading-anchor-permalink" href="#API-Documentation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2" id="MLDatasets.SVHN2"><code>MLDatasets.SVHN2</code></a> — <span class="docstring-category">Module</span></header><section><div><p>The Street View House Numbers (SVHN) Dataset</p><ul><li>Authors: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng</li><li>Website: http://ufldl.stanford.edu/housenumbers</li></ul><p>SVHN was obtained from house numbers in Google Street View images. As such they are quite diverse in terms of orientation and image background. Similar to MNIST, SVHN has 10 classes (the digits 0-9), but unlike MNIST there is more data and the images are a little bigger (32x32 instead of 28x28) with an additional RGB color channel. The dataset is split up into three subsets: 73257 digits for training, 26032 digits for testing, and 531131 additional to use as extra training data.</p><p><strong>Interface</strong></p><ul><li><a href="#MLDatasets.SVHN2.traintensor"><code>SVHN2.traintensor</code></a>, <a href="#MLDatasets.SVHN2.trainlabels"><code>SVHN2.trainlabels</code></a>, <a href="#MLDatasets.SVHN2.traindata"><code>SVHN2.traindata</code></a></li><li><a href="#MLDatasets.SVHN2.testtensor"><code>SVHN2.testtensor</code></a>, <a href="#MLDatasets.SVHN2.testlabels"><code>SVHN2.testlabels</code></a>, <a href="#MLDatasets.SVHN2.testdata"><code>SVHN2.testdata</code></a></li><li><a href="#MLDatasets.SVHN2.extratensor"><code>SVHN2.extratensor</code></a>, <a href="#MLDatasets.SVHN2.extralabels"><code>SVHN2.extralabels</code></a>, <a href="#MLDatasets.SVHN2.extradata"><code>SVHN2.extradata</code></a></li></ul><p><strong>Utilities</strong></p><ul><li><a href="#MLDatasets.SVHN2.download"><code>SVHN2.download</code></a></li><li><a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a></li><li><a href="#MLDatasets.SVHN2.convert2image"><code>SVHN2.convert2image</code></a></li></ul></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/SVHN2.jl#L3-L29" target="_blank">source</a></section></article><h3 id="Trainingset"><a class="docs-heading-anchor" href="#Trainingset">Trainingset</a><a id="Trainingset-1"></a><a class="docs-heading-anchor-permalink" href="#Trainingset" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.traintensor" id="MLDatasets.SVHN2.traintensor"><code>MLDatasets.SVHN2.traintensor</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">traintensor([T = N0f8], [indices]; [dir]) -&gt; Array{T}</code></pre><p>Return the SVHN <strong>training</strong> images corresponding to the given <code>indices</code> as a multi-dimensional array of eltype <code>T</code>.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>.</p><p>If the parameter <code>indices</code> is omitted or an <code>AbstractVector</code>, the images are returned as a 4D array (i.e. a <code>Array{T,4}</code>), in which the first dimension corresponds to the pixel <em>columns</em> (y) of the image, the second dimension to the pixel <em>rows</em> (x) of the image, the third dimension the RGB color channels, and the fourth dimension denotes the index of the image.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.traintensor() # load all training images
32×32×3×73257 Array{N0f8,4}:
[...]

julia&gt; SVHN.traintensor(Float32, 1:3) # first three images as Float32
32×32×3×3 Array{Float32,4}:
[...]</code></pre><p>If <code>indices</code> is an <code>Integer</code>, the single image is returned as <code>Array{T,3}</code> in vertical-major layout, which means that the first dimension denotes the pixel <em>columns</em> (y), the second dimension denotes the pixel <em>rows</em> (x), and the third dimension the RGB color channels of the image.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.traintensor(1) # load first training image
32×32×3 Array{N0f8,3}:
[...]</code></pre><p>As mentioned above, the color channel is encoded in the third dimension. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.convert2image(SVHN2.traintensor(1))
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L15-L71" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.trainlabels" id="MLDatasets.SVHN2.trainlabels"><code>MLDatasets.SVHN2.trainlabels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainlabels([indices]; [dir])</code></pre><p>Returns the SVHN <strong>training</strong> labels corresponding to the given <code>indices</code> as an <code>Int</code> or <code>Vector{Int}</code>. The values of the labels denote the zero-based class-index that they represent (see <a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a> for the corresponding names). If <code>indices</code> is omitted, all labels are returned.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.trainlabels() # full training set
73257-element Array{Int64,1}:
[...]

julia&gt; SVHN2.trainlabels(1:3) # first three labels
3-element Array{Int64,1}:
[...]

julia&gt; SVHN2.trainlabels(1) # first label
[...]

julia&gt; SVHN2.classnames()[SVHN2.trainlabels(1)] # corresponding class
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L97-L124" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.traindata" id="MLDatasets.SVHN2.traindata"><code>MLDatasets.SVHN2.traindata</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">traindata([T = N0f8], [indices]; [dir]) -&gt; images, labels</code></pre><p>Returns the SVHN <strong>trainset</strong> corresponding to the given <code>indices</code> as a two-element tuple. If <code>indices</code> is omitted the full trainset is returned. The first element of the return values will be the images as a multi-dimensional array, and the second element the corresponding labels as integers.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array of eltype <code>T</code>. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype. The integer values of the labels correspond 1-to-1 the digit that they represent with the exception of 0 which is encoded as <code>10</code>.</p><p>Note that because of the nature of how the dataset is stored on disk, <code>SVHN2.traindata</code> will always load the full trainset, regardless of which observations are requested. In the case <code>indices</code> are provided by the user, it will simply result in a sub-setting. This option is just provided for convenience.</p><pre><code class="language-julia hljs">images, labels = SVHN2.traindata() # full dataset
images, labels = SVHN2.traindata(2) # only second observation
images, labels = SVHN2.traindata(dir="./SVHN") # custom folder</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L145-L180" target="_blank">source</a></section></article><h3 id="Testset"><a class="docs-heading-anchor" href="#Testset">Testset</a><a id="Testset-1"></a><a class="docs-heading-anchor-permalink" href="#Testset" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.testtensor" id="MLDatasets.SVHN2.testtensor"><code>MLDatasets.SVHN2.testtensor</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">testtensor([T = N0f8], [indices]; [dir]) -&gt; Array{T}</code></pre><p>Return the SVHN <strong>test</strong> images corresponding to the given <code>indices</code> as a multi-dimensional array of eltype <code>T</code>.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>.</p><p>If the parameter <code>indices</code> is omitted or an <code>AbstractVector</code>, the images are returned as a 4D array (i.e. a <code>Array{T,4}</code>), in which the first dimension corresponds to the pixel <em>columns</em> (y) of the image, the second dimension to the pixel <em>rows</em> (x) of the image, the third dimension the RGB color channels, and the fourth dimension denotes the index of the image.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.testtensor() # load all test images
32×32×3×26032 Array{N0f8,4}:
[...]

julia&gt; SVHN.testtensor(Float32, 1:3) # first three images as Float32
32×32×3×3 Array{Float32,4}:
[...]</code></pre><p>If <code>indices</code> is an <code>Integer</code>, the single image is returned as <code>Array{T,3}</code> in vertical-major layout, which means that the first dimension denotes the pixel <em>columns</em> (y), the second dimension denotes the pixel <em>rows</em> (x), and the third dimension the RGB color channels of the image.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.testtensor(1) # load first test image
32×32×3 Array{N0f8,3}:
[...]</code></pre><p>As mentioned above, the color channel is encoded in the third dimension. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.convert2image(SVHN2.testtensor(1))
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L15-L71" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.testlabels" id="MLDatasets.SVHN2.testlabels"><code>MLDatasets.SVHN2.testlabels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">testlabels([indices]; [dir])</code></pre><p>Returns the SVHN <strong>test</strong> labels corresponding to the given <code>indices</code> as an <code>Int</code> or <code>Vector{Int}</code>. The values of the labels denote the zero-based class-index that they represent (see <a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a> for the corresponding names). If <code>indices</code> is omitted, all labels are returned.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.testlabels() # full test set
26032-element Array{Int64,1}:
[...]

julia&gt; SVHN2.testlabels(1:3) # first three labels
3-element Array{Int64,1}:
[...]

julia&gt; SVHN2.testlabels(1) # first label
[...]

julia&gt; SVHN2.classnames()[SVHN2.testlabels(1)] # corresponding class
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L97-L124" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.testdata" id="MLDatasets.SVHN2.testdata"><code>MLDatasets.SVHN2.testdata</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">testdata([T = N0f8], [indices]; [dir]) -&gt; images, labels</code></pre><p>Returns the SVHN <strong>testset</strong> corresponding to the given <code>indices</code> as a two-element tuple. If <code>indices</code> is omitted the full testset is returned. The first element of the return values will be the images as a multi-dimensional array, and the second element the corresponding labels as integers.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array of eltype <code>T</code>. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype. The integer values of the labels correspond 1-to-1 the digit that they represent with the exception of 0 which is encoded as <code>10</code>.</p><p>Note that because of the nature of how the dataset is stored on disk, <code>SVHN2.testdata</code> will always load the full testset, regardless of which observations are requested. In the case <code>indices</code> are provided by the user, it will simply result in a sub-setting. This option is just provided for convenience.</p><pre><code class="language-julia hljs">images, labels = SVHN2.testdata() # full dataset
images, labels = SVHN2.testdata(2) # only second observation
images, labels = SVHN2.testdata(dir="./SVHN") # custom folder</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L145-L180" target="_blank">source</a></section></article><h3 id="Extraset"><a class="docs-heading-anchor" href="#Extraset">Extraset</a><a id="Extraset-1"></a><a class="docs-heading-anchor-permalink" href="#Extraset" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.extratensor" id="MLDatasets.SVHN2.extratensor"><code>MLDatasets.SVHN2.extratensor</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">extratensor([T = N0f8], [indices]; [dir]) -&gt; Array{T}</code></pre><p>Return the SVHN <strong>extra training</strong> images corresponding to the given <code>indices</code> as a multi-dimensional array of eltype <code>T</code>.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>.</p><p>If the parameter <code>indices</code> is omitted or an <code>AbstractVector</code>, the images are returned as a 4D array (i.e. a <code>Array{T,4}</code>), in which the first dimension corresponds to the pixel <em>columns</em> (y) of the image, the second dimension to the pixel <em>rows</em> (x) of the image, the third dimension the RGB color channels, and the fourth dimension denotes the index of the image.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.extratensor() # load all extra training images
32×32×3×531131 Array{N0f8,4}:
[...]

julia&gt; SVHN.extratensor(Float32, 1:3) # first three images as Float32
32×32×3×3 Array{Float32,4}:
[...]</code></pre><p>If <code>indices</code> is an <code>Integer</code>, the single image is returned as <code>Array{T,3}</code> in vertical-major layout, which means that the first dimension denotes the pixel <em>columns</em> (y), the second dimension denotes the pixel <em>rows</em> (x), and the third dimension the RGB color channels of the image.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.extratensor(1) # load first extra training image
32×32×3 Array{N0f8,3}:
[...]</code></pre><p>As mentioned above, the color channel is encoded in the third dimension. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.convert2image(SVHN2.extratensor(1))
32×32 Array{RGB{N0f8},2}:
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L15-L71" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.extralabels" id="MLDatasets.SVHN2.extralabels"><code>MLDatasets.SVHN2.extralabels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">extralabels([indices]; [dir])</code></pre><p>Returns the SVHN <strong>extra training</strong> labels corresponding to the given <code>indices</code> as an <code>Int</code> or <code>Vector{Int}</code>. The values of the labels denote the zero-based class-index that they represent (see <a href="#MLDatasets.SVHN2.classnames"><code>SVHN2.classnames</code></a> for the corresponding names). If <code>indices</code> is omitted, all labels are returned.</p><pre><code class="language-julia-repl hljs">julia&gt; SVHN2.extralabels() # full extra training set
531131-element Array{Int64,1}:
[...]

julia&gt; SVHN2.extralabels(1:3) # first three labels
3-element Array{Int64,1}:
[...]

julia&gt; SVHN2.extralabels(1) # first label
[...]

julia&gt; SVHN2.classnames()[SVHN2.extralabels(1)] # corresponding class
[...]</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L97-L124" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.extradata" id="MLDatasets.SVHN2.extradata"><code>MLDatasets.SVHN2.extradata</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">extradata([T = N0f8], [indices]; [dir]) -&gt; images, labels</code></pre><p>Returns the SVHN <strong>extra trainset</strong> corresponding to the given <code>indices</code> as a two-element tuple. If <code>indices</code> is omitted the full extra trainset is returned. The first element of the return values will be the images as a multi-dimensional array, and the second element the corresponding labels as integers.</p><p>The image(s) is/are returned in the native vertical-major memory layout as a single numeric array of eltype <code>T</code>. If <code>T &lt;: Integer</code>, then all values will be within <code>0</code> and <code>255</code>, otherwise the values are scaled to be between <code>0</code> and <code>1</code>. You can use the utility function <a href="#MLDatasets.SVHN2.convert2image"><code>convert2image</code></a> to convert an SVHN array into a Julia image with the appropriate <code>RGB</code> eltype. The integer values of the labels correspond 1-to-1 the digit that they represent with the exception of 0 which is encoded as <code>10</code>.</p><p>Note that because of the nature of how the dataset is stored on disk, <code>SVHN2.extradata</code> will always load the full extra trainset, regardless of which observations are requested. In the case <code>indices</code> are provided by the user, it will simply result in a sub-setting. This option is just provided for convenience.</p><pre><code class="language-julia hljs">images, labels = SVHN2.extradata() # full dataset
images, labels = SVHN2.extradata(2) # only second observation
images, labels = SVHN2.extradata(dir="./SVHN") # custom folder</code></pre><p>The corresponding resource file(s) of the dataset is/are expected to be located in the specified directory <code>dir</code>. If <code>dir</code> is omitted the directories in <code>DataDeps.default_loadpath</code> will be searched for an existing <code>SVHN2</code> subfolder. In case no such subfolder is found, <code>dir</code> will default to <code>~/.julia/datadeps/SVHN2</code>. In the case that <code>dir</code> does not yet exist, a download prompt will be triggered. You can also use <code>SVHN2.download([dir])</code> explicitly for pre-downloading (or re-downloading) the dataset. Please take a look at the documentation of the package DataDeps.jl for more detail and configuration options.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L145-L180" target="_blank">source</a></section></article><h3 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.download" id="MLDatasets.SVHN2.download"><code>MLDatasets.SVHN2.download</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">download([dir]; [i_accept_the_terms_of_use])</code></pre><p>Trigger the (interactive) download of the full dataset into "<code>dir</code>". If no <code>dir</code> is provided the dataset will be downloaded into "~/.julia/datadeps/SVHN2".</p><p>This function will display an interactive dialog unless either the keyword parameter <code>i_accept_the_terms_of_use</code> or the environment variable <code>DATADEPS_ALWAY_ACCEPT</code> is set to <code>true</code>. Note that using the data responsibly and respecting copyright/terms-of-use remains your responsibility.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/SVHN2.jl#L65-L77" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.classnames" id="MLDatasets.SVHN2.classnames"><code>MLDatasets.SVHN2.classnames</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">classnames() -&gt; Vector{Int}</code></pre><p>Return the 10 digits for the SVHN classes as a vector of integers.</p></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/interface.jl#L1-L5" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MLDatasets.SVHN2.convert2image" id="MLDatasets.SVHN2.convert2image"><code>MLDatasets.SVHN2.convert2image</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">convert2image(array) -&gt; Array{RGB}</code></pre><p>Convert the given SVHN tensor in WHCN format (or feature vector/matrix) to a <code>RGB</code> array in HWN format.</p><pre><code class="language-julia hljs">julia&gt; SVHN2.convert2image(SVHN2.traindata()[1]) # full training dataset
32×32×50000 Array{RGB{N0f8},3}:
[...]

julia&gt; SVHN2.convert2image(SVHN2.traindata(1)[1]) # first training image
32×32 Array{RGB{N0f8},2}:
[...]</code></pre></div><a class="docs-sourcelink" href="https://github.com/JuliaML/MLDatasets.jl/blob/6f4aa5a78f66d2f09ae0506ee16168a577698b5a/src/SVHN2/utils.jl#L1-L16" target="_blank">source</a></section></article><h3 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h3><ul><li><p><strong>Authors</strong>: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng</p></li><li><p><strong>Website</strong>: http://ufldl.stanford.edu/housenumbers</p></li><li><p><strong>[Netzer et al., 2011]</strong> Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng. "Reading Digits in Natural Images with Unsupervised Feature Learning" NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011</p></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../CIFAR100/">« CIFAR-100</a><a class="docs-footer-nextpage" href="../Iris/">Iris »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Saturday 23 October 2021 08:29">Saturday 23 October 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>