<!DOCTYPE html><HTML lang="en" xmlns="http://www.w3.org/1999/xhtml"><head><script charset="utf-8" src="../../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/'</script><script charset="utf-8" src="../../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="utf-8"/><meta content="Publish.jl" name="generator"/><meta content="width=device-width, initial-scale=1" name="viewport"/><meta content="" name="keywords"/><title>ParameterSchedulers.jl</title><link href="../../normalize.css" rel="stylesheet"/><link href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" rel="stylesheet"/><link href="../../tabulator_simple.min.css" rel="stylesheet"/><link href="../../publish.css" rel="stylesheet"/><link href="../../default.min.css" rel="stylesheet"/><link href="../../custom.css" rel="stylesheet"/><script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"></script><script src="../../versions.js"></script><script src="../../lunr.js"></script><script src="../../highlight.min.js"></script><script src="../../tabulator.min.js"></script><script src="../../julia.min.js"></script><script src="../../julia-repl.min.js"></script><script src="../../publish.js"></script><link href="../../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link nav-item" href="../../../../flux/">Flux</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Building Blocks</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a><a class="nav-link nav-item" href="../../../../functors/">Functors</a></ul></div><div class="nav-mega-column"><div class="column-header">Automatic differentiation libraries</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../zygote/">Zygote</a></ul></div><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Training</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Data Wrangling</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mlutils/">MLUtils</a><a class="nav-link nav-item" href="../../../../onehotarrays/">OneHotArrays</a></ul></div><div class="nav-mega-column"><div class="column-header">Data Augmentation</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../dataaugmentation/">DataAugmentation</a></ul></div><div class="nav-mega-column"><div class="column-header">Datasets</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mldatasets/">MLDatasets</a></ul></div><div class="nav-mega-column"><div class="column-header">Schedulers</div><ul class="column-content"><a class="nav-link active nav-item" href="../../../dev/">ParameterSchedulers</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Models</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Computer Vision</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../metalhead/">Metalhead</a></ul></div><div class="nav-mega-column"><div class="column-header">Natural Language Processing</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../transformers/">Transformers</a></ul></div></div></div><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><main id="page"><div class="menu"><div id="projectname">ParameterSchedulers.jl</div><input id="search-input" placeholder="Search"/><select id="version-selector"></select><svg fill="none" height="24" id="menu-toggler" onclick="toggleIndexPage();" title="Contents" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M2 6C2 5.44772 2.44772 5 3 5H21C21.5523 5 22 5.44772 22 6C22 6.55228 21.5523 7 21 7H3C2.44772 7 2 6.55228 2 6Z" fill="currentColor"></path><path d="M2 12.0322C2 11.4799 2.44772 11.0322 3 11.0322H21C21.5523 11.0322 22 11.4799 22 12.0322C22 12.5845 21.5523 13.0322 21 13.0322H3C2.44772 13.0322 2 12.5845 2 12.0322Z" fill="currentColor"></path><path d="M3 17.0645C2.44772 17.0645 2 17.5122 2 18.0645C2 18.6167 2.44772 19.0645 3 19.0645H21C21.5523 19.0645 22 18.6167 22 18.0645C22 17.5122 21.5523 17.0645 21 17.0645H3Z" fill="currentColor"></path></svg></div><div id="toc"><p><a href="../../README.html">Introduction</a></p><p><a href="../../docs/cheatsheet.html">Schedule cheatsheet</a></p><h1 id="tutorials"><a class="anchor" href="../../#tutorials"></a>Tutorials</h1><ul><li><a href="../../docs/tutorials/getting-started.html">Getting started</a></li><li><a href="../../docs/tutorials/basic-schedules.html">Basic schedules</a></li><li><a href="../../docs/tutorials/optimizers.html">Optimizers</a></li><li><a href="../../docs/tutorials/complex-schedules.html">Complex schedules</a></li><li><a href="../../docs/tutorials/warmup-schedules.html">Warmup schedules</a></li></ul><p><a href="../../docs/interfaces/generic.html">Interface</a></p><hr/><p><a href="../../docstrings.html">API Reference</a></p></div><article id="content"><h1 id="scheduling-optimizers"><a class="anchor" href="#scheduling-optimizers"></a>Scheduling optimizers</h1><p>A schedule by itself is not helpful; we need to use the schedules to adjust parameters. In this tutorial, we will examine three ways to do just that — iterating the schedule, using a stateful iterator, and using an scheduled optimizer.</p><h2 id="iterating-during-training"><a class="anchor" href="#iterating-during-training"></a>Iterating during training</h2><p>Since every schedule is a standard iterator, we can insert it into a training loop by simply zipping up with another iterator. For example, the following code adjusts the learning rate of the optimizer before each batch of training.</p><pre><code cell="optimizers" class="language-julia">using Flux, ParameterSchedulers

data = [(rand(4, 10), rand([-1, 1], 1, 10)) for _ in 1:3]
m = Chain(Dense(4, 4, tanh), Dense(4, 1, tanh))
p = Flux.params(m)
opt = Descent()
s = Exp(λ = 1e-1, γ = 0.2)

for (η, (x, y)) in zip(s, data)
    opt.eta = η
    g = Flux.gradient(() -&gt; Flux.mse(m(x), y), p)
    Flux.update!(opt, p, g)
    println("η: ", opt.eta)
end
</code></pre><pre><code class="plaintext cell-output cell-stream">η: 0.1
η: 0.020000000000000004
η: 0.004000000000000001
</code></pre><p>We can also adjust the learning on an epoch basis instead. All that is required is to change what we zip our schedule with.</p><pre><code cell="optimizers" class="language-julia">nepochs = 6
s = Step(λ = 1e-1, γ = 0.2, step_sizes = [3, 2, 1])
for (η, epoch) in zip(s, 1:nepochs)
    opt.eta = η
    for (i, (x, y)) in enumerate(data)
        g = Flux.gradient(() -&gt; Flux.mse(m(x), y), p)
        Flux.update!(opt, p, g)
        println("epoch: $epoch, batch: $i, η: $(opt.eta)")
    end
end
</code></pre><pre><code class="plaintext cell-output cell-stream">epoch: 1, batch: 1, η: 0.1
epoch: 1, batch: 2, η: 0.1
epoch: 1, batch: 3, η: 0.1
epoch: 2, batch: 1, η: 0.1
epoch: 2, batch: 2, η: 0.1
epoch: 2, batch: 3, η: 0.1
epoch: 3, batch: 1, η: 0.1
epoch: 3, batch: 2, η: 0.1
epoch: 3, batch: 3, η: 0.1
epoch: 4, batch: 1, η: 0.020000000000000004
epoch: 4, batch: 2, η: 0.020000000000000004
epoch: 4, batch: 3, η: 0.020000000000000004
epoch: 5, batch: 1, η: 0.020000000000000004
epoch: 5, batch: 2, η: 0.020000000000000004
epoch: 5, batch: 3, η: 0.020000000000000004
epoch: 6, batch: 1, η: 0.004000000000000001
epoch: 6, batch: 2, η: 0.004000000000000001
epoch: 6, batch: 3, η: 0.004000000000000001
</code></pre><h2 id="stateful-iteration-with-training"><a class="anchor" href="#stateful-iteration-with-training"></a>Stateful iteration with training</h2><p>Sometimes zipping up the schedule with an iterator isn’t sufficient. For example, we might want to advance the schedule with every batch but not be forced to restart each epoch. In such a situation with nested loops, it becomes useful to use <a href="../../docstrings/ParameterSchedulers.Stateful.html"><code>ParameterSchedulers.Stateful</code></a> which maintains its own iteration state.</p><pre><code cell="optimizers" class="language-julia">nepochs = 3
s = ParameterSchedulers.Stateful(Inv(λ = 1e-1, γ = 0.2, p = 2))
for epoch in 1:nepochs
    for (i, (x, y)) in enumerate(data)
        opt.eta = ParameterSchedulers.next!(s)
        g = Flux.gradient(() -&gt; Flux.mse(m(x), y), p)
        Flux.update!(opt, p, g)
        println("epoch: $epoch, batch: $i, η: $(opt.eta)")
    end
end
</code></pre><pre><code class="plaintext cell-output cell-stream">epoch: 1, batch: 1, η: 0.1
epoch: 1, batch: 2, η: 0.06944444444444445
epoch: 1, batch: 3, η: 0.051020408163265314
epoch: 2, batch: 1, η: 0.03906249999999999
epoch: 2, batch: 2, η: 0.030864197530864196
epoch: 2, batch: 3, η: 0.025
epoch: 3, batch: 1, η: 0.020661157024793386
epoch: 3, batch: 2, η: 0.01736111111111111
epoch: 3, batch: 3, η: 0.014792899408284023
</code></pre><h2 id="working-with-flux-optimizers"><a class="anchor" href="#working-with-flux-optimizers"></a>Working with Flux optimizers</h2><div class="admonition warning"><p class="admonition-title">Warning</p><p>Currently, we are porting <code>Scheduler</code> to Flux.jl.
It may be renamed once it is ported out of this package.
The API will also undergo minor changes.</p></div><p>While the approaches above can be helpful when dealing with fine-grained training loops, it is usually simpler to just use a <a href="../../docstrings/ParameterSchedulers.Scheduler.html"><code>ParameterSchedulers.Scheduler</code></a>.</p><pre><code cell="optimizers" class="language-julia">using ParameterSchedulers: Scheduler

nepochs = 3
s = Inv(λ = 1e-1, p = 2, γ = 0.2)
opt = Scheduler(s, Descent())
for epoch in 1:nepochs
    for (i, (x, y)) in enumerate(data)
        g = Flux.gradient(() -&gt; Flux.mse(m(x), y), p)
        Flux.update!(opt, p, g)
        println("epoch: $epoch, batch: $i, η: $(opt.optim.eta)")
    end
end
</code></pre><pre><code class="plaintext cell-output cell-stream">epoch: 1, batch: 1, η: 0.1
epoch: 1, batch: 2, η: 0.06944444444444445
epoch: 1, batch: 3, η: 0.051020408163265314
epoch: 2, batch: 1, η: 0.03906249999999999
epoch: 2, batch: 2, η: 0.030864197530864196
epoch: 2, batch: 3, η: 0.025
epoch: 3, batch: 1, η: 0.020661157024793386
epoch: 3, batch: 2, η: 0.01736111111111111
epoch: 3, batch: 3, η: 0.014792899408284023
</code></pre><p>The scheduler, <code>opt</code>, can be used anywhere a Flux optimizer can. For example, it can be passed to <code>Flux.train!</code>:</p><pre><code cell="optimizers" class="language-julia">s = Inv(λ = 1e-1, p = 2, γ = 0.2)
opt = Scheduler(s, Descent())
loss(x, y, m) = Flux.mse(m(x), y)
cb = () -&gt; @show(opt.optim.eta)
Flux.@epochs nepochs Flux.train!((x, y) -&gt; loss(x, y, m), Flux.params(m), data, opt, cb = cb)
</code></pre><pre><code class="plaintext cell-output cell-stream">┌ Warning: The macro `@epochs` will be removed from Flux 0.14.
│ As an alternative, you can write a simple `for i in 1:epochs` loop.
│   caller = eval at boot.jl:368 [inlined]
└ @ Core ./boot.jl:368
[ Info: Epoch 1
opt.optim.eta = 0.1
opt.optim.eta = 0.06944444444444445
opt.optim.eta = 0.051020408163265314
[ Info: Epoch 2
opt.optim.eta = 0.03906249999999999
opt.optim.eta = 0.030864197530864196
opt.optim.eta = 0.025
[ Info: Epoch 3
opt.optim.eta = 0.020661157024793386
opt.optim.eta = 0.01736111111111111
opt.optim.eta = 0.014792899408284023
</code></pre><p>Finally, you might be interested in reading <a href="complex-schedules.html#interpolating-schedules">Interpolating schedules</a> to see how to specify a schedule in terms of epochs but iterate it at the granularity of batches.</p></article><div id="page-navigation"><a href="basic-schedules.html" id="previous-page" title="Previous"><svg fill="none" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M16.2426 6.34317L14.8284 4.92896L7.75739 12L14.8285 19.0711L16.2427 17.6569L10.5858 12L16.2426 6.34317Z" fill="currentColor"></path></svg></a><a href="complex-schedules.html" id="next-page" title="Next"><svg fill="none" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M10.5858 6.34317L12 4.92896L19.0711 12L12 19.0711L10.5858 17.6569L16.2427 12L10.5858 6.34317Z" fill="currentColor"></path></svg></a></div><footer>
            Built with <a href="https://github.com/MichaelHatherly/Publish.jl" target="_blank">Publish.jl</a> and the <a href="https://julialang.org" target="_blank">Julia Language</a>.
        </footer></main><script>hljs.initHighlightingOnLoad();</script><script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ]
            });
        });
    </script></body></HTML>