<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/'</script><script charset="utf-8" src="../../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>More advanced layers · Metalhead.jl</title><script data-outdated-warner="" src="../../assets/warner.js"></script><link href="nothing/metalhead/stable/api/layers_adv/" rel="canonical"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link nav-item" href="../../../../flux/">Flux</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Building Blocks</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a><a class="nav-link nav-item" href="../../../../functors/">Functors</a></ul></div><div class="nav-mega-column"><div class="column-header">Automatic differentiation libraries</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../zygote/">Zygote</a></ul></div><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Training</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Data Wrangling</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mlutils/">MLUtils</a><a class="nav-link nav-item" href="../../../../onehotarrays/">OneHotArrays</a></ul></div><div class="nav-mega-column"><div class="column-header">Data Augmentation</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../dataaugmentation/">DataAugmentation</a></ul></div><div class="nav-mega-column"><div class="column-header">Datasets</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mldatasets/">MLDatasets</a></ul></div><div class="nav-mega-column"><div class="column-header">Schedulers</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../paramschedulers/dev/">ParameterSchedulers</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Models</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Computer Vision</div><ul class="column-content"><a class="nav-link active nav-item" href="../../../">Metalhead</a></ul></div><div class="nav-mega-column"><div class="column-header">Natural Language Processing</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../transformers/">Transformers</a></ul></div></div></div><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Metalhead.jl</a></span></div><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/quickstart/">A guide to getting started with Metalhead</a></li><li><a class="tocitem" href="../../tutorials/pretrained/">Working with pre-trained models from Metalhead</a></li></ul></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../../howto/resnet/">Using the ResNet model family in Metalhead.jl</a></li></ul></li><li><a class="tocitem" href="../../contributing/">Contributing to Metalhead</a></li><li><span class="tocitem">API reference</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Convolutional Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../resnet/">ResNet-like models</a></li><li><a class="tocitem" href="../densenet/">DenseNet</a></li><li><a class="tocitem" href="../efficientnet/">EfficientNet family of models</a></li><li><a class="tocitem" href="../mobilenet/">MobileNet family of models</a></li><li><a class="tocitem" href="../inception/">Inception family of models</a></li><li><a class="tocitem" href="../hybrid/">Hybrid CNN architectures</a></li><li><a class="tocitem" href="../others/">Other models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-2" type="checkbox"/><label class="tocitem" for="menuitem-5-2"><span class="docs-label">Mixers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../mixers/">MLPMixer-like models</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Vision Transformers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../vit/">Vision Transformer models</a></li></ul></li><li><input checked="" class="collapse-toggle" id="menuitem-5-4" type="checkbox"/><label class="tocitem" for="menuitem-5-4"><span class="docs-label">Layers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../layers_intro/">An introduction to the <code>Layers</code> module in Metalhead.jl</a></li><li class="is-active"><a class="tocitem" href="">More advanced layers</a><ul class="internal"><li><a class="tocitem" href="#Squeeze-and-excitation-blocks"><span>Squeeze-and-excitation blocks</span></a></li><li><a class="tocitem" href="#Inverted-residual-blocks"><span>Inverted residual blocks</span></a></li><li><a class="tocitem" href="#Vision-transformer-related-layers"><span>Vision transformer-related layers</span></a></li><li><a class="tocitem" href="#MLPMixer-related-blocks"><span>MLPMixer-related blocks</span></a></li><li><a class="tocitem" href="#Miscellaneous-utilities-for-layers"><span>Miscellaneous utilities for layers</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../utilities/">Model Utilities</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API reference</a></li><li><a class="is-disabled">Layers</a></li><li class="is-active"><a href="">More advanced layers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">More advanced layers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Metalhead.jl/blob/master/docs/src/api/layers_adv.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="More-advanced-layers"><a class="docs-heading-anchor" href="#More-advanced-layers">More advanced layers</a><a id="More-advanced-layers-1"></a><a class="docs-heading-anchor-permalink" href="#More-advanced-layers" title="Permalink"></a></h1><p>This page contains the API reference for some more advanced layers present in the <code>Layers</code> module. These layers are used in Metalhead.jl to build more complex models, and can also be used by the user to build custom models. For a more basic introduction to the <code>Layers</code> module, please refer to the <a href="../layers_intro/#layers-intro">introduction guide</a> for the <code>Layers</code> module.</p><h2 id="Squeeze-and-excitation-blocks"><a class="docs-heading-anchor" href="#Squeeze-and-excitation-blocks">Squeeze-and-excitation blocks</a><a id="Squeeze-and-excitation-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Squeeze-and-excitation-blocks" title="Permalink"></a></h2><p>These are used in models like SE-ResNet and SE-ResNeXt, as well as in the design of inverted residual blocks used in the MobileNet and EfficientNet family of models.</p><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.squeeze_excite" id="Metalhead.Layers.squeeze_excite"><code>Metalhead.Layers.squeeze_excite</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">squeeze_excite(inplanes::Integer; reduction::Real = 16, round_fn = _round_channels, 
               norm_layer = identity, activation = relu, gate_activation = sigmoid)</code></pre><p>Creates a squeeze-and-excitation layer used in MobileNets, EfficientNets and SE-ResNets.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: The number of input feature maps</li><li><code>reduction</code>: The reduction factor for the number of hidden feature maps in the squeeze and excite layer. The number of hidden feature maps is calculated as <code>round_fn(inplanes / reduction)</code>.</li><li><code>round_fn</code>: The function to round the number of reduced feature maps.</li><li><code>activation</code>: The activation function for the first convolution layer</li><li><code>gate_activation</code>: The activation function for the gate layer</li><li><code>norm_layer</code>: The normalization layer to be used after the convolution layers</li><li><code>rd_planes</code>: The number of hidden feature maps in a squeeze and excite layer</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/selayers.jl#L1-L18" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.effective_squeeze_excite" id="Metalhead.Layers.effective_squeeze_excite"><code>Metalhead.Layers.effective_squeeze_excite</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">effective_squeeze_excite(inplanes, gate_activation = sigmoid)</code></pre><p>Effective squeeze-and-excitation layer. (reference: <a href="https://arxiv.org/abs/1911.06667">CenterMask : Real-Time Anchor-Free Instance Segmentation</a>)</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: The number of input feature maps</li><li><code>gate_activation</code>: The activation function for the gate layer</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/selayers.jl#L29-L39" target="_blank">source</a></section></article><h2 id="Inverted-residual-blocks"><a class="docs-heading-anchor" href="#Inverted-residual-blocks">Inverted residual blocks</a><a id="Inverted-residual-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Inverted-residual-blocks" title="Permalink"></a></h2><p>These blocks are designed to be used in the MobileNet and EfficientNet family of convolutional neural networks.</p><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.dwsep_conv_norm" id="Metalhead.Layers.dwsep_conv_norm"><code>Metalhead.Layers.dwsep_conv_norm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dwsep_conv_norm(kernel_size::Dims{2}, inplanes::Integer, outplanes::Integer,
                activation = relu; norm_layer = BatchNorm, stride::Integer = 1,
                bias::Bool = !(norm_layer !== identity), pad::Integer = 0, [bias, weight, init])</code></pre><p>Create a depthwise separable convolution chain as used in MobileNetv1. This is sequence of layers:</p><ul><li>a <code>kernel_size</code> depthwise convolution from <code>inplanes =&gt; inplanes</code></li><li>a (batch) normalisation layer + <code>activation</code> (if <code>norm_layer !== identity</code>; otherwise <code>activation</code> is applied to the convolution output)</li><li>a <code>kernel_size</code> convolution from <code>inplanes =&gt; outplanes</code></li><li>a (batch) normalisation layer + <code>activation</code> (if <code>norm_layer !== identity</code>; otherwise <code>activation</code> is applied to the convolution output)</li></ul><p>See Fig. 3 in <a href="https://arxiv.org/abs/1704.04861v1">reference</a>.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: size of the convolution kernel (tuple)</li><li><code>inplanes</code>: number of input feature maps</li><li><code>outplanes</code>: number of output feature maps</li><li><code>activation</code>: the activation function for the final layer</li><li><code>norm_layer</code>: the normalisation layer used. Note that using <code>identity</code> as the normalisation layer will result in no normalisation being applied.</li><li><code>bias</code>: whether to use bias in the convolution layers.</li><li><code>stride</code>: stride of the first convolution kernel</li><li><code>pad</code>: padding of the first convolution kernel</li><li><code>weight</code>, <code>init</code>: initialization for the convolution kernel (see <a href="api/@ref"><code>Flux.Conv</code></a>)</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/mbconv.jl#L1-L30" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.mbconv" id="Metalhead.Layers.mbconv"><code>Metalhead.Layers.mbconv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mbconv(kernel_size::Dims{2}, inplanes::Integer, explanes::Integer,
       outplanes::Integer, activation = relu; stride::Integer,
       reduction::Union{Nothing, Real} = nothing,
       se_round_fn = x -&gt; round(Int, x), norm_layer = BatchNorm, kwargs...)</code></pre><p>Create a basic inverted residual block for MobileNet and Efficient variants. This is a sequence of layers:</p><ul><li><p>a 1x1 convolution from <code>inplanes =&gt; explanes</code> followed by a (batch) normalisation layer</p></li><li><p><code>activation</code> if <code>inplanes != explanes</code></p></li><li><p>a <code>kernel_size</code> depthwise separable convolution from <code>explanes =&gt; explanes</code></p></li><li><p>a (batch) normalisation layer</p></li><li><p>a squeeze-and-excitation block (if <code>reduction != nothing</code>) from <code>explanes =&gt; se_round_fn(explanes / reduction)</code> and back to <code>explanes</code></p></li><li><p>a 1x1 convolution from <code>explanes =&gt; outplanes</code></p></li><li><p>a (batch) normalisation layer + <code>activation</code></p></li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This function does not handle the residual connection by default. The user must add this manually to use this block as a standalone. To construct a model, check out the builders, which handle the residual connection and other details.</p></div></div><p>First introduced in the MobileNetv2 paper. (See Fig. 3 in <a href="https://arxiv.org/abs/1801.04381v4">reference</a>.)</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: kernel size of the convolutional layers</li><li><code>inplanes</code>: number of input feature maps</li><li><code>explanes</code>: The number of expanded feature maps. This is the number of feature maps after the first 1x1 convolution.</li><li><code>outplanes</code>: The number of output feature maps</li><li><code>activation</code>: The activation function for the first two convolution layer</li><li><code>stride</code>: The stride of the convolutional kernel, has to be either 1 or 2</li><li><code>reduction</code>: The reduction factor for the number of hidden feature maps in a squeeze and excite layer (see <a href="#Metalhead.Layers.squeeze_excite"><code>squeeze_excite</code></a>)</li><li><code>se_round_fn</code>: The function to round the number of reduced feature maps in the squeeze and excite layer</li><li><code>norm_layer</code>: The normalization layer to use</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/mbconv.jl#L39-L80" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.fused_mbconv" id="Metalhead.Layers.fused_mbconv"><code>Metalhead.Layers.fused_mbconv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fused_mbconv(kernel_size::Dims{2}, inplanes::Integer, explanes::Integer,
             outplanes::Integer, activation = relu;
             stride::Integer, norm_layer = BatchNorm)</code></pre><p>Create a fused inverted residual block.</p><p>This is a sequence of layers:</p><ul><li>a <code>kernel_size</code> depthwise separable convolution from <code>explanes =&gt; explanes</code></li><li>a (batch) normalisation layer</li><li>a 1x1 convolution from <code>explanes =&gt; outplanes</code> followed by a (batch) normalisation layer + <code>activation</code> if <code>inplanes != explanes</code></li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This function does not handle the residual connection by default. The user must add this manually to use this block as a standalone. To construct a model, check out the builders, which handle the residual connection and other details.</p></div></div><p>Originally introduced by Google in <a href="https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html">EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML</a>. Later used in the EfficientNetv2 paper.</p><p><strong>Arguments</strong></p><ul><li><code>kernel_size</code>: kernel size of the convolutional layers</li><li><code>inplanes</code>: number of input feature maps</li><li><code>explanes</code>: The number of expanded feature maps</li><li><code>outplanes</code>: The number of output feature maps</li><li><code>activation</code>: The activation function for the first two convolution layer</li><li><code>stride</code>: The stride of the convolutional kernel, has to be either 1 or 2</li><li><code>norm_layer</code>: The normalization layer to use</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/mbconv.jl#L107-L138" target="_blank">source</a></section></article><h2 id="Vision-transformer-related-layers"><a class="docs-heading-anchor" href="#Vision-transformer-related-layers">Vision transformer-related layers</a><a id="Vision-transformer-related-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Vision-transformer-related-layers" title="Permalink"></a></h2><p>The <code>Layers</code> module contains specific layers that are used to build vision transformer (ViT)-inspired models:</p><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.MultiHeadSelfAttention" id="Metalhead.Layers.MultiHeadSelfAttention"><code>Metalhead.Layers.MultiHeadSelfAttention</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MultiHeadSelfAttention(planes::Integer, nheads::Integer = 8; qkv_bias::Bool = false, 
            attn_dropout_prob = 0., proj_dropout_prob = 0.)</code></pre><p>Multi-head self-attention layer.</p><p><strong>Arguments</strong></p><ul><li><code>planes</code>: number of input channels</li><li><code>nheads</code>: number of heads</li><li><code>qkv_bias</code>: whether to use bias in the layer to get the query, key and value</li><li><code>attn_dropout_prob</code>: dropout probability after the self-attention layer</li><li><code>proj_dropout_prob</code>: dropout probability after the projection layer</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/attention.jl#L1-L14" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.ClassTokens" id="Metalhead.Layers.ClassTokens"><code>Metalhead.Layers.ClassTokens</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClassTokens(planes::Integer; init = Flux.zeros32)</code></pre><p>Appends class tokens to an input with embedding dimension <code>planes</code> for use in many vision transformer models.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/embeddings.jl#L51-L56" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.ViPosEmbedding" id="Metalhead.Layers.ViPosEmbedding"><code>Metalhead.Layers.ViPosEmbedding</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ViPosEmbedding(embedsize::Integer, npatches::Integer; 
               init = (dims::Dims{2}) -&gt; rand(Float32, dims))</code></pre><p>Positional embedding layer used by many vision transformer-like models.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/embeddings.jl#L33-L38" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.PatchEmbedding" id="Metalhead.Layers.PatchEmbedding"><code>Metalhead.Layers.PatchEmbedding</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">PatchEmbedding(imsize::Dims{2} = (224, 224); inchannels::Integer = 3,
               patch_size::Dims{2} = (16, 16), embedplanes = 768,
               norm_layer = planes -&gt; identity, flatten = true)</code></pre><p>Patch embedding layer used by many vision transformer-like models to split the input image into patches.</p><p><strong>Arguments</strong></p><ul><li><code>imsize</code>: the size of the input image</li><li><code>inchannels</code>: number of input channels</li><li><code>patch_size</code>: the size of the patches</li><li><code>embedplanes</code>: the number of channels in the embedding</li><li><code>norm_layer</code>: the normalization layer - by default the identity function but otherwise takes a single argument constructor for a normalization layer like LayerNorm or BatchNorm</li><li><code>flatten</code>: set true to flatten the input spatial dimensions after the embedding</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/embeddings.jl#L3-L20" target="_blank">source</a></section></article><h2 id="MLPMixer-related-blocks"><a class="docs-heading-anchor" href="#MLPMixer-related-blocks">MLPMixer-related blocks</a><a id="MLPMixer-related-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#MLPMixer-related-blocks" title="Permalink"></a></h2><p>Apart from this, the <code>Layers</code> module also contains certain blocks used in MLPMixer-style models:</p><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.gated_mlp_block" id="Metalhead.Layers.gated_mlp_block"><code>Metalhead.Layers.gated_mlp_block</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gated_mlp(gate_layer, inplanes::Integer, hidden_planes::Integer, 
          outplanes::Integer = inplanes; dropout_prob = 0.0, activation = gelu)</code></pre><p>Feedforward block based on the implementation in the paper "Pay Attention to MLPs". (<a href="https://arxiv.org/abs/2105.08050">reference</a>)</p><p><strong>Arguments</strong></p><ul><li><code>gate_layer</code>: Layer to use for the gating.</li><li><code>inplanes</code>: Number of dimensions in the input.</li><li><code>hidden_planes</code>: Number of dimensions in the intermediate layer.</li><li><code>outplanes</code>: Number of dimensions in the output - by default it is the same as <code>inplanes</code>.</li><li><code>dropout_prob</code>: Dropout probability.</li><li><code>activation</code>: Activation function to use.</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/mlp.jl#L22-L37" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.mlp_block" id="Metalhead.Layers.mlp_block"><code>Metalhead.Layers.mlp_block</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mlp_block(inplanes::Integer, hidden_planes::Integer, outplanes::Integer = inplanes; 
          dropout_prob = 0., activation = gelu)</code></pre><p>Feedforward block used in many MLPMixer-like and vision-transformer models.</p><p><strong>Arguments</strong></p><ul><li><code>inplanes</code>: Number of dimensions in the input.</li><li><code>hidden_planes</code>: Number of dimensions in the intermediate layer.</li><li><code>outplanes</code>: Number of dimensions in the output - by default it is the same as <code>inplanes</code>.</li><li><code>dropout_prob</code>: Dropout probability.</li><li><code>activation</code>: Activation function to use.</li></ul></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/mlp.jl#L2-L15" target="_blank">source</a></section></article><h2 id="Miscellaneous-utilities-for-layers"><a class="docs-heading-anchor" href="#Miscellaneous-utilities-for-layers">Miscellaneous utilities for layers</a><a id="Miscellaneous-utilities-for-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Miscellaneous-utilities-for-layers" title="Permalink"></a></h2><p>These are some miscellaneous utilities present in the <code>Layers</code> module, and are used with other custom/inbuilt layers to make certain common operations in neural networks easier.</p><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.inputscale" id="Metalhead.Layers.inputscale"><code>Metalhead.Layers.inputscale</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">inputscale(λ; activation = identity)</code></pre><p>Scale the input by a scalar <code>λ</code> and applies an activation function to it. Equivalent to <code>activation.(λ .* x)</code>.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/layers/scale.jl#L1-L6" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.actadd" id="Metalhead.Layers.actadd"><code>Metalhead.Layers.actadd</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">actadd(activation = relu, xs...)</code></pre><p>Convenience function for adding input arrays after applying an activation function to them. Useful as the <code>connection</code> argument for the block function in <a href="api/@ref"><code>Metalhead.resnet</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/utilities.jl#L21-L27" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.addact" id="Metalhead.Layers.addact"><code>Metalhead.Layers.addact</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">addact(activation = relu, xs...)</code></pre><p>Convenience function for applying an activation function to the output after summing up the input arrays. Useful as the <code>connection</code> argument for the block function in <a href="api/@ref"><code>Metalhead.resnet</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/utilities.jl#L12-L18" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.cat_channels" id="Metalhead.Layers.cat_channels"><code>Metalhead.Layers.cat_channels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">cat_channels(x, y, zs...)</code></pre><p>Concatenate <code>x</code> and <code>y</code> (and any <code>z</code>s) along the channel dimension (third dimension). Equivalent to <code>cat(x, y, zs...; dims=3)</code>. Convenient reduction operator for use with <code>Parallel</code>.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/utilities.jl#L30-L36" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.flatten_chains" id="Metalhead.Layers.flatten_chains"><code>Metalhead.Layers.flatten_chains</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">flatten_chains(m::Chain)
flatten_chains(m)</code></pre><p>Convenience function for traversing nested layers of a Chain object and flatten them into a single iterator.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/utilities.jl#L70-L76" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#Metalhead.Layers.swapdims" id="Metalhead.Layers.swapdims"><code>Metalhead.Layers.swapdims</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">swapdims(perm)</code></pre><p>Convenience function that returns a closure which permutes the dimensions of an array. <code>perm</code> is a vector or tuple specifying a permutation of the input dimensions. Equivalent to <code>permutedims(x, perm)</code>.</p></div><a class="docs-sourcelink" href="https://github.com/FluxML/Metalhead.jl/blob/4e5b8f16964468518eeb6eb8d7e5f85af4ecf959/src/utilities.jl#L42-L48" target="_blank">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../layers_intro/">« An introduction to the <code>Layers</code> module in Metalhead.jl</a><a class="docs-footer-nextpage" href="../utilities/">Model Utilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 10 August 2023 01:56">Thursday 10 August 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>