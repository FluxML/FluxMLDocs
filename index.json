[{"id":3,"pagetitle":"Welcome","title":"Flux: The Julia Machine Learning Library","ref":"/flux/stable/#Flux:-The-Julia-Machine-Learning-Library","content":" Flux: The Julia Machine Learning Library Flux is a library for machine learning. It comes \"batteries-included\" with many useful tools built in, but also lets you use the full power of the Julia language where you need it. We follow a few key principles: Doing the obvious thing . Flux has relatively few explicit APIs. Instead, writing down the mathematical form will work – and be fast. Extensible by default . Flux is written to be highly flexible while being performant. Extending Flux is as simple as using your own code as part of the model you want - it is all  high-level Julia code . Play nicely with others . Flux works well with unrelated Julia libraries from  images  to  differential equation solvers , rather than duplicating them."},{"id":4,"pagetitle":"Welcome","title":"Installation","ref":"/flux/stable/#Installation","content":" Installation Download  Julia 1.9  or later, preferably the current stable release. You can add Flux using Julia's package manager, by typing  ] add Flux  in the Julia prompt.  For Nvidia GPU support, you will also need to install the  CUDA  and the  cuDNN  packages. For AMD GPU support, install the  AMDGPU  package. For acceleration on Apple Silicon, install the  Metal  package."},{"id":5,"pagetitle":"Welcome","title":"Learning Flux","ref":"/flux/stable/#Learning-Flux","content":" Learning Flux The  quick start  page trains a simple neural network. This rest of the  guide  provides a from-scratch introduction to Flux's take on models and how they work, starting with  fitting a line . Once you understand these docs, congratulations, you also understand  Flux's source code , which is intended to be concise, legible and a good reference for more advanced concepts. There are some  tutorials  about building particular models. The  model zoo  has starting points for many other common ones. And finally, the  ecosystem page  lists packages which define Flux models. The  reference  section includes, beside Flux's own functions, those of some companion packages:  Zygote.jl  (automatic differentiation),  Optimisers.jl  (training) and others."},{"id":6,"pagetitle":"Welcome","title":"Community","ref":"/flux/stable/#Community","content":" Community Everyone is welcome to join our community on the  Julia discourse forum , or the  slack chat  (channel #machine-learning). If you have questions or issues we'll try to help you out. If you're interested in hacking on Flux, the  source code  is open and easy to understand – it's all just the same Julia code you work with normally. You might be interested in our  intro issues  to get started, or our  contributing guide ."},{"id":9,"pagetitle":"Batching Data – MLUtils.jl","title":"Working with Data, using MLUtils.jl","ref":"/flux/stable/data/mlutils/#Working-with-Data,-using-MLUtils.jl","content":" Working with Data, using MLUtils.jl Flux re-exports the  DataLoader  type and utility functions for working with data from  MLUtils ."},{"id":10,"pagetitle":"Batching Data – MLUtils.jl","title":"DataLoader","ref":"/flux/stable/data/mlutils/#DataLoader","content":" DataLoader The  DataLoader  can be used to create mini-batches of data, in the format  train!  expects. Flux 's website has a  dedicated tutorial  on  DataLoader  for more information. "},{"id":11,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.DataLoader","ref":"/flux/stable/data/mlutils/#MLUtils.DataLoader","content":" MLUtils.DataLoader  —  Type DataLoader(data; [batchsize, buffer, collate, parallel, partial, rng, shuffle]) An object that iterates over mini-batches of  data , each mini-batch containing  batchsize  observations (except possibly the last one). Takes as input a single data array, a tuple (or a named tuple) of arrays, or in general any  data  object that implements the  numobs  and  getobs  methods. The last dimension in each array is the observation dimension, i.e. the one divided into mini-batches. The original data is preserved in the  data  field of the DataLoader. Arguments data : The data to be iterated over. The data type has to be supported by  numobs  and  getobs . batchsize : If less than 0, iterates over individual observations. Otherwise, each iteration (except possibly the last) yields a mini-batch containing  batchsize  observations. Default  1 . buffer : If  buffer=true  and supported by the type of  data , a buffer will be allocated and reused for memory efficiency. You can also pass a preallocated object to  buffer . Default  false . collate : Batching behavior. If  nothing  (default), a batch is  getobs(data, indices) . If  false , each batch is   [getobs(data, i) for i in indices] . When  true , applies  batch  to the vector of observations in a batch,   recursively collating arrays in the last dimensions. See  batch  for more information and examples. parallel : Whether to use load data in parallel using worker threads. Greatly   speeds up data loading by factor of available threads. Requires starting   Julia with multiple threads. Check  Threads.nthreads()  to see the number of   available threads.  Passing  parallel = true  breaks ordering guarantees .   Default  false . partial : This argument is used only when  batchsize > 0 . If  partial=false  and the number of observations is not divisible by the batchsize, then the last mini-batch is dropped. Default  true . rng : A random number generator. Default  Random.GLOBAL_RNG . shuffle : Whether to shuffle the observations before iterating. Unlike   wrapping the data container with  shuffleobs(data) ,  shuffle=true  ensures   that the observations are shuffled anew every time you start iterating over    eachobs . Default  false . Examples julia> Xtrain = rand(10, 100);\n\njulia> array_loader = DataLoader(Xtrain, batchsize=2);\n\njulia> for x in array_loader\n         @assert size(x) == (10, 2)\n         # do something with x, 50 times\n       end\n\njulia> array_loader.data === Xtrain\ntrue\n\njulia> tuple_loader = DataLoader((Xtrain,), batchsize=2);  # similar, but yielding 1-element tuples\n\njulia> for x in tuple_loader\n         @assert x isa Tuple{Matrix}\n         @assert size(x[1]) == (10, 2)\n       end\n\njulia> Ytrain = rand('a':'z', 100);  # now make a DataLoader yielding 2-element named tuples\n\njulia> train_loader = DataLoader((data=Xtrain, label=Ytrain), batchsize=5, shuffle=true);\n\njulia> for epoch in 1:100\n         for (x, y) in train_loader  # access via tuple destructuring\n           @assert size(x) == (10, 5)\n           @assert size(y) == (5,)\n           # loss += f(x, y) # etc, runs 100 * 20 times\n         end\n       end\n\njulia> first(train_loader).label isa Vector{Char}  # access via property name\ntrue\n\njulia> first(train_loader).label == Ytrain[1:5]  # because of shuffle=true\nfalse\n\njulia> foreach(println∘summary, DataLoader(rand(Int8, 10, 64), batchsize=30))  # partial=false would omit last\n10×30 Matrix{Int8}\n10×30 Matrix{Int8}\n10×4 Matrix{Int8}"},{"id":12,"pagetitle":"Batching Data – MLUtils.jl","title":"Utility Functions","ref":"/flux/stable/data/mlutils/#Utility-Functions","content":" Utility Functions The utility functions are meant to be used while working with data; these functions help create inputs for your models or batch your dataset."},{"id":13,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.batch","ref":"/flux/stable/data/mlutils/#MLUtils.batch","content":" MLUtils.batch  —  Function batch(xs) Batch the arrays in  xs  into a single array with  an extra dimension. If the elements of  xs  are tuples, named tuples, or dicts,  the output will be of the same type.  See also  unbatch . Examples julia> batch([[1,2,3], \n              [4,5,6]])\n3×2 Matrix{Int64}:\n 1  4\n 2  5\n 3  6\n\njulia> batch([(a=[1,2], b=[3,4])\n               (a=[5,6], b=[7,8])]) \n(a = [1 5; 2 6], b = [3 7; 4 8])"},{"id":14,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.batchsize","ref":"/flux/stable/data/mlutils/#MLUtils.batchsize","content":" MLUtils.batchsize  —  Function batchsize(data) -> Int Return the fixed size of each batch in  data ."},{"id":15,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.batchseq","ref":"/flux/stable/data/mlutils/#MLUtils.batchseq","content":" MLUtils.batchseq  —  Function batchseq(seqs, val = 0) Take a list of  N  sequences, and turn them into a single sequence where each item is a batch of  N . Short sequences will be padded by  val . Examples julia> batchseq([[1, 2, 3], [4, 5]], 0)\n3-element Vector{Vector{Int64}}:\n [1, 4]\n [2, 5]\n [3, 0]"},{"id":16,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.BatchView","ref":"/flux/stable/data/mlutils/#MLUtils.BatchView","content":" MLUtils.BatchView  —  Type BatchView(data, batchsize; partial=true, collate=nothing)\nBatchView(data; batchsize=1, partial=true, collate=nothing) Create a view of the given  data  that represents it as a vector of batches. Each batch will contain an equal amount of observations in them. The batch-size can be specified using the  parameter  batchsize . In the case that the size of the dataset is not dividable by the specified  batchsize , the remaining observations will be ignored if  partial=false . If   partial=true  instead the last batch-size can be slightly smaller. Note that any data access is delayed until  getindex  is called. If used as an iterator, the object will iterate over the dataset once, effectively denoting an epoch. For  BatchView  to work on some data structure, the type of the given variable  data  must implement the data container interface. See  ObsView  for more info. Arguments data  : The object describing the dataset. Can be of any   type as long as it implements  getobs  and    numobs  (see Details for more information). batchsize  : The batch-size of each batch.   It is the number of observations that each batch must contain   (except possibly for the last one). partial  : If  partial=false  and the number of observations is   not divisible by the batch-size, then the last mini-batch is dropped. collate : Batching behavior. If  nothing  (default), a batch   is  getobs(data, indices) . If  false , each batch is    [getobs(data, i) for i in indices] . When  true , applies  batch    to the vector of observations in a batch, recursively collating   arrays in the last dimensions. See  batch  for more information   and examples. Examples using MLUtils\nX, Y = MLUtils.load_iris()\n\nA = BatchView(X, batchsize=30)\n@assert typeof(A) <: BatchView <: AbstractVector\n@assert eltype(A) <: SubArray{Float64,2}\n@assert length(A) == 5 # Iris has 150 observations\n@assert size(A[1]) == (4,30) # Iris has 4 features\n\n# 5 batches of size 30 observations\nfor x in BatchView(X, batchsize=30)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert numobs(x) === 30\nend\n\n# 7 batches of size 20 observations\n# Note that the iris dataset has 150 observations,\n# which means that with a batchsize of 20, the last\n# 10 observations will be ignored\nfor (x, y) in BatchView((X, Y), batchsize=20, partial=false)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\n    @assert numobs(x) == numobs(y) == 20\nend\n\n# collate tuple observations\nfor (x, y) in BatchView((rand(10, 3), [\"a\", \"b\", \"c\"]), batchsize=2, collate=true, partial=false)\n    @assert size(x) == (10, 2)\n    @assert size(y) == (2,)\nend\n\n\n# randomly assign observations to one and only one batch.\nfor (x, y) in BatchView(shuffleobs((X, Y)), batchsize=20)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\nend"},{"id":17,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.chunk","ref":"/flux/stable/data/mlutils/#MLUtils.chunk","content":" MLUtils.chunk  —  Function chunk(x, n; [dims])\nchunk(x; [size, dims]) Split  x  into  n  parts or alternatively, if  size  is an integer, into equal chunks of size  size .  The parts contain the same number of elements except possibly for the last one that can be smaller. In case  size  is a collection of integers instead, the elements of  x  are split into chunks of the given sizes. If  x  is an array,  dims  can be used to specify along which dimension to  split (defaults to the last dimension). Examples julia> chunk(1:10, 3)\n3-element Vector{UnitRange{Int64}}:\n 1:4\n 5:8\n 9:10\n\njulia> chunk(1:10; size = 2)\n5-element Vector{UnitRange{Int64}}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10\n\njulia> x = reshape(collect(1:20), (5, 4))\n5×4 Matrix{Int64}:\n 1   6  11  16\n 2   7  12  17\n 3   8  13  18\n 4   9  14  19\n 5  10  15  20\n\njulia> xs = chunk(x, 2, dims=1)\n2-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}}, false}}:\n [1 6 11 16; 2 7 12 17; 3 8 13 18]\n [4 9 14 19; 5 10 15 20]\n\njulia> xs[1]\n3×4 view(::Matrix{Int64}, 1:3, :) with eltype Int64:\n 1  6  11  16\n 2  7  12  17\n 3  8  13  18\n\njulia> xes = chunk(x; size = 2, dims = 2)\n2-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, UnitRange{Int64}}, true}}:\n [1 6; 2 7; … ; 4 9; 5 10]\n [11 16; 12 17; … ; 14 19; 15 20]\n\njulia> xes[2]\n5×2 view(::Matrix{Int64}, :, 3:4) with eltype Int64:\n 11  16\n 12  17\n 13  18\n 14  19\n 15  20\n\njulia> chunk(1:6; size = [2, 4])\n2-element Vector{UnitRange{Int64}}:\n 1:2\n 3:6 chunk(x, partition_idxs; [npartitions, dims]) Partition the array  x  along the dimension  dims  according to the indexes  in  partition_idxs . partition_idxs  must be sorted and contain only positive integers  between 1 and the number of partitions.  If the number of partition  npartitions  is not provided,  it is inferred from  partition_idxs . If  dims  is not provided, it defaults to the last dimension. See also  unbatch . Examples julia> x = reshape([1:10;], 2, 5)\n2×5 Matrix{Int64}:\n 1  3  5  7   9\n 2  4  6  8  10\n\njulia> chunk(x, [1, 2, 2, 3, 3])\n3-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, UnitRange{Int64}}, true}}:\n [1; 2;;]\n [3 5; 4 6]\n [7 9; 8 10]"},{"id":18,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.eachobs","ref":"/flux/stable/data/mlutils/#MLUtils.eachobs","content":" MLUtils.eachobs  —  Function eachobs(data; kws...) Return an iterator over  data . Supports the same arguments as  DataLoader . The  batchsize  default is  -1  here while it is  1  for  DataLoader . Examples X = rand(4,100)\n\nfor x in eachobs(X)\n    # loop entered 100 times\n    @assert typeof(x) <: Vector{Float64}\n    @assert size(x) == (4,)\nend\n\n# mini-batch iterations\nfor x in eachobs(X, batchsize=10)\n    # loop entered 10 times\n    @assert typeof(x) <: Matrix{Float64}\n    @assert size(x) == (4,10)\nend\n\n# support for tuples, named tuples, dicts\nfor (x, y) in eachobs((X, Y))\n    # ...\nend"},{"id":19,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.fill_like","ref":"/flux/stable/data/mlutils/#MLUtils.fill_like","content":" MLUtils.fill_like  —  Function fill_like(x, val, [element_type=eltype(x)], [dims=size(x)])) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to  val .  The third and fourth arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. See also  zeros_like  and  ones_like . Examples julia> x = rand(Float32, 2)\n2-element Vector{Float32}:\n 0.16087806\n 0.89916044\n\njulia> fill_like(x, 1.7, (3, 3))\n3×3 Matrix{Float32}:\n 1.7  1.7  1.7\n 1.7  1.7  1.7\n 1.7  1.7  1.7\n\njulia> using CUDA\n\njulia> x = CUDA.rand(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 0.803167  0.476101\n 0.303041  0.317581\n\njulia> fill_like(x, 1.7, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n 1.7  1.7\n 1.7  1.7"},{"id":20,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.filterobs","ref":"/flux/stable/data/mlutils/#MLUtils.filterobs","content":" MLUtils.filterobs  —  Function filterobs(f, data) Return a subset of data container  data  including all indices  i  for which  f(getobs(data, i)) === true . data = 1:10\nnumobs(data) == 10\nfdata = filterobs(>(5), data)\nnumobs(fdata) == 5"},{"id":21,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.flatten","ref":"/flux/stable/data/mlutils/#MLUtils.flatten","content":" MLUtils.flatten  —  Function flatten(x::AbstractArray) Reshape arbitrarly-shaped input into a matrix-shaped output, preserving the size of the last dimension. See also  unsqueeze . Examples julia> rand(3,4,5) |> flatten |> size\n(12, 5)"},{"id":22,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.getobs","ref":"/flux/stable/data/mlutils/#MLUtils.getobs","content":" MLUtils.getobs  —  Function getobs(data, [idx]) Return the observations corresponding to the observation index  idx . Note that  idx  can be any type as long as  data  has defined  getobs  for that type. If  idx  is not provided, then materialize all observations in  data . If  data  does not have  getobs  defined, then in the case of  Tables.table(data) == true  returns the row(s) in position  idx , otherwise returns  data[idx] . Authors of custom data containers should implement  Base.getindex  for their type instead of  getobs .  getobs  should only be implemented for types where there is a difference between  getobs  and  Base.getindex  (such as multi-dimensional arrays). The returned observation(s) should be in the form intended to be passed as-is to some learning algorithm. There is no strict interface requirement on how this \"actual data\" must look like. Every author behind some custom data container can make this decision themselves. The output should be consistent when  idx  is a scalar vs vector. getobs  supports by default nested combinations of array, tuple, named tuples, and dictionaries.  See also  getobs!  and  numobs . Examples # named tuples \nx = (a = [1, 2, 3], b = rand(6, 3))\n\ngetobs(x, 2) == (a = 2, b = x.b[:, 2])\ngetobs(x, [1, 3]) == (a = [1, 3], b = x.b[:, [1, 3]])\n\n\n# dictionaries\nx = Dict(:a => [1, 2, 3], :b => rand(6, 3))\n\ngetobs(x, 2) == Dict(:a => 2, :b => x[:b][:, 2])\ngetobs(x, [1, 3]) == Dict(:a => [1, 3], :b => x[:b][:, [1, 3]])"},{"id":23,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.getobs!","ref":"/flux/stable/data/mlutils/#MLUtils.getobs!","content":" MLUtils.getobs!  —  Function getobs!(buffer, data, idx) Inplace version of  getobs(data, idx) . If this method is defined for the type of  data , then  buffer  should be used to store the result, instead of allocating a dedicated object. Implementing this function is optional. In the case no such method is provided for the type of  data , then  buffer  will be  ignored  and the result of  getobs  returned. This could be because the type of  data  may not lend itself to the concept of  copy! . Thus, supporting a custom  getobs!  is optional and not required. See also  getobs  and  numobs . "},{"id":24,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.joinobs","ref":"/flux/stable/data/mlutils/#MLUtils.joinobs","content":" MLUtils.joinobs  —  Function joinobs(datas...) Concatenate data containers  datas . data1, data2 = 1:10, 11:20\njdata = joinumobs(data1, data2)\ngetobs(jdata, 15) == 15"},{"id":25,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.group_counts","ref":"/flux/stable/data/mlutils/#MLUtils.group_counts","content":" MLUtils.group_counts  —  Function group_counts(x) Count the number of times that each element of  x  appears. See also  group_indices Examples julia> group_counts(['a', 'b', 'b'])\nDict{Char, Int64} with 2 entries:\n  'a' => 1\n  'b' => 2"},{"id":26,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.group_indices","ref":"/flux/stable/data/mlutils/#MLUtils.group_indices","content":" MLUtils.group_indices  —  Function group_indices(x) -> Dict Computes the indices of elements in the vector  x  for each distinct value contained.  This information is useful for resampling strategies, such as stratified sampling. See also  group_counts . Examples julia> x = [:yes, :no, :maybe, :yes];\n\njulia> group_indices(x)\nDict{Symbol, Vector{Int64}} with 3 entries:\n  :yes   => [1, 4]\n  :maybe => [3]\n  :no    => [2]"},{"id":27,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.groupobs","ref":"/flux/stable/data/mlutils/#MLUtils.groupobs","content":" MLUtils.groupobs  —  Function groupobs(f, data) Split data container data  data  into different data containers, grouping observations by  f(obs) . data = -10:10\ndatas = groupobs(>(0), data)\nlength(datas) == 2"},{"id":28,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.kfolds","ref":"/flux/stable/data/mlutils/#MLUtils.kfolds","content":" MLUtils.kfolds  —  Function kfolds(n::Integer, k = 5) -> Tuple Compute the train/validation assignments for  k  repartitions of  n  observations, and return them in the form of two vectors. The first vector contains the index-vectors for the training subsets, and the second vector the index-vectors for the validation subsets respectively. A general rule of thumb is to use either  k = 5  or  k = 10 . The following code snippet generates the indices assignments for  k = 5 julia> train_idx, val_idx = kfolds(10, 5); Each observation is assigned to the validation subset once (and only once). Thus, a union over all validation index-vectors reproduces the full range  1:n . Note that there is no random assignment of observations to subsets, which means that adjacent observations are likely to be part of the same validation subset. julia> train_idx\n5-element Array{Array{Int64,1},1}:\n [3,4,5,6,7,8,9,10]\n [1,2,5,6,7,8,9,10]\n [1,2,3,4,7,8,9,10]\n [1,2,3,4,5,6,9,10]\n [1,2,3,4,5,6,7,8]\n\njulia> val_idx\n5-element Array{UnitRange{Int64},1}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10 kfolds(data, [k = 5]) Repartition a  data  container  k  times using a  k  folds strategy and return the sequence of folds as a lazy iterator.  Only data subsets are created, which means that no actual data is copied until  getobs  is invoked. Conceptually, a k-folds repartitioning strategy divides the given  data  into  k  roughly equal-sized parts. Each part will serve as validation set once, while the remaining parts are used for training. This results in  k  different partitions of  data . In the case that the size of the dataset is not dividable by the specified  k , the remaining observations will be evenly distributed among the parts. for (x_train, x_val) in kfolds(X, k=10)\n    # code called 10 times\n    # nobs(x_val) may differ up to ±1 over iterations\nend Multiple variables are supported (e.g. for labeled data) for ((x_train, y_train), val) in kfolds((X, Y), k=10)\n    # ...\nend By default the folds are created using static splits. Use  shuffleobs  to randomly assign observations to the folds. for (x_train, x_val) in kfolds(shuffleobs(X), k = 10)\n    # ...\nend See  leavepout  for a related function."},{"id":29,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.leavepout","ref":"/flux/stable/data/mlutils/#MLUtils.leavepout","content":" MLUtils.leavepout  —  Function leavepout(n::Integer, [size = 1]) -> Tuple Compute the train/validation assignments for  k ≈ n/size  repartitions of  n  observations, and return them in the form of two vectors. The first vector contains the index-vectors for the training subsets, and the second vector the index-vectors for the validation subsets respectively. Each validation subset will have either  size  or  size+1  observations assigned to it. The following code snippet generates the index-vectors for  size = 2 . julia> train_idx, val_idx = leavepout(10, 2); Each observation is assigned to the validation subset once (and only once). Thus, a union over all validation index-vectors reproduces the full range  1:n . Note that there is no random assignment of observations to subsets, which means that adjacent observations are likely to be part of the same validation subset. julia> train_idx\n5-element Array{Array{Int64,1},1}:\n [3,4,5,6,7,8,9,10]\n [1,2,5,6,7,8,9,10]\n [1,2,3,4,7,8,9,10]\n [1,2,3,4,5,6,9,10]\n [1,2,3,4,5,6,7,8]\n\njulia> val_idx\n5-element Array{UnitRange{Int64},1}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10 leavepout(data, p = 1) Repartition a  data  container using a k-fold strategy, where  k  is chosen in such a way, that each validation subset of the resulting folds contains roughly  p  observations. Defaults to  p = 1 , which is also known as \"leave-one-out\" partitioning. The resulting sequence of folds is returned as a lazy iterator. Only data subsets are created. That means no actual data is copied until  getobs  is invoked. for (train, val) in leavepout(X, p=2)\n    # if nobs(X) is dividable by 2,\n    # then numobs(val) will be 2 for each iteraton,\n    # otherwise it may be 3 for the first few iterations.\nend See kfolds  for a related function."},{"id":30,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.mapobs","ref":"/flux/stable/data/mlutils/#MLUtils.mapobs","content":" MLUtils.mapobs  —  Function mapobs(f, data; batched=:auto) Lazily map  f  over the observations in a data container  data . Returns a new data container  mdata  that can be indexed and has a length. Indexing triggers the transformation  f . The batched keyword argument controls the behavior of  mdata[idx]  and  mdata[idxs]   where  idx  is an integer and  idxs  is a vector of integers: batched=:auto  (default). Let  f  handle the two cases.   Calls  f(getobs(data, idx))  and  f(getobs(data, idxs)) . batched=:never . The function  f  is always called on a single observation.   Calls  f(getobs(data, idx))  and  [f(getobs(data, idx)) for idx in idxs] . batched=:always . The function  f  is always called on a batch of observations.   Calls  getobs(f(getobs(data, [idx])), 1)  and  f(getobs(data, idxs)) . Examples julia> data = (a=[1,2,3], b=[1,2,3]);\n\njulia> mdata = mapobs(data) do x\n         (c = x.a .+ x.b,  d = x.a .- x.b)\n       end\nmapobs(#25, (a = [1, 2, 3], b = [1, 2, 3]); batched=:auto))\n\njulia> mdata[1]\n(c = 2, d = 0)\n\njulia> mdata[1:2]\n(c = [2, 4], d = [0, 0]) mapobs(fs, data) Lazily map each function in tuple  fs  over the observations in data container  data . Returns a tuple of transformed data containers. mapobs(namedfs::NamedTuple, data) Map a  NamedTuple  of functions over  data , turning it into a data container of  NamedTuple s. Field syntax can be used to select a column of the resulting data container. data = 1:10\nnameddata = mapobs((x = sqrt, y = log), data)\ngetobs(nameddata, 10) == (x = sqrt(10), y = log(10))\ngetobs(nameddata.x, 10) == sqrt(10)"},{"id":31,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.numobs","ref":"/flux/stable/data/mlutils/#MLUtils.numobs","content":" MLUtils.numobs  —  Function numobs(data) Return the total number of observations contained in  data . If  data  does not have  numobs  defined,  then in the case of  Tables.table(data) == true  returns the number of rows, otherwise returns  length(data) . Authors of custom data containers should implement  Base.length  for their type instead of  numobs .  numobs  should only be implemented for types where there is a difference between  numobs  and  Base.length  (such as multi-dimensional arrays). getobs  supports by default nested combinations of array, tuple, named tuples, and dictionaries.  See also  getobs . Examples \n# named tuples \nx = (a = [1, 2, 3], b = rand(6, 3))\nnumobs(x) == 3\n\n# dictionaries\nx = Dict(:a => [1, 2, 3], :b => rand(6, 3))\nnumobs(x) == 3 All internal containers must have the same number of observations: julia> x = (a = [1, 2, 3, 4], b = rand(6, 3));\n\njulia> numobs(x)\nERROR: DimensionMismatch: All data containers must have the same number of observations.\nStacktrace:\n [1] _check_numobs_error()\n   @ MLUtils ~/.julia/dev/MLUtils/src/observation.jl:163\n [2] _check_numobs\n   @ ~/.julia/dev/MLUtils/src/observation.jl:130 [inlined]\n [3] numobs(data::NamedTuple{(:a, :b), Tuple{Vector{Int64}, Matrix{Float64}}})\n   @ MLUtils ~/.julia/dev/MLUtils/src/observation.jl:177\n [4] top-level scope\n   @ REPL[35]:1"},{"id":32,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.normalise","ref":"/flux/stable/data/mlutils/#MLUtils.normalise","content":" MLUtils.normalise  —  Function normalise(x; dims=ndims(x), ϵ=1e-5) Normalise the array  x  to mean 0 and standard deviation 1 across the dimension(s) given by  dims . Per default,  dims  is the last dimension.  ϵ  is a small additive factor added to the denominator for numerical stability."},{"id":33,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.obsview","ref":"/flux/stable/data/mlutils/#MLUtils.obsview","content":" MLUtils.obsview  —  Function obsview(data, [indices]) Returns a lazy view of the observations in  data  that correspond to the given  indices . No data will be copied except of the indices. It is similar to constructing an  ObsView ,  but returns a  SubArray  if the type of  data  is  Array  or  SubArray . Furthermore, this function may be extended for custom types of  data  that also want to provide their own subset-type. In case  data  is a tuple, the constructor will be mapped over its elements. That means that the constructor returns a tuple of  ObsView  instead of a  ObsView  of tuples. If instead you want to get the subset of observations corresponding to the given  indices  in their native type, use  getobs . See  ObsView  for more information."},{"id":34,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.ObsView","ref":"/flux/stable/data/mlutils/#MLUtils.ObsView","content":" MLUtils.ObsView  —  Type ObsView(data, [indices]) Used to represent a subset of some  data  of arbitrary type by storing which observation-indices the subset spans. Furthermore, subsequent subsettings are accumulated without needing to access actual data. The main purpose for the existence of  ObsView  is to delay data access and movement until an actual batch of data (or single observation) is needed for some computation. This is particularily useful when the data is not located in memory, but on the hard drive or some remote location. In such a scenario one wants to load the required data only when needed. Any data access is delayed until  getindex  is called,  and even  getindex  returns the result of  obsview  which in general avoids data movement until  getobs  is called. If used as an iterator, the view will iterate over the dataset once, effectively denoting an epoch. Each iteration will return a lazy subset to the current observation. Arguments data  : The object describing the dataset. Can be of any   type as long as it implements  getobs  and    numobs  (see Details for more information). indices  : Optional. The index or indices of the   observation(s) in  data  that the subset should represent.   Can be of type  Int  or some subtype of  AbstractVector . Methods getindex  : Returns the observation(s) of the given   index/indices. No data is copied aside   from the required indices. numobs  : Returns the total number observations in the subset. getobs  : Returns the underlying data that the    ObsView  represents at the given relative indices. Note   that these indices are in \"subset space\", and in general will   not directly correspond to the same indices in the underlying   data set. Details For  ObsView  to work on some data structure, the desired type  MyType  must implement the following interface: getobs(data::MyType, idx)  :   Should return the observation(s) indexed by  idx .   In what form is up to the user.   Note that  idx  can be of type  Int  or  AbstractVector . numobs(data::MyType)  :   Should return the total number of observations in  data The following methods can also be provided and are optional: getobs(data::MyType)  :   By default this function is the identity function.   If that is not the behaviour that you want for your type,   you need to provide this method as well. obsview(data::MyType, idx)  :   If your custom type has its own kind of subset type, you can   return it here. An example for such a case are  SubArray  for   representing a subset of some  AbstractArray . getobs!(buffer, data::MyType, [idx])  :   Inplace version of  getobs(data, idx) . If this method   is provided for  MyType , then  eachobs  can preallocate a buffer that is then reused   every iteration. Note:  buffer  should be equivalent to the   return value of  getobs(::MyType, ...) , since this is how    buffer  is preallocated by default. Examples X, Y = MLUtils.load_iris()\n\n# The iris set has 150 observations and 4 features\n@assert size(X) == (4,150)\n\n# Represents the 80 observations as a ObsView\nv = ObsView(X, 21:100)\n@assert numobs(v) == 80\n@assert typeof(v) <: ObsView\n# getobs indexes into v\n@assert getobs(v, 1:10) == X[:, 21:30]\n\n# Use `obsview` to avoid boxing into ObsView\n# for types that provide a custom \"subset\", such as arrays.\n# Here it instead creates a native SubArray.\nv = obsview(X, 1:100)\n@assert numobs(v) == 100\n@assert typeof(v) <: SubArray\n\n# Also works for tuples of arbitrary length\nsubset = obsview((X, Y), 1:100)\n@assert numobs(subset) == 100\n@assert typeof(subset) <: Tuple # tuple of SubArray\n\n# Use as iterator\nfor x in ObsView(X)\n    @assert typeof(x) <: SubArray{Float64,1}\nend\n\n# iterate over each individual labeled observation\nfor (x, y) in ObsView((X, Y))\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert typeof(y) <: String\nend\n\n# same but in random order\nfor (x, y) in ObsView(shuffleobs((X, Y)))\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert typeof(y) <: String\nend\n\n# Indexing: take first 10 observations\nx, y = ObsView((X, Y))[1:10] See also obsview ,   getobs ,  numobs ,  splitobs ,  shuffleobs ,  kfolds ."},{"id":35,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.ones_like","ref":"/flux/stable/data/mlutils/#MLUtils.ones_like","content":" MLUtils.ones_like  —  Function ones_like(x, [element_type=eltype(x)], [dims=size(x)])) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to 1.  The second and third arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. See also  zeros_like  and  fill_like . Examples julia> x = rand(Float32, 2)\n2-element Vector{Float32}:\n 0.8621633\n 0.5158395\n\njulia> ones_like(x, (3, 3))\n3×3 Matrix{Float32}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia> using CUDA\n\njulia> x = CUDA.rand(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 0.82297   0.656143\n 0.701828  0.391335\n\njulia> ones_like(x, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n 1.0  1.0\n 1.0  1.0"},{"id":36,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.oversample","ref":"/flux/stable/data/mlutils/#MLUtils.oversample","content":" MLUtils.oversample  —  Function oversample(data, classes; fraction=1, shuffle=true)\noversample(data::Tuple; fraction=1, shuffle=true) Generate a re-balanced version of  data  by repeatedly sampling existing observations in such a way that every class will have at least  fraction  times the number observations of the largest class in  classes . This way, all classes will have a minimum number of observations in the resulting data set relative to what largest class has in the given (original)  data . As an example, by default (i.e. with  fraction = 1 ) the resulting dataset will be near perfectly balanced. On the other hand, with  fraction = 0.5  every class in the resulting data with have at least 50% as many observations as the largest class. The  classes  input is an array with the same length as  numobs(data) .   The convenience parameter  shuffle  determines if the resulting data will be shuffled after its creation; if it is not shuffled then all the repeated samples will be together at the end, sorted by class. Defaults to  true . The output will contain both the resampled data and classes. # 6 observations with 3 features each\nX = rand(3, 6)\n# 2 classes, severely imbalanced\nY = [\"a\", \"b\", \"b\", \"b\", \"b\", \"a\"]\n\n# oversample the class \"a\" to match \"b\"\nX_bal, Y_bal = oversample(X, Y)\n\n# this results in a bigger dataset with repeated data\n@assert size(X_bal) == (3,8)\n@assert length(Y_bal) == 8\n\n# now both \"a\", and \"b\" have 4 observations each\n@assert sum(Y_bal .== \"a\") == 4\n@assert sum(Y_bal .== \"b\") == 4 For this function to work, the type of  data  must implement  numobs  and  getobs .  Note that if  data  is a tuple and  classes  is not given,  then it will be assumed that the last element of the tuple contains the classes. julia> data = DataFrame(X1=rand(6), X2=rand(6), Y=[:a,:b,:b,:b,:b,:a])\n6×3 DataFrames.DataFrame\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.933372  │ 0.812814    │ b │\n│ 4   │ 0.522172  │ 0.245457    │ b │\n│ 5   │ 0.505208  │ 0.11202     │ b │\n│ 6   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> getobs(oversample(data, data.Y))\n8×3 DataFrame\n Row │ X1        X2         Y      \n     │ Float64   Float64    Symbol \n─────┼─────────────────────────────\n   1 │ 0.376304  0.100022   a\n   2 │ 0.467095  0.185437   b\n   3 │ 0.481957  0.319906   b\n   4 │ 0.336762  0.390811   b\n   5 │ 0.376304  0.100022   a\n   6 │ 0.427064  0.0648339  a\n   7 │ 0.427064  0.0648339  a\n   8 │ 0.457043  0.490688   b See  ObsView  for more information on data subsets. See also  undersample ."},{"id":37,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.randobs","ref":"/flux/stable/data/mlutils/#MLUtils.randobs","content":" MLUtils.randobs  —  Function randobs(data, [n]) Pick a random observation or a batch of  n  random observations from  data . For this function to work, the type of  data  must implement  numobs  and  getobs ."},{"id":38,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.rand_like","ref":"/flux/stable/data/mlutils/#MLUtils.rand_like","content":" MLUtils.rand_like  —  Function rand_like([rng=default_rng()], x, [element_type=eltype(x)], [dims=size(x)]) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to a random value. The last two arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. The default random number generator is used, unless a custom one is passed in explicitly as the first argument. See also  Base.rand  and  randn_like . Examples julia> x = ones(Float32, 2)\n2-element Vector{Float32}:\n 1.0\n 1.0\n\njulia> rand_like(x, (3, 3))\n3×3 Matrix{Float32}:\n 0.780032  0.920552  0.53689\n 0.121451  0.741334  0.5449\n 0.55348   0.138136  0.556404\n\njulia> using CUDA\n\njulia> CUDA.ones(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 1.0  1.0\n 1.0  1.0\n\njulia> rand_like(x, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n 0.429274  0.135379\n 0.718895  0.0098756"},{"id":39,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.randn_like","ref":"/flux/stable/data/mlutils/#MLUtils.randn_like","content":" MLUtils.randn_like  —  Function randn_like([rng=default_rng()], x, [element_type=eltype(x)], [dims=size(x)]) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to a random value drawn from a normal distribution. The last two arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. The default random number generator is used, unless a custom one is passed in explicitly as the first argument. See also  Base.randn  and  rand_like . Examples julia> x = ones(Float32, 2)\n2-element Vector{Float32}:\n 1.0\n 1.0\n\njulia> randn_like(x, (3, 3))\n3×3 Matrix{Float32}:\n -0.385331    0.956231   0.0745102\n  1.43756    -0.967328   2.06311\n  0.0482372   1.78728   -0.902547\n\njulia> using CUDA\n\njulia> CUDA.ones(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 1.0  1.0\n 1.0  1.0\n\njulia> randn_like(x, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n -0.578527   0.823445\n -1.01338   -0.612053"},{"id":40,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.rpad_constant","ref":"/flux/stable/data/mlutils/#MLUtils.rpad_constant","content":" MLUtils.rpad_constant  —  Function rpad_constant(v::AbstractArray, n::Union{Integer, Tuple}, val = 0; dims=:) Return the given sequence padded with  val  along the dimensions  dims  up to a maximum length in each direction specified by  n . Examples julia> rpad_constant([1, 2], 4, -1) # passing with -1 up to size 4\n4-element Vector{Int64}:\n 1\n 2\n -1\n -1\n\njulia> rpad_constant([1, 2, 3], 2) # no padding if length is already greater than n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> rpad_constant([1 2; 3 4], 4; dims=1) # padding along the first dimension\n4×2 Matrix{Int64}:\n 1  2\n 3  4\n 0  0\n 0  0 \n\njulia> rpad_constant([1 2; 3 4], 4) # padding along all dimensions by default\n4×2 Matrix{Int64}:\n 1  2\n 3  4\n 0  0\n 0  0 "},{"id":41,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.shuffleobs","ref":"/flux/stable/data/mlutils/#MLUtils.shuffleobs","content":" MLUtils.shuffleobs  —  Function shuffleobs([rng], data) Return a \"subset\" of  data  that spans all observations, but has the order of the observations shuffled. The values of  data  itself are not copied. Instead only the indices are shuffled. This function calls  obsview  to accomplish that, which means that the return value is likely of a different type than  data . # For Arrays the subset will be of type SubArray\n@assert typeof(shuffleobs(rand(4,10))) <: SubArray\n\n# Iterate through all observations in random order\nfor x in eachobs(shuffleobs(X))\n    ...\nend The optional parameter  rng  allows one to specify the random number generator used for shuffling. This is useful when reproducible results are desired. By default, uses the global RNG. See  Random  in Julia's standard library for more info. For this function to work, the type of  data  must implement  numobs  and  getobs . See  ObsView  for more information."},{"id":42,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.splitobs","ref":"/flux/stable/data/mlutils/#MLUtils.splitobs","content":" MLUtils.splitobs  —  Function splitobs(n::Int; at) -> Tuple Compute the indices for two or more disjoint subsets of the range  1:n  with splits given by  at . Examples julia> splitobs(100, at=0.7)\n(1:70, 71:100)\n\njulia> splitobs(100, at=(0.1, 0.4))\n(1:10, 11:50, 51:100) splitobs(data; at, shuffle=false) -> Tuple Split the  data  into multiple subsets proportional to the value(s) of  at .  If  shuffle=true , randomly permute the observations before splitting. Supports any datatype implementing the  numobs  and  getobs  interfaces. Examples # A 70%-30% split\ntrain, test = splitobs(X, at=0.7)\n\n# A 50%-30%-20% split\ntrain, val, test = splitobs(X, at=(0.5, 0.3))\n\n# A 70%-30% split with multiple arrays and shuffling\ntrain, test = splitobs((X, y), at=0.7, shuffle=true)\nXtrain, Ytrain = train"},{"id":43,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.unbatch","ref":"/flux/stable/data/mlutils/#MLUtils.unbatch","content":" MLUtils.unbatch  —  Function unbatch(x) Reverse of the  batch  operation, unstacking the last dimension of the array  x . See also  unstack  and  chunk . Examples julia> unbatch([1 3 5 7;\n                2 4 6 8])\n4-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n [7, 8]"},{"id":44,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.undersample","ref":"/flux/stable/data/mlutils/#MLUtils.undersample","content":" MLUtils.undersample  —  Function undersample(data, classes; shuffle=true) Generate a class-balanced version of  data  by subsampling its observations in such a way that the resulting number of observations will be the same number for every class. This way, all classes will have as many observations in the resulting data set as the smallest class has in the given (original)  data . The convenience parameter  shuffle  determines if the resulting data will be shuffled after its creation; if it is not shuffled then all the observations will be in their original order. Defaults to  false . The output will contain both the resampled data and classes. # 6 observations with 3 features each\nX = rand(3, 6)\n# 2 classes, severely imbalanced\nY = [\"a\", \"b\", \"b\", \"b\", \"b\", \"a\"]\n\n# subsample the class \"b\" to match \"a\"\nX_bal, Y_bal = undersample(X, Y)\n\n# this results in a smaller dataset\n@assert size(X_bal) == (3,4)\n@assert length(Y_bal) == 4\n\n# now both \"a\", and \"b\" have 2 observations each\n@assert sum(Y_bal .== \"a\") == 2\n@assert sum(Y_bal .== \"b\") == 2 For this function to work, the type of  data  must implement  numobs  and  getobs .  Note that if  data  is a tuple, then it will be assumed that the last element of the tuple contains the targets. julia> data = DataFrame(X1=rand(6), X2=rand(6), Y=[:a,:b,:b,:b,:b,:a])\n6×3 DataFrames.DataFrame\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.933372  │ 0.812814    │ b │\n│ 4   │ 0.522172  │ 0.245457    │ b │\n│ 5   │ 0.505208  │ 0.11202     │ b │\n│ 6   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> getobs(undersample(data, data.Y))\n4×3 DataFrame\n Row │ X1        X2         Y      \n     │ Float64   Float64    Symbol \n─────┼─────────────────────────────\n   1 │ 0.427064  0.0648339  a\n   2 │ 0.376304  0.100022   a\n   3 │ 0.467095  0.185437   b\n   4 │ 0.457043  0.490688   b See  ObsView  for more information on data subsets. See also  oversample ."},{"id":45,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.unsqueeze","ref":"/flux/stable/data/mlutils/#MLUtils.unsqueeze","content":" MLUtils.unsqueeze  —  Function unsqueeze(x; dims) Return  x  reshaped into an array one dimensionality higher than  x , where  dims  indicates in which dimension  x  is extended.  dims  can be an integer between 1 and  ndims(x)+1 . See also  flatten ,  stack . Examples julia> unsqueeze([1 2; 3 4], dims=2)\n2×1×2 Array{Int64, 3}:\n[:, :, 1] =\n 1\n 3\n\n[:, :, 2] =\n 2\n 4\n\n\njulia> xs = [[1, 2], [3, 4], [5, 6]]\n3-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n\njulia> unsqueeze(xs, dims=1)\n1×3 Matrix{Vector{Int64}}:\n [1, 2]  [3, 4]  [5, 6] unsqueeze(; dims) Returns a function which, acting on an array, inserts a dimension of size 1 at  dims . Examples julia> rand(21, 22, 23) |> unsqueeze(dims=2) |> size\n(21, 1, 22, 23)"},{"id":46,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.unstack","ref":"/flux/stable/data/mlutils/#MLUtils.unstack","content":" MLUtils.unstack  —  Function unstack(xs; dims) Unroll the given  xs  into an array of arrays along the given dimension  dims . See also  stack ,  unbatch , and  chunk . Examples julia> unstack([1 3 5 7; 2 4 6 8], dims=2)\n4-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n [7, 8]"},{"id":47,"pagetitle":"Batching Data – MLUtils.jl","title":"MLUtils.zeros_like","ref":"/flux/stable/data/mlutils/#MLUtils.zeros_like","content":" MLUtils.zeros_like  —  Function zeros_like(x, [element_type=eltype(x)], [dims=size(x)])) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to 0.  The second and third arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. See also  ones_like  and  fill_like . Examples julia> x = rand(Float32, 2)\n2-element Vector{Float32}:\n 0.4005432\n 0.36934233\n\njulia> zeros_like(x, (3, 3))\n3×3 Matrix{Float32}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia> using CUDA\n\njulia> x = CUDA.rand(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 0.0695155  0.667979\n 0.558468   0.59903\n\njulia> zeros_like(x, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n 0.0  0.0\n 0.0  0.0"},{"id":50,"pagetitle":"OneHotArrays.jl","title":"One-Hot Encoding with OneHotArrays.jl","ref":"/flux/stable/data/onehot/#One-Hot-Encoding-with-OneHotArrays.jl","content":" One-Hot Encoding with OneHotArrays.jl It's common to encode categorical variables (like  true ,  false  or  cat ,  dog ) in \"one-of-k\" or  \"one-hot\"  form.  OneHotArrays.jl  provides the  onehot  function to make this easy. julia> using OneHotArrays\n\njulia> onehot(:b, [:a, :b, :c])\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n 1\n ⋅\n\njulia> onehot(:c, [:a, :b, :c])\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n ⋅\n 1 There is also a  onecold  function, which is an inverse of  onehot . It can also be given an array of numbers instead of booleans, in which case it performs an  argmax -like operation, returning the label with the highest corresponding weight. julia> onecold(ans, [:a, :b, :c])\n:c\n\njulia> onecold([true, false, false], [:a, :b, :c])\n:a\n\njulia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\n:c For multiple samples at once,  onehotbatch  creates a batch (matrix) of one-hot vectors, and  onecold  treats matrices as batches. julia> using OneHotArrays\n\njulia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n3×3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n ⋅  1  ⋅\n 1  ⋅  1\n ⋅  ⋅  ⋅\n\njulia> onecold(ans, [:a, :b, :c])\n3-element Vector{Symbol}:\n :b\n :a\n :b Note that these operations returned  OneHotVector  and  OneHotMatrix  rather than  Array s.  OneHotVector s behave like normal vectors but avoid any unnecessary cost compared to using an integer index directly. For example, multiplying a matrix with a one-hot vector simply slices out the relevant row of the matrix under the hood."},{"id":51,"pagetitle":"OneHotArrays.jl","title":"Function listing","ref":"/flux/stable/data/onehot/#Function-listing","content":" Function listing"},{"id":52,"pagetitle":"OneHotArrays.jl","title":"OneHotArrays.onehot","ref":"/flux/stable/data/onehot/#OneHotArrays.onehot","content":" OneHotArrays.onehot  —  Function onehot(x, labels, [default]) Returns a  OneHotVector  which is roughly a sparse representation of  x .== labels . Instead of storing say  Vector{Bool} , it stores the index of the first occurrence  of  x  in  labels . If  x  is not found in labels, then it either returns  onehot(default, labels) , or gives an error if no default is given. See also  onehotbatch  to apply this to many  x s,  and  onecold  to reverse either of these, as well as to generalise  argmax . Examples julia> β = onehot(:b, (:a, :b, :c))\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n 1\n ⋅\n\njulia> αβγ = (onehot(0, 0:2), β, onehot(:z, [:a, :b, :c], :c))  # uses default\n(Bool[1, 0, 0], Bool[0, 1, 0], Bool[0, 0, 1])\n\njulia> hcat(αβγ...)  # preserves sparsity\n3×3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅\n ⋅  1  ⋅\n ⋅  ⋅  1"},{"id":53,"pagetitle":"OneHotArrays.jl","title":"OneHotArrays.onecold","ref":"/flux/stable/data/onehot/#OneHotArrays.onecold","content":" OneHotArrays.onecold  —  Function onecold(y::AbstractArray, labels = 1:size(y,1)) Roughly the inverse operation of  onehot  or  onehotbatch :  This finds the index of the largest element of  y , or each column of  y ,  and looks them up in  labels . If  labels  are not specified, the default is integers  1:size(y,1)  – the same operation as  argmax(y, dims=1)  but sometimes a different return type. Examples julia> onecold([false, true, false])\n2\n\njulia> onecold([0.3, 0.2, 0.5], (:a, :b, :c))\n:c\n\njulia> onecold([ 1  0  0  1  0  1  0  1  0  0  1\n                 0  1  0  0  0  0  0  0  1  0  0\n                 0  0  0  0  1  0  0  0  0  0  0\n                 0  0  0  0  0  0  1  0  0  0  0\n                 0  0  1  0  0  0  0  0  0  1  0 ], 'a':'e') |> String\n\"abeacadabea\""},{"id":54,"pagetitle":"OneHotArrays.jl","title":"OneHotArrays.onehotbatch","ref":"/flux/stable/data/onehot/#OneHotArrays.onehotbatch","content":" OneHotArrays.onehotbatch  —  Function onehotbatch(xs, labels, [default]) Returns a  OneHotMatrix  where  k th column of the matrix is  onehot(xs[k], labels) . This is a sparse matrix, which stores just a  Vector{UInt32}  containing the indices of the nonzero elements. If one of the inputs in  xs  is not found in  labels , that column is  onehot(default, labels)  if  default  is given, else an error. If  xs  has more dimensions,  N = ndims(xs) > 1 , then the result is an   AbstractArray{Bool, N+1}  which is one-hot along the first dimension,  i.e.  result[:, k...] == onehot(xs[k...], labels) . Note that  xs  can be any iterable, such as a string. And that using a tuple for  labels  will often speed up construction, certainly for less than 32 classes. Examples julia> oh = onehotbatch(\"abracadabra\", 'a':'e', 'e')\n5×11 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  1\n ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n\njulia> reshape(1:15, 3, 5) * oh  # this matrix multiplication is done efficiently\n3×11 Matrix{Int64}:\n 1  4  13  1  7  1  10  1  4  13  1\n 2  5  14  2  8  2  11  2  5  14  2\n 3  6  15  3  9  3  12  3  6  15  3"},{"id":55,"pagetitle":"OneHotArrays.jl","title":"OneHotArrays.OneHotArray","ref":"/flux/stable/data/onehot/#OneHotArrays.OneHotArray","content":" OneHotArrays.OneHotArray  —  Type OneHotArray{T, N, M, I} <: AbstractArray{Bool, M}\nOneHotArray(indices, L) A one-hot  M -dimensional array with  L  labels (i.e.  size(A, 1) == L  and  sum(A, dims=1) == 1 ) stored as a compact  N == M-1 -dimensional array of indices. Typically constructed by  onehot  and  onehotbatch . Parameter  I  is the type of the underlying storage, and  T  its eltype."},{"id":56,"pagetitle":"OneHotArrays.jl","title":"OneHotArrays.OneHotVector","ref":"/flux/stable/data/onehot/#OneHotArrays.OneHotVector","content":" OneHotArrays.OneHotVector  —  Type OneHotVector{T} = OneHotArray{T, 0, 1, T}\nOneHotVector(indices, L) A one-hot vector with  L  labels (i.e.  length(A) == L  and  count(A) == 1 ) typically constructed by  onehot . Stored efficiently as a single index of type  T , usually  UInt32 ."},{"id":57,"pagetitle":"OneHotArrays.jl","title":"OneHotArrays.OneHotMatrix","ref":"/flux/stable/data/onehot/#OneHotArrays.OneHotMatrix","content":" OneHotArrays.OneHotMatrix  —  Type OneHotMatrix{T, I} = OneHotArray{T, 1, 2, I}\nOneHotMatrix(indices, L) A one-hot matrix (with  L  labels) typically constructed using  onehotbatch . Stored efficiently as a vector of indices with type  I  and eltype  T ."},{"id":60,"pagetitle":"Flat vs. Nested","title":"Flat vs. Nested Structures","ref":"/flux/stable/destructure/#man-destructure","content":" Flat vs. Nested Structures A Flux model is a nested structure, with parameters stored within many layers. Sometimes you may want a flat representation of them, to interact with functions expecting just one vector. This is provided by  destructure : julia> model = Chain(Dense(2=>1, tanh), Dense(1=>1))\nChain(\n  Dense(2 => 1, tanh),                  # 3 parameters\n  Dense(1 => 1),                        # 2 parameters\n)                   # Total: 4 arrays, 5 parameters, 276 bytes.\n\njulia> flat, rebuild = Flux.destructure(model)\n(Float32[0.863101, 1.2454957, 0.0, -1.6345707, 0.0], Restructure(Chain, ..., 5))\n\njulia> rebuild(zeros(5))  # same structure, new parameters\nChain(\n  Dense(2 => 1, tanh),                  # 3 parameters  (all zero)\n  Dense(1 => 1),                        # 2 parameters  (all zero)\n)                   # Total: 4 arrays, 5 parameters, 276 bytes. Both  destructure  and the  Restructure  function can be used within gradient computations. For instance, this computes the Hessian  ∂²L/∂θᵢ∂θⱼ  of some loss function, with respect to all parameters of the Flux model. The resulting matrix has off-diagonal entries, which cannot really be expressed in a nested structure: julia> x = rand(Float32, 2, 16);\n\njulia> grad = gradient(m -> sum(abs2, m(x)), model)  # nested gradient\n((layers = ((weight = Float32[10.339018 11.379145], bias = Float32[22.845667], σ = nothing), (weight = Float32[-29.565302;;], bias = Float32[-37.644184], σ = nothing)),),)\n\njulia> function loss(v::Vector)\n         m = rebuild(v)\n         y = m(x)\n         sum(abs2, y)\n       end;\n\njulia> gradient(loss, flat)  # flat gradient, same numbers\n(Float32[10.339018, 11.379145, 22.845667, -29.565302, -37.644184],)\n\njulia> Zygote.hessian(loss, flat)  # second derivative\n5×5 Matrix{Float32}:\n  -7.13131   -5.54714  -11.1393  -12.6504   -8.13492\n  -5.54714   -7.11092  -11.0208  -13.9231   -9.36316\n -11.1393   -11.0208   -13.7126  -27.9531  -22.741\n -12.6504   -13.9231   -27.9531   18.0875   23.03\n  -8.13492   -9.36316  -22.741    23.03     32.0\n\njulia> Flux.destructure(grad)  # acts on non-models, too\n(Float32[10.339018, 11.379145, 22.845667, -29.565302, -37.644184], Restructure(Tuple, ..., 5)) Flux ≤ 0.12 Old versions of Flux had an entirely different implementation of  destructure , which had many bugs (and almost no tests). Many comments online still refer to that now-deleted function, or to memories of it."},{"id":61,"pagetitle":"Flat vs. Nested","title":"All Parameters","ref":"/flux/stable/destructure/#All-Parameters","content":" All Parameters The function  destructure  now lives in  Optimisers.jl . (Be warned this package is unrelated to the  Flux.Optimisers  sub-module! The confusion is temporary.)"},{"id":62,"pagetitle":"Flat vs. Nested","title":"Optimisers.destructure","ref":"/flux/stable/destructure/#Optimisers.destructure","content":" Optimisers.destructure  —  Function destructure(model) -> vector, reconstructor Copies all  trainable ,  isnumeric  parameters in the model to a vector, and returns also a function which reverses this transformation. Differentiable. Example julia> v, re = destructure((x=[1.0, 2.0], y=(sin, [3.0 + 4.0im])))\n(ComplexF64[1.0 + 0.0im, 2.0 + 0.0im, 3.0 + 4.0im], Restructure(NamedTuple, ..., 3))\n\njulia> re([3, 5, 7+11im])\n(x = [3.0, 5.0], y = (sin, ComplexF64[7.0 + 11.0im])) If  model  contains various number types, they are promoted to make  vector , and are usually restored by  Restructure . Such restoration follows the rules  of  ChainRulesCore.ProjectTo , and thus will restore floating point precision, but will permit more exotic numbers like  ForwardDiff.Dual . If  model  contains only GPU arrays, then  vector  will also live on the GPU. At present, a mixture of GPU and ordinary CPU arrays is undefined behaviour."},{"id":63,"pagetitle":"Flat vs. Nested","title":"Optimisers.trainable","ref":"/flux/stable/destructure/#Optimisers.trainable","content":" Optimisers.trainable  —  Function trainable(x::Layer) -> NamedTuple This may be overloaded to make optimisers ignore some fields of every  Layer , which would otherwise contain trainable parameters. Warning This is very rarely required. Fields of  struct Layer  which contain functions, or integers like sizes, are always ignored anyway. Overloading  trainable  is only necessary when some arrays of numbers are to be optimised, and some arrays of numbers are not. The default is  Functors.children(x) , usually a NamedTuple of all fields, and  trainable(x)  must contain a subset of these."},{"id":64,"pagetitle":"Flat vs. Nested","title":"Optimisers.isnumeric","ref":"/flux/stable/destructure/#Optimisers.isnumeric","content":" Optimisers.isnumeric  —  Function isnumeric(x) -> Bool Returns  true  on any parameter to be adjusted by Optimisers.jl, namely arrays of non-integer numbers. Returns  false  on all other types. Requires also that  Functors.isleaf(x) == true , to focus on e.g. the parent of a transposed matrix, not the wrapper."},{"id":65,"pagetitle":"Flat vs. Nested","title":"All Layers","ref":"/flux/stable/destructure/#All-Layers","content":" All Layers Another kind of flat view of a nested model is provided by the  modules  command. This extracts a list of all layers:"},{"id":66,"pagetitle":"Flat vs. Nested","title":"Flux.modules","ref":"/flux/stable/destructure/#Flux.modules","content":" Flux.modules  —  Function modules(m) Return an iterator over non-leaf objects that can be reached by recursing  m  over the children given by  functor . Useful for applying a function (e.g. a regularizer) over specific modules or subsets of the parameters (e.g. the weights but not the biases). Examples julia> m1 = Chain(Dense(28^2, 64), BatchNorm(64, relu));\n\njulia> m2 = Chain(m1, Dense(64, 10))\nChain(\n  Chain(\n    Dense(784 => 64),                   # 50_240 parameters\n    BatchNorm(64, relu),                # 128 parameters, plus 128\n  ),\n  Dense(64 => 10),                      # 650 parameters\n)         # Total: 6 trainable arrays, 51_018 parameters,\n          # plus 2 non-trainable, 128 parameters, summarysize 200.312 KiB.\n\njulia> Flux.modules(m2)\n7-element Vector{Any}:\n Chain(Chain(Dense(784 => 64), BatchNorm(64, relu)), Dense(64 => 10))  # 51_018 parameters, plus 128 non-trainable\n (Chain(Dense(784 => 64), BatchNorm(64, relu)), Dense(64 => 10))\n Chain(Dense(784 => 64), BatchNorm(64, relu))  # 50_368 parameters, plus 128 non-trainable\n (Dense(784 => 64), BatchNorm(64, relu))\n Dense(784 => 64)    # 50_240 parameters\n BatchNorm(64, relu)  # 128 parameters, plus 128 non-trainable\n Dense(64 => 10)     # 650 parameters\n\njulia> L2(m) = sum(sum(abs2, l.weight) for l in Flux.modules(m) if l isa Dense)\nL2 (generic function with 1 method)\n\njulia> L2(m2) isa Float32\ntrue source"},{"id":67,"pagetitle":"Flat vs. Nested","title":"Save and Load","ref":"/flux/stable/destructure/#Save-and-Load","content":" Save and Load"},{"id":68,"pagetitle":"Flat vs. Nested","title":"Flux.state","ref":"/flux/stable/destructure/#Flux.state","content":" Flux.state  —  Function state(x) Return an object with the same nested structure as  x  according to  Functors.children ,  but made only of basic containers (e.g. named tuples, tuples, arrays, and dictionaries). Besides trainable and non-trainable arrays, the state will contain leaf nodes that are not arrays, such as numbers, symbols, strings, and nothing values. The leaf types that end up in the state could increase in the future. This method is particularly useful for saving and loading models,  since the state contain only simple data types that can be easily serialized. The state can be passed to  loadmodel!  to restore the model. Examples Copy the state into another model julia> m1 = Chain(Dense(1, 2, tanh; init=ones), Dense(2, 1; init=ones));\n\njulia> s = Flux.state(m1)\n(layers = ((weight = [1.0; 1.0;;], bias = [0.0, 0.0], σ = ()), (weight = [1.0 1.0], bias = [0.0], σ = ())),)\n\njulia> m2 = Chain(Dense(1, 2, tanh), Dense(2, 1; bias=false));  # weights are random numbers\n\njulia> Flux.loadmodel!(m2, s);\n\njulia> m2[1].weight   # now the weights of m2 are the same as m1\n2×1 Matrix{Float32}:\n 1.0\n 1.0\n\njulia> Flux.state(trainmode!(Dropout(0.2)))  # contains p & activity, but not RNG state\n(p = 0.2, dims = (), active = true, rng = ())\n\njulia> Flux.state(BatchNorm(1))  # contains non-trainable arrays μ, σ²\n(λ = (), β = Float32[0.0], γ = Float32[1.0], μ = Float32[0.0], σ² = Float32[1.0], ϵ = 1.0f-5, momentum = 0.1f0, affine = true, track_stats = true, active = nothing, chs = 1) Save and load with BSON julia> using BSON\n\njulia> BSON.@save \"checkpoint.bson\" model_state = s\n\njulia> Flux.loadmodel!(m2, BSON.load(\"checkpoint.bson\")[:model_state]) Save and load with JLD2 julia> using JLD2\n\njulia> JLD2.jldsave(\"checkpoint.jld2\", model_state = s)\n\njulia> Flux.loadmodel!(m2, JLD2.load(\"checkpoint.jld2\", \"model_state\")) source"},{"id":69,"pagetitle":"Flat vs. Nested","title":"Flux.loadmodel!","ref":"/flux/stable/destructure/#Flux.loadmodel!","content":" Flux.loadmodel!  —  Function loadmodel!(dst, src) Copy all the parameters (trainable and non-trainable) from  src  into  dst . Recursively walks  dst  and  src  together using  Functors.children , and calling  copyto!  on parameter arrays or throwing an error when there is a mismatch. Non-array elements (such as activation functions) are not copied and need not match. Zero bias vectors and  bias=false  are considered equivalent (see extended help for more details). See also  Flux.state . Examples julia> dst = Chain(Dense(Flux.ones32(2, 5), Flux.ones32(2), tanh), Dense(2 => 1; bias = [1f0]))\nChain(\n  Dense(5 => 2, tanh),                  # 12 parameters\n  Dense(2 => 1),                        # 3 parameters\n)                   # Total: 4 arrays, 15 parameters, 316 bytes.\n\njulia> dst[1].weight ≈ ones(2, 5)  # by construction\ntrue\n\njulia> src = Chain(Dense(5 => 2, relu), Dense(2 => 1, bias=false));\n\njulia> Flux.loadmodel!(dst, src);\n\njulia> dst[1].weight ≈ ones(2, 5)  # values changed\nfalse\n\njulia> iszero(dst[2].bias)\ntrue Extended help Throws an error when: dst  and  src  do not share the same fields (at any level) the sizes of leaf nodes are mismatched between  dst  and  src copying non-array values to/from an array parameter (except inactive parameters described below) dst  is a \"tied\" parameter (i.e. refers to another parameter) and loaded into multiple times with mismatched source values Inactive parameters can be encoded by using the boolean value  false  instead of an array. If  dst == false  and  src  is an all-zero array, no error will be raised (and no values copied); however, attempting to copy a non-zero array to an inactive parameter will throw an error. Likewise, copying a  src  value of  false  to any  dst  array is valid, but copying a  src  value of  true  will error. source"},{"id":72,"pagetitle":"Ecosystem","title":"The Julia Ecosystem around Flux","ref":"/flux/stable/ecosystem/#The-Julia-Ecosystem-around-Flux","content":" The Julia Ecosystem around Flux One of the main strengths of Julia lies in an ecosystem of packages  globally providing a rich and consistent user experience. This is a non-exhaustive list of Julia packages, nicely complementing  Flux  in typical machine learning and deep learning workflows. To add your project please send a  PR . See also academic work  citing Flux  or  citing Zygote ."},{"id":73,"pagetitle":"Ecosystem","title":"Flux models","ref":"/flux/stable/ecosystem/#Flux-models","content":" Flux models Flux's  model-zoo  contains examples from many domains."},{"id":74,"pagetitle":"Ecosystem","title":"Computer vision","ref":"/flux/stable/ecosystem/#Computer-vision","content":" Computer vision ObjectDetector.jl  provides ready-to-go image detection via YOLO. Metalhead.jl  includes many state-of-the-art computer vision models which can easily be used for transfer learning. UNet.jl  is a generic UNet implementation."},{"id":75,"pagetitle":"Ecosystem","title":"Natural language processing","ref":"/flux/stable/ecosystem/#Natural-language-processing","content":" Natural language processing Transformers.jl  provides components for Transformer models for NLP, as well as providing several trained models out of the box. TextAnalysis.jl  provides several NLP algorithms that use Flux models under the hood."},{"id":76,"pagetitle":"Ecosystem","title":"Reinforcement learning","ref":"/flux/stable/ecosystem/#Reinforcement-learning","content":" Reinforcement learning AlphaZero.jl  provides a generic, simple and fast implementation of Deepmind's AlphaZero algorithm. ReinforcementLearning.jl  offers a collection of tools for doing reinforcement learning research in Julia."},{"id":77,"pagetitle":"Ecosystem","title":"Graph learning","ref":"/flux/stable/ecosystem/#Graph-learning","content":" Graph learning GraphNeuralNetworks.jl  is a fresh, performant and flexible graph neural network library based on Flux.jl. GeometricFlux.jl  is the first graph neural network library for julia.  NeuralOperators.jl  enables training infinite dimensional PDEs by learning a continuous function instead of using the finite element method. SeaPearl.jl  is a Constraint Programming solver that uses Reinforcement Learning based on graphs as input."},{"id":78,"pagetitle":"Ecosystem","title":"Time series","ref":"/flux/stable/ecosystem/#Time-series","content":" Time series FluxArchitectures.jl  is a collection of advanced network architectures for time series forecasting."},{"id":79,"pagetitle":"Ecosystem","title":"Robust networks","ref":"/flux/stable/ecosystem/#Robust-networks","content":" Robust networks RobustNeuralNetworks.jl  includes classes of neural networks that are constructed to naturally satisfy robustness constraints."},{"id":80,"pagetitle":"Ecosystem","title":"Tools closely associated with Flux","ref":"/flux/stable/ecosystem/#Tools-closely-associated-with-Flux","content":" Tools closely associated with Flux Utility tools you're unlikely to have met if you never used Flux!"},{"id":81,"pagetitle":"Ecosystem","title":"High-level training flows","ref":"/flux/stable/ecosystem/#High-level-training-flows","content":" High-level training flows FastAI.jl  is a Julia port of Python's fast.ai library. FluxTraining.jl  is a package for using and writing powerful, extensible training loops for deep learning models. It supports callbacks for many common use cases like hyperparameter scheduling, metrics tracking and logging, checkpointing, early stopping, and more. It powers training in FastAI.jl"},{"id":82,"pagetitle":"Ecosystem","title":"Datasets","ref":"/flux/stable/ecosystem/#Datasets","content":" Datasets Commonly used machine learning datasets are provided by the following packages in the julia ecosystem: MLDatasets.jl  focuses on downloading, unpacking, and accessing benchmark datasets. GraphMLDatasets.jl : a library for machine learning datasets on graph."},{"id":83,"pagetitle":"Ecosystem","title":"Plumbing","ref":"/flux/stable/ecosystem/#Plumbing","content":" Plumbing Tools to put data into the right order for creating a model. Augmentor.jl  is a real-time library augmentation library for increasing the number of training images. DataAugmentation.jl  aims to make it easy to build stochastic, label-preserving augmentation pipelines for vision use cases involving images, keypoints and segmentation masks. MLUtils.jl  (replaces  MLDataUtils.jl  and  MLLabelUtils.jl ) is a library for processing Machine Learning datasets."},{"id":84,"pagetitle":"Ecosystem","title":"Parameters","ref":"/flux/stable/ecosystem/#Parameters","content":" Parameters ParameterSchedulers.jl  standard scheduling policies for machine learning."},{"id":85,"pagetitle":"Ecosystem","title":"Differentiable programming","ref":"/flux/stable/ecosystem/#Differentiable-programming","content":" Differentiable programming Packages based on differentiable programming but not necessarily related to Machine Learning.  The  SciML  ecosystem uses Flux and Zygote to mix neural nets with differential equations, to get the best of black box and mechanistic modelling. DiffEqFlux.jl  provides tools for creating Neural Differential Equations. Flux3D.jl  shows off machine learning on 3D data. RayTracer.jl  combines ML with computer vision via a differentiable renderer. Duckietown.jl  Differentiable Duckietown simulator. The  Yao.jl  project uses Flux and Zygote for Quantum Differentiable Programming. AtomicGraphNets.jl  enables learning graph based models on atomic systems used in chemistry. DiffImages.jl  differentiable computer vision modeling in Julia with the Images.jl ecosystem."},{"id":86,"pagetitle":"Ecosystem","title":"Probabilistic programming","ref":"/flux/stable/ecosystem/#Probabilistic-programming","content":" Probabilistic programming Turing.jl  extends Flux's differentiable programming capabilities to probabilistic programming. Omega.jl  is a research project aimed at causal, higher-order probabilistic programming. Stheno.jl  provides flexible Gaussian processes."},{"id":87,"pagetitle":"Ecosystem","title":"Statistics","ref":"/flux/stable/ecosystem/#Statistics","content":" Statistics OnlineStats.jl  provides single-pass algorithms for statistics."},{"id":88,"pagetitle":"Ecosystem","title":"Useful miscellaneous packages","ref":"/flux/stable/ecosystem/#Useful-miscellaneous-packages","content":" Useful miscellaneous packages Some useful and random packages! AdversarialPrediction.jl  provides a way to easily optimise generic performance metrics in supervised learning settings using the  Adversarial Prediction  framework. Mill.jl  helps to prototype flexible multi-instance learning models. MLMetrics.jl  is a utility for scoring models in data science and machine learning. Torch.jl  exposes torch in Julia. ValueHistories.jl  is a utility for efficient tracking of optimization histories, training curves or other information of arbitrary types and at arbitrarily spaced sampling times. InvertibleNetworks.jl  Building blocks for invertible neural networks in the Julia programming language. ProgressMeter.jl  progress meters for long-running computations. TensorBoardLogger.jl  easy peasy logging to  tensorboard  in Julia ArgParse.jl  is a package for parsing command-line arguments to Julia programs. Parameters.jl  types with default field values, keyword constructors and (un-)pack macros. BSON.jl  is a package for working with the Binary JSON serialisation format. DataFrames.jl  in-memory tabular data in Julia. DrWatson.jl  is a scientific project assistant software. This tight integration among Julia packages is shown in some of the examples in the  model-zoo  repository."},{"id":89,"pagetitle":"Ecosystem","title":"Alternatives to Flux","ref":"/flux/stable/ecosystem/#Alternatives-to-Flux","content":" Alternatives to Flux Julia has several other libraries for making neural networks.  SimpleChains.jl  is focused on making small, simple, CPU-based, neural networks fast. Uses  LoopVectorization.jl . (Was  FastChain  in DiffEqFlux.jl)  Knet.jl  is a neural network library built around  AutoGrad.jl . Lux.jl  (earlier ExplicitFluxLayers.jl) shares much of the design, use-case, and NNlib.jl / Optimisers.jl back-end of Flux. But instead of encapsulating all parameters within the model structure, it separates this into 3 components: a model, a tree of parameters, and a tree of model states. Explicit or explicit? Flux's  training docs  talk about changes from Zygote's implicit to explicit gradients, dictionary-like to tree-like structures. (See also  Zygote's description  of these.) Lux also uses Zygote, but uses the word \"explicit\" to mean something unrelated, namely storing the tree of parameters (and of state) separately from the model."},{"id":92,"pagetitle":"GPU Support","title":"GPU Support","ref":"/flux/stable/gpu/#GPU-Support","content":" GPU Support Starting with v0.14, Flux doesn't force a specific GPU backend and the corresponding package dependencies on the users.  Thanks to the  package extension mechanism  introduced in julia v1.9, Flux conditionally loads GPU specific code once a GPU package is made available (e.g. through  using CUDA ). NVIDIA GPU support requires the packages  CUDA.jl  and  cuDNN.jl  to be installed in the environment. In the julia REPL, type  ] add CUDA, cuDNN  to install them. For more details see the  CUDA.jl  readme. AMD GPU support is available since Julia 1.9 on systems with ROCm and MIOpen installed. For more details refer to the  AMDGPU.jl  repository. Metal GPU acceleration is available on Apple Silicon hardware. For more details refer to the  Metal.jl  repository. Metal support in Flux is experimental and many features are not yet available. In order to trigger GPU support in Flux, you need to call  using CUDA ,  using AMDGPU  or  using Metal  in your code. Notice that for CUDA, explicitly loading also  cuDNN  is not required, but the package has to be installed in the environment.  Flux ≤ 0.13 Old versions of Flux automatically installed CUDA.jl to provide GPU support. Starting from Flux v0.14, CUDA.jl is not a dependency anymore and has to be installed manually."},{"id":93,"pagetitle":"GPU Support","title":"Checking GPU Availability","ref":"/flux/stable/gpu/#Checking-GPU-Availability","content":" Checking GPU Availability By default, Flux will run the checks on your system to see if it can support GPU functionality. You can check if Flux identified a valid GPU setup by typing the following: julia> using CUDA\n\njulia> CUDA.functional()\ntrue For AMD GPU: julia> using AMDGPU\n\njulia> AMDGPU.functional()\ntrue\n\njulia> AMDGPU.functional(:MIOpen)\ntrue For Metal GPU: julia> using Metal\n\njulia> Metal.functional()\ntrue"},{"id":94,"pagetitle":"GPU Support","title":"Selecting GPU backend","ref":"/flux/stable/gpu/#Selecting-GPU-backend","content":" Selecting GPU backend Available GPU backends are:  CUDA ,  AMDGPU  and  Metal . Flux relies on  Preferences.jl  for selecting default GPU backend to use. There are two ways you can specify it: From the REPL/code in your project, call  Flux.gpu_backend!(\"AMDGPU\")  and restart (if needed) Julia session for the changes to take effect. In  LocalPreferences.toml  file in you project directory specify: [Flux]\ngpu_backend = \"AMDGPU\" Current GPU backend can be fetched from  Flux.GPU_BACKEND  variable: julia> Flux.GPU_BACKEND\n\"CUDA\" The current backend will affect the behaviour of methods like the method  gpu  described below."},{"id":95,"pagetitle":"GPU Support","title":"Basic GPU Usage","ref":"/flux/stable/gpu/#Basic-GPU-Usage","content":" Basic GPU Usage Support for array operations on other hardware backends, like GPUs, is provided by external packages like  CUDA.jl ,  AMDGPU.jl , and  Metal.jl . Flux is agnostic to array types, so we simply need to move model weights and data to the GPU and Flux will handle it. For example, we can use  CUDA.CuArray  (with the  cu  converter) to run our  basic example  on an NVIDIA GPU. (Note that you need to have CUDA available to use CUDA.CuArray – please see the  CUDA.jl  instructions for more details.) using CUDA\n\nW = cu(rand(2, 5)) # a 2×5 CuArray\nb = cu(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = cu(rand(5)), cu(rand(2)) # Dummy data\nloss(x, y) # ~ 3 Note that we convert both the parameters ( W ,  b ) and the data set ( x ,  y ) to cuda arrays. Taking derivatives and training works exactly as before. If you define a structured model, like a  Dense  layer or  Chain , you just need to convert the internal parameters. Flux provides  fmap , which allows you to alter all parameters of a model at once. d = Dense(10 => 5, σ)\nd = fmap(cu, d)\nd.weight # CuArray\nd(cu(rand(10))) # CuArray output\n\nm = Chain(Dense(10 => 5, σ), Dense(5 => 2), softmax)\nm = fmap(cu, m)\nm(cu(rand(10))) As a convenience, Flux provides the  gpu  function to convert models and data to the GPU if one is available. By default, it'll do nothing. So, you can safely call  gpu  on some data or model (as shown below), and the code will not error, regardless of whether the GPU is available or not. If a GPU library (e.g. CUDA) loads successfully,  gpu  will move data from the CPU to the GPU. As is shown below, this will change the type of something like a regular array to a  CuArray . julia> using Flux, CUDA\n\njulia> m = Dense(10, 5) |> gpu\nDense(10 => 5)      # 55 parameters\n\njulia> x = rand(10) |> gpu\n10-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n 0.066846445\n ⋮\n 0.76706964\n\njulia> m(x)\n5-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n -0.99992573\n ⋮\n -0.547261 The analogue  cpu  is also available for moving models and data back off of the GPU. julia> x = rand(10) |> gpu\n10-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n 0.8019236\n ⋮\n 0.7766742\n\njulia> x |> cpu\n10-element Vector{Float32}:\n 0.8019236\n ⋮\n 0.7766742"},{"id":96,"pagetitle":"GPU Support","title":"Transferring Training Data","ref":"/flux/stable/gpu/#Transferring-Training-Data","content":" Transferring Training Data In order to train the model using the GPU both model and the training data have to be transferred to GPU memory. Moving the data can be done in two different ways: Iterating over the batches in a  DataLoader  object transferring each one of the training batches at a time to the GPU. This is recommended for large datasets. Done by hand, it might look like this: train_loader = Flux.DataLoader((X, Y), batchsize=64, shuffle=true)\n# ... model definition, optimiser setup\nfor epoch in 1:epochs\n    for (x_cpu, y_cpu) in train_loader\n        x = gpu(x_cpu)\n        y = gpu(y_cpu)\n        grads = gradient(m -> loss(m, x, y), model)\n        Flux.update!(opt_state, model, grads[1])\n    end\nend Rather than write this out every time, you can just call  gpu(::DataLoader) : gpu_train_loader = Flux.DataLoader((X, Y), batchsize=64, shuffle=true) |> gpu\n# ... model definition, optimiser setup\nfor epoch in 1:epochs\n    for (x, y) in gpu_train_loader\n        grads = gradient(m -> loss(m, x, y), model)\n        Flux.update!(opt_state, model, grads[1])\n    end\nend This is equivalent to  DataLoader(MLUtils.mapobs(gpu, (X, Y)); keywords...) . Something similar can also be done with  CUDA.CuIterator ,  gpu_train_loader = CUDA.CuIterator(train_loader) . However, this only works with a limited number of data types:  first(train_loader)  should be a tuple (or  NamedTuple ) of arrays. Transferring all training data to the GPU at once before creating the  DataLoader . This is usually performed for smaller datasets which are sure to fit in the available GPU memory. gpu_train_loader = Flux.DataLoader((X, Y) |> gpu, batchsize = 32)\n# ...\nfor epoch in 1:epochs\n    for (x, y) in gpu_train_loader\n        # ... Here  (X, Y) |> gpu  applies  gpu  to both arrays, as it recurses into structures."},{"id":97,"pagetitle":"GPU Support","title":"Saving GPU-Trained Models","ref":"/flux/stable/gpu/#Saving-GPU-Trained-Models","content":" Saving GPU-Trained Models After the training process is done, one must always transfer the trained model back to the  cpu  memory scope before serializing or saving to disk. This can be done, as described in the previous section, with: model = cpu(model) # or model = model |> cpu and then using BSON\n# ...\nBSON.@save \"./path/to/trained_model.bson\" model\n\n# in this approach the cpu-transferred model (referenced by the variable `model`)\n# only exists inside the `let` statement\nlet model = cpu(model)\n   # ...\n   BSON.@save \"./path/to/trained_model.bson\" model\nend\n\n# is equivalent to the above, but uses `key=value` storing directive from BSON.jl\nBSON.@save \"./path/to/trained_model.bson\" model = cpu(model) The reason behind this is that models trained in the GPU but not transferred to the CPU memory scope will expect  CuArray s as input. In other words, Flux models expect input data coming from the same kind device in which they were trained on. In controlled scenarios in which the data fed to the loaded models is garanteed to be in the GPU there's no need to transfer them back to CPU memory scope, however in production environments, where artifacts are shared among different processes, equipments or configurations, there is no garantee that the CUDA.jl package will be available for the process performing inference on the model loaded from the disk."},{"id":98,"pagetitle":"GPU Support","title":"Disabling CUDA or choosing which GPUs are visible to Flux","ref":"/flux/stable/gpu/#Disabling-CUDA-or-choosing-which-GPUs-are-visible-to-Flux","content":" Disabling CUDA or choosing which GPUs are visible to Flux Sometimes it is required to control which GPUs are visible to  julia  on a system with multiple GPUs or disable GPUs entirely. This can be achieved with an environment variable  CUDA_VISIBLE_DEVICES . To disable all devices: $ export CUDA_VISIBLE_DEVICES='-1' To select specific devices by device id: $ export CUDA_VISIBLE_DEVICES='0,1' More information for conditional use of GPUs in CUDA.jl can be found in its  documentation , and information about the specific use of the variable is described in the  Nvidia CUDA blog post ."},{"id":99,"pagetitle":"GPU Support","title":"Using device objects","ref":"/flux/stable/gpu/#Using-device-objects","content":" Using device objects As a more convenient syntax, Flux allows the usage of GPU  device  objects which can be used to easily transfer models to GPUs (and defaulting to using the CPU if no GPU backend is available). This syntax has a few advantages including automatic selection of the GPU backend and type stability of data movement. To do this, the  Flux.get_device  function can be used. Flux.get_device  first checks for a GPU preference, and if possible returns a device for the preference backend. For instance, consider the following example, where we load the  CUDA.jl  package to use an NVIDIA GPU ( \"CUDA\"  is the default preference): julia> using Flux, CUDA;\n\njulia> device = Flux.get_device(; verbose=true)   # returns handle to an NVIDIA GPU\n[ Info: Using backend set in preferences: CUDA.\n(::Flux.FluxCUDADevice) (generic function with 1 method)\n\njulia> device.deviceID      # check the id of the GPU\nCuDevice(0): NVIDIA GeForce GTX 1650\n\njulia> model = Dense(2 => 3);\n\njulia> model.weight     # the model initially lives in CPU memory\n3×2 Matrix{Float32}:\n -0.984794  -0.904345\n  0.720379  -0.486398\n  0.851011  -0.586942\n\njulia> model = model |> device      # transfer model to the GPU\nDense(2 => 3)       # 9 parameters\n\njulia> model.weight\n3×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n -0.984794  -0.904345\n  0.720379  -0.486398\n  0.851011  -0.586942\n The device preference can also be set via the  Flux.gpu_backend!  function. For instance, below we first set our device preference to  \"CPU\" : julia> using Flux; Flux.gpu_backend!(\"CPU\")\n┌ Info: New GPU backend set: CPU.\n└ Restart your Julia session for this change to take effect! Then, after restarting the Julia session,  Flux.get_device  returns a handle to the  \"CPU\" : julia> using Flux, CUDA;    # even if CUDA is loaded, we'll still get a CPU device\n\njulia> device = Flux.get_device(; verbose=true)   # get a CPU device\n[ Info: Using backend set in preferences: CPU.\n(::Flux.FluxCPUDevice) (generic function with 1 method)\n\njulia> model = Dense(2 => 3);\n\njulia> model = model |> device\nDense(2 => 3)       # 9 parameters\n\njulia> model.weight     # no change; model still lives on CPU\n3×2 Matrix{Float32}:\n -0.942968   0.856258\n  0.440009   0.714106\n -0.419192  -0.471838 Clearly, this means that the same code will work for any GPU backend and the CPU.  If the preference backend isn't available or isn't functional, then  Flux.get_device  looks for a CUDA, AMDGPU or Metal backend, and returns a corresponding device (if the backend is available and functional). Otherwise, a CPU device is returned. In the below example, the GPU preference is  \"CUDA\" : julia> using Flux;      # preference is CUDA, but CUDA.jl not loaded\n\njulia> device = Flux.get_device(; verbose=true)       # this will resort to automatic device selection\n[ Info: Using backend set in preferences: CUDA.\n┌ Warning: Trying to use backend: CUDA but it's trigger package is not loaded.\n│ Please load the package and call this function again to respect the preferences backend.\n└ @ Flux ~/fluxml/Flux.jl/src/functor.jl:637\n[ Info: Using backend: CPU.\n(::Flux.FluxCPUDevice) (generic function with 1 method) For detailed information about how the backend is selected, check the documentation for  Flux.get_device ."},{"id":100,"pagetitle":"GPU Support","title":"Data movement across GPU devices","ref":"/flux/stable/gpu/#Data-movement-across-GPU-devices","content":" Data movement across GPU devices Flux also supports getting handles to specific GPU devices, and transferring models from one GPU device to another GPU device from the same backend. Let's try it out for NVIDIA GPUs. First, we list all the available devices: julia> using Flux, CUDA;\n\njulia> CUDA.devices()\nCUDA.DeviceIterator() for 3 devices:\n0. GeForce RTX 2080 Ti\n1. GeForce RTX 2080 Ti\n2. TITAN X (Pascal)\n Then, let's select the device with id  0 : julia> device0 = Flux.get_device(\"CUDA\", 0)        # the currently supported values for backend are \"CUDA\" and \"AMDGPU\"\n(::Flux.FluxCUDADevice) (generic function with 1 method)\n Then, let's move a simple dense layer to the GPU represented by  device0 : julia> dense_model = Dense(2 => 3)\nDense(2 => 3)       # 9 parameters\n\njulia> dense_model = dense_model |> device0;\n\njulia> dense_model.weight\n3×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n  0.695662   0.816299\n -0.204763  -0.10232\n -0.955829   0.538412\n\njulia> CUDA.device(dense_model.weight)      # check the GPU to which dense_model is attached\nCuDevice(0): GeForce RTX 2080 Ti\n Next, we'll get a handle to the device with id  1 , and move  dense_model  to that device: julia> device1 = Flux.get_device(\"CUDA\", 1)\n(::Flux.FluxCUDADevice) (generic function with 1 method)\n\njulia> dense_model = dense_model |> device1;    # don't directly print the model; see warning below\n\njulia> CUDA.device(dense_model.weight)\nCuDevice(1): GeForce RTX 2080 Ti\n Due to a limitation in  Metal.jl , currently this kind of data movement across devices is only supported for  CUDA  and  AMDGPU  backends. Printing models after moving to a different device Due to a limitation in how GPU packages currently work, printing models on the REPL after moving them to a GPU device which is different from the current device will lead to an error."},{"id":101,"pagetitle":"GPU Support","title":"Flux.AbstractDevice","ref":"/flux/stable/gpu/#Flux.AbstractDevice","content":" Flux.AbstractDevice  —  Type Flux.AbstractDevice <: Function An abstract type representing  device  objects for different GPU backends. The currently supported backends are  \"CUDA\" ,  \"AMDGPU\" ,  \"Metal\"  and  \"CPU\" ; the  \"CPU\"  backend is the fallback case when no GPU is available. GPU extensions of Flux define subtypes of this type. source"},{"id":102,"pagetitle":"GPU Support","title":"Flux.FluxCPUDevice","ref":"/flux/stable/gpu/#Flux.FluxCPUDevice","content":" Flux.FluxCPUDevice  —  Type Flux.FluxCPUDevice <: Flux.AbstractDevice A type representing  device  objects for the  \"CPU\"  backend for Flux. This is the fallback case when no GPU is available to Flux. source"},{"id":103,"pagetitle":"GPU Support","title":"Flux.FluxCUDADevice","ref":"/flux/stable/gpu/#Flux.FluxCUDADevice","content":" Flux.FluxCUDADevice  —  Type FluxCUDADevice <: AbstractDevice A type representing  device  objects for the  \"CUDA\"  backend for Flux. source"},{"id":104,"pagetitle":"GPU Support","title":"Flux.FluxAMDGPUDevice","ref":"/flux/stable/gpu/#Flux.FluxAMDGPUDevice","content":" Flux.FluxAMDGPUDevice  —  Type FluxAMDGPUDevice <: AbstractDevice A type representing  device  objects for the  \"AMDGPU\"  backend for Flux. source"},{"id":105,"pagetitle":"GPU Support","title":"Flux.FluxMetalDevice","ref":"/flux/stable/gpu/#Flux.FluxMetalDevice","content":" Flux.FluxMetalDevice  —  Type FluxMetalDevice <: AbstractDevice A type representing  device  objects for the  \"Metal\"  backend for Flux. source"},{"id":106,"pagetitle":"GPU Support","title":"Flux.supported_devices","ref":"/flux/stable/gpu/#Flux.supported_devices","content":" Flux.supported_devices  —  Function Flux.supported_devices() Get all supported backends for Flux, in order of preference. Example julia> using Flux;\n\njulia> Flux.supported_devices()\n(\"CUDA\", \"AMDGPU\", \"Metal\", \"CPU\") source"},{"id":107,"pagetitle":"GPU Support","title":"Flux.get_device","ref":"/flux/stable/gpu/#Flux.get_device","content":" Flux.get_device  —  Function Flux.get_device(; verbose=false)::Flux.AbstractDevice Returns a  device  object for the most appropriate backend for the current Julia session.  First, the function checks whether a backend preference has been set via the  Flux.gpu_backend!  function. If so, an attempt is made to load this backend. If the corresponding trigger package has been loaded and the backend is functional, a  device  corresponding to the given backend is loaded. Otherwise, the backend is chosen automatically. To update the backend preference, use  Flux.gpu_backend! . If there is no preference, then for each of the  \"CUDA\" ,  \"AMDGPU\" ,  \"Metal\"  and  \"CPU\"  backends in the given order, this function checks whether the given backend has been loaded via the corresponding trigger package, and whether the backend is functional. If so, the  device  corresponding to the backend is returned. If no GPU backend is available, a  Flux.FluxCPUDevice  is returned. If  verbose  is set to  true , then the function prints informative log messages. Examples For the example given below, the backend preference was set to  \"AMDGPU\"  via the  gpu_backend!  function. julia> using Flux;\n\njulia> model = Dense(2 => 3)\nDense(2 => 3)       # 9 parameters\n\njulia> device = Flux.get_device(; verbose=true)       # this will just load the CPU device\n[ Info: Using backend set in preferences: AMDGPU.\n┌ Warning: Trying to use backend: AMDGPU but it's trigger package is not loaded.\n│ Please load the package and call this function again to respect the preferences backend.\n└ @ Flux ~/fluxml/Flux.jl/src/functor.jl:638\n[ Info: Using backend: CPU.\n(::Flux.FluxCPUDevice) (generic function with 1 method)\n\njulia> model = model |> device\nDense(2 => 3)       # 9 parameters\n\njulia> model.weight\n3×2 Matrix{Float32}:\n -0.304362  -0.700477\n -0.861201   0.67825\n -0.176017   0.234188 Here is the same example, but using  \"CUDA\" : julia> using Flux, CUDA;\n\njulia> model = Dense(2 => 3)\nDense(2 => 3)       # 9 parameters\n\njulia> device = Flux.get_device(; verbose=true)\n[ Info: Using backend set in preferences: AMDGPU.\n┌ Warning: Trying to use backend: AMDGPU but it's trigger package is not loaded.\n│ Please load the package and call this function again to respect the preferences backend.\n└ @ Flux ~/fluxml/Flux.jl/src/functor.jl:637\n[ Info: Using backend: CUDA.\n(::Flux.FluxCUDADevice) (generic function with 1 method)\n\njulia> model = model |> device\nDense(2 => 3)       # 9 parameters\n\njulia> model.weight\n3×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n  0.820013   0.527131\n -0.915589   0.549048\n  0.290744  -0.0592499 source Flux.get_device(backend::String, idx::Int = 0)::Flux.AbstractDevice Get a device object for a backend specified by the string  backend  and  idx . The currently supported values of  backend  are  \"CUDA\" ,  \"AMDGPU\"  and  \"CPU\" .  idx  must be an integer value between  0  and the number of available devices. Examples julia> using Flux, CUDA;\n\njulia> CUDA.devices()\nCUDA.DeviceIterator() for 3 devices:\n0. GeForce RTX 2080 Ti\n1. GeForce RTX 2080 Ti\n2. TITAN X (Pascal)\n\njulia> device0 = Flux.get_device(\"CUDA\", 0)\n(::Flux.FluxCUDADevice) (generic function with 1 method)\n\njulia> device0.deviceID\nCuDevice(0): GeForce RTX 2080 Ti\n\njulia> device1 = Flux.get_device(\"CUDA\", 1)\n(::Flux.FluxCUDADevice) (generic function with 1 method)\n\njulia> device1.deviceID\nCuDevice(1): GeForce RTX 2080 Ti\n\njulia> cpu_device = Flux.get_device(\"CPU\")\n(::Flux.FluxCPUDevice) (generic function with 1 method)\n source"},{"id":110,"pagetitle":"Activation Functions","title":"Activation Functions from NNlib.jl","ref":"/flux/stable/models/activation/#man-activations","content":" Activation Functions from NNlib.jl These non-linearities used between layers of your model are exported by the  NNlib  package. Note that, unless otherwise stated, activation functions operate on scalars. To apply them to an array you can call  σ.(xs) ,  relu.(xs)  and so on. Alternatively, they can be passed to a layer like  Dense(784 => 1024, relu)  which will handle this broadcasting. Functions like  softmax  are sometimes described as activation functions, but not by Flux. They must see all the outputs, and hence cannot be broadcasted. See the next page for details."},{"id":111,"pagetitle":"Activation Functions","title":"Alphabetical Listing","ref":"/flux/stable/models/activation/#Alphabetical-Listing","content":" Alphabetical Listing"},{"id":112,"pagetitle":"Activation Functions","title":"NNlib.celu","ref":"/flux/stable/models/activation/#NNlib.celu","content":" NNlib.celu  —  Function celu(x, α=1) = x ≥ 0 ? x : α * (exp(x/α) - 1) Activation function from  \"Continuously Differentiable Exponential Linear Units\" . julia> lineplot(celu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ celu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠔⠒⠋⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⠤⠤⠤⠤⠔⠒⠒⠒⠊⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> celu(-10f0)\n-0.9999546f0"},{"id":113,"pagetitle":"Activation Functions","title":"NNlib.elu","ref":"/flux/stable/models/activation/#NNlib.elu","content":" NNlib.elu  —  Function elu(x, α=1) = x > 0 ? x : α * (exp(x) - 1) Exponential Linear Unit activation function. See  \"Fast and Accurate Deep Network Learning by Exponential Linear Units\" . You can also specify the coefficient explicitly, e.g.  elu(x, 1) . julia> lineplot(elu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐       \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ elu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│       \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠔⠒⠋⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n        -1 │⠤⠤⠤⠤⠔⠒⠒⠒⠊⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           └────────────────────────────────────────┘       \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀       \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀       \n\njulia> elu(-10f0)\n-0.9999546f0\n\njulia> elu(-10f0, 2)\n-1.9999092f0"},{"id":114,"pagetitle":"Activation Functions","title":"NNlib.gelu","ref":"/flux/stable/models/activation/#NNlib.gelu","content":" NNlib.gelu  —  Function gelu(x) = 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x^3))) Activation function from  \"Gaussian Error Linear Units\" . julia> lineplot(gelu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊│ gelu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⣀⡠⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⣤⣤⣤⣤⣤⣤⣤⣤⡤⠤⠤⠤⠤⠤⠤⠤⣤⣤⣤⡤⡧⠶⠶⠭⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> lineplot(gelu, -5, 0, height=7);\n\njulia> lineplot!(ans, swish)\n             ┌────────────────────────────────────────┐         \n           0 │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠒⠒⠤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ gelu(x) \n             │⠑⠒⠢⠤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇│ swish(x)\n             │⠀⠀⠀⠀⠀⠈⠉⠒⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⠁│         \n   f(x)      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇⠀│         \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⣄⠀⠀⠀⠀⠀⢠⡞⠀⠀│         \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⢄⣀⣀⡤⢣⠃⠀⠀│         \n        -0.2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠇⠀⠀⠀│         \n             └────────────────────────────────────────┘         \n             ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0⠀         \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         "},{"id":115,"pagetitle":"Activation Functions","title":"NNlib.hardsigmoid","ref":"/flux/stable/models/activation/#NNlib.hardsigmoid","content":" NNlib.hardsigmoid  —  Function hardσ(x) = max(0, min(1, (x + 3) / 6)) Piecewise linear approximation of  sigmoid . julia> lineplot(hardsigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐         \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋⠉⠉⠉⠉⠉⠉⠉⠉│ hardσ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡠⠔⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⡗⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠋⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⠤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐     \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          └────────────────────────────────────────┘     \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀     "},{"id":116,"pagetitle":"Activation Functions","title":"NNlib.hardswish","ref":"/flux/stable/models/activation/#NNlib.hardswish","content":" NNlib.hardswish  —  Function hardswish(x) = x * hardσ(x) Hard-Swish activation function. See  \"Searching for MobileNetV3\" . julia> lineplot(hardswish, -2, 5, height = 7)\n           ┌────────────────────────────────────────┐             \n         5 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠔⠒⠉│ hardswish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠔⠒⠉⠁⠀⠀⠀⠀│             \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠖⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⣤⣤⣖⣚⣉⣁⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀│             \n        -1 │⠉⠒⠒⠒⠒⠉⠉⠉⠉⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           └────────────────────────────────────────┘             \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀             \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀             \n\njulia> lineplot(hardswish, -4, 0, height = 7);\n\njulia> lineplot!(ans, swish)\n             ┌────────────────────────────────────────┐             \n           0 │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⢣⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡜│ hardswish(x)\n             │⠒⠒⠢⠤⢄⣀⡀⠀⠀⠀⠀⠱⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠎⠀│ swish(x)    \n             │⠀⠀⠀⠀⠀⠀⠈⠉⠑⠒⠦⢄⣘⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡴⠃⠀⠀│             \n   f(x)      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⡖⠦⢄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⢔⠏⠁⠀⠀⠀│             \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠣⣄⠀⠉⠑⠒⠦⠤⢄⣀⣀⣀⣀⡠⠤⠖⣊⠕⠁⠀⠀⠀⠀⠀│             \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⠤⡀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀│             \n        -0.4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠒⠢⠤⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n             └────────────────────────────────────────┘             \n             ⠀-4⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0⠀             \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀             \n\njulia> hardswish.(-5:5)'\n1×11 adjoint(::Vector{Float64}) with eltype Float64:\n -0.0  -0.0  -0.0  -0.333333  -0.333333  0.0  0.666667  1.66667  3.0  4.0  5.0"},{"id":117,"pagetitle":"Activation Functions","title":"NNlib.hardtanh","ref":"/flux/stable/models/activation/#NNlib.hardtanh","content":" NNlib.hardtanh  —  Function hardtanh(x) = max(-1, min(1, x)) Segment-wise linear approximation of  tanh , much cheaper to compute. See  \"Large Scale Machine Learning\" . See also  tanh_fast . julia> lineplot(hardtanh, -2, 2, height=7)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⠔⠋⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ hardtanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⣀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡷⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠖⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠖⠋⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⠔⠋⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x\n\njulia> lineplot(tanh, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠤⠒⠒⠒⠊⠉⠉⠉│ tanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡷⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠔⠊⠁⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⣀⣀⣀⡠⠤⠤⠤⠖⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        "},{"id":118,"pagetitle":"Activation Functions","title":"NNlib.leakyrelu","ref":"/flux/stable/models/activation/#NNlib.leakyrelu","content":" NNlib.leakyrelu  —  Function leakyrelu(x, a=0.01) = max(a*x, x) Leaky  Rectified Linear Unit  activation function. You can also specify the coefficient explicitly, e.g.  leakyrelu(x, 0.01) . julia> lineplot(x -> leakyrelu(x, 0.5), -2, 2, height=7)\n           ┌────────────────────────────────────────┐       \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ #42(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│       \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⣤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠤⠒⠒⠋⠉⠁⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n        -1 │⣀⣀⠤⠤⠒⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           └────────────────────────────────────────┘       \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀       \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀       \n\njulia> leakyrelu(-10f0, 0.2)\n-2.0f0\n\njulia> leakyrelu(-10f0, 0.02)\n-0.5f0"},{"id":119,"pagetitle":"Activation Functions","title":"NNlib.lisht","ref":"/flux/stable/models/activation/#NNlib.lisht","content":" NNlib.lisht  —  Function lisht(x) = x * tanh(x) Activation function from   \"LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent ...\" julia> lineplot(lisht, -2, 2, height=7)\n          ┌────────────────────────────────────────┐         \n        2 │⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔│ lisht(x)\n          │⠀⠈⠑⢦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀│         \n          │⠀⠀⠀⠀⠈⠣⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠁⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⠔⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⢄⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡠⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⠦⣄⣀⣀⣇⣀⣀⠤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> lineplot!(ans, logcosh)\n          ┌────────────────────────────────────────┐           \n        2 │⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔│ lisht(x)  \n          │⠀⠈⠑⢦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀│ logcosh(x)\n          │⠢⣄⠀⠀⠈⠣⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⠀⠀⣀⠔│           \n   f(x)   │⠀⠈⠑⠢⣀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠁⠀⣀⠔⠊⠁⠀│           \n          │⠀⠀⠀⠀⠀⠉⠢⢄⡀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⠔⠋⠀⡠⠔⠋⠁⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠦⣌⡓⢄⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡠⠖⣁⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀│           \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠪⠷⣦⣄⣀⣀⣇⣀⣀⣤⠶⠕⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n          └────────────────────────────────────────┘           \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀           \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀           "},{"id":120,"pagetitle":"Activation Functions","title":"NNlib.logcosh","ref":"/flux/stable/models/activation/#NNlib.logcosh","content":" NNlib.logcosh  —  Function logcosh(x) Return  log(cosh(x))  which is computed in a numerically stable way. julia> lineplot(logcosh, -5, 5, height=7)\n          ┌────────────────────────────────────────┐           \n        5 │⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ logcosh(x)\n          │⠉⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋│           \n          │⠀⠀⠀⠑⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⠀│           \n   f(x)   │⠀⠀⠀⠀⠀⠀⠑⠦⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠊⠁⠀⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⠦⡀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⠦⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠢⢄⣀⣀⣇⣀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n          └────────────────────────────────────────┘           \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀           \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀           "},{"id":121,"pagetitle":"Activation Functions","title":"NNlib.logsigmoid","ref":"/flux/stable/models/activation/#NNlib.logsigmoid","content":" NNlib.logsigmoid  —  Function logσ(x) Return  log(σ(x))  which is computed in a numerically stable way. julia> lineplot(logsigmoid, -5, 5, height=7)\n           ┌────────────────────────────────────────┐        \n         0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡧⠤⠔⠒⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ logσ(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠉⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⢀⡤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⣀⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⡤⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -6 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        "},{"id":122,"pagetitle":"Activation Functions","title":"NNlib.mish","ref":"/flux/stable/models/activation/#NNlib.mish","content":" NNlib.mish  —  Function mish(x) = x * tanh(softplus(x)) Activation function from  \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\" . julia> lineplot(mish, -5, 5, height=7)\n           ┌────────────────────────────────────────┐        \n         5 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋│ mish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠒⠁⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠔⠋⠁⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣧⣔⣊⣁⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀│        \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        "},{"id":123,"pagetitle":"Activation Functions","title":"NNlib.relu","ref":"/flux/stable/models/activation/#NNlib.relu","content":" NNlib.relu  —  Function relu(x) = max(0, x) Rectified Linear Unit  activation function. julia> lineplot(relu, -2, 2, height=7)\n          ┌────────────────────────────────────────┐        \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋│ relu(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠊⠁⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀⠀⠀⠀⠀│        \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⡠⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⠔⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n          └────────────────────────────────────────┘        \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        "},{"id":124,"pagetitle":"Activation Functions","title":"NNlib.relu6","ref":"/flux/stable/models/activation/#NNlib.relu6","content":" NNlib.relu6  —  Function relu6(x) = min(max(0, x), 6) Rectified Linear Unit  activation function capped at 6. See  \"Convolutional Deep Belief Networks\"  from CIFAR-10. julia> lineplot(relu6, -10, 10, height=7)\n          ┌────────────────────────────────────────┐         \n        6 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠎⠉⠉⠉⠉⠉⠉⠉⠉│ relu6(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⡤⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⡠⠎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡔⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡧⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-10⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀10⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         "},{"id":125,"pagetitle":"Activation Functions","title":"NNlib.rrelu","ref":"/flux/stable/models/activation/#NNlib.rrelu","content":" NNlib.rrelu  —  Function rrelu(x, lo=1/8, hi=1/3) = max(a*x, x)\n# where `a` is randomly sampled from uniform distribution `U(lo, hi)` Randomized Leaky Rectified Linear Unit activation function. See  \"Empirical Evaluation of Rectified Activations\"  You can also specify the bound explicitly, e.g.  rrelu(x, 0.0, 1.0) . julia> lineplot(rrelu, -20, 10, height=7)\n            ┌────────────────────────────────────────┐         \n         10 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋│ rrelu(x)\n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀│         \n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)     │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⠤⣤⣤⢤⣤⣤⠤⠤⠤⢼⠮⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│         \n            │⣰⢀⣆⡄⣄⡄⡠⡰⠦⠷⡜⢢⠷⠳⠢⠊⠉⠉⠀⠀⠁⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n            │⠃⠉⠙⠘⠃⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        -10 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n            └────────────────────────────────────────┘         \n            ⠀-20⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀10⠀         \n            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> extrema(rrelu.(fill(-10f0, 1000)))\n(-3.3316886f0, -1.2548422f0)"},{"id":126,"pagetitle":"Activation Functions","title":"NNlib.selu","ref":"/flux/stable/models/activation/#NNlib.selu","content":" NNlib.selu  —  Function selu(x) = λ * (x ≥ 0 ? x : α * (exp(x) - 1))\n\nλ ≈ 1.05070...\nα ≈ 1.67326... Scaled exponential linear units. See  \"Self-Normalizing Neural Networks\" . julia> lineplot(selu, -3, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ selu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⠒│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⣀⠤⠖⠊⠉⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⡠⠤⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⣉⠭⠛⡏⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⡤⠤⠒⠊⠉⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -2 │⠤⠤⠖⠒⠒⠒⠒⠒⠒⠒⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> selu(-10f0)\n-1.7580194f0"},{"id":127,"pagetitle":"Activation Functions","title":"NNlib.sigmoid","ref":"/flux/stable/models/activation/#NNlib.sigmoid","content":" NNlib.sigmoid  —  Function σ(x) = 1 / (1 + exp(-x)) Classic  sigmoid  activation function. Unicode  σ  can be entered as  \\sigma  then tab, in many editors. The ascii name  sigmoid  is also exported. See also  sigmoid_fast . julia> using UnicodePlots\n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐     \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          └────────────────────────────────────────┘     \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀     \n\njulia> sigmoid === σ\ntrue"},{"id":128,"pagetitle":"Activation Functions","title":"NNlib.sigmoid_fast","ref":"/flux/stable/models/activation/#NNlib.sigmoid_fast","content":" NNlib.sigmoid_fast  —  Function sigmoid_fast(x) This is a faster, and very slightly less accurate, version of  sigmoid . For `x::Float32, perhaps 3 times faster, and maximum errors 2 eps instead of 1. See also  tanh_fast . julia> sigmoid(0.2f0)\n0.54983395f0\n\njulia> sigmoid_fast(0.2f0)\n0.54983395f0\n\njulia> hardσ(0.2f0)\n0.53333336f0"},{"id":129,"pagetitle":"Activation Functions","title":"NNlib.softplus","ref":"/flux/stable/models/activation/#NNlib.softplus","content":" NNlib.softplus  —  Function softplus(x) = log(exp(x) + 1) See  \"Deep Sparse Rectifier Neural Networks\" , JMLR 2011. julia> lineplot(softplus, -3, 3, height=7)\n          ┌────────────────────────────────────────┐            \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ softplus(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀│            \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠔⠊⠁⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡠⠤⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠤⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        0 │⣀⣀⣀⣀⣀⣀⣀⡠⠤⠤⠤⠤⠔⠒⠒⠚⠉⠉⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          └────────────────────────────────────────┘            \n          ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀            \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> lineplot!(ans, relu)\n          ┌────────────────────────────────────────┐            \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ softplus(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠│ relu(x)    \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⡴⠞⠋⠁│            \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⡴⠞⠋⠁⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡠⢤⡲⠝⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠤⠒⠊⣉⠥⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        0 │⣀⣀⣀⣀⣀⣀⣀⣠⣤⣤⣤⣤⣔⣒⣒⣚⣉⣉⣁⣀⣇⠴⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          └────────────────────────────────────────┘            \n          ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀            \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> softplus(16f0)\n16.0f0"},{"id":130,"pagetitle":"Activation Functions","title":"NNlib.softshrink","ref":"/flux/stable/models/activation/#NNlib.softshrink","content":" NNlib.softshrink  —  Function softshrink(x, λ=0.5) =\n    (x ≥ λ ? x - λ : (-λ ≥ x ? x + λ : 0)) See  \"Softshrink Activation Function\" . julia> lineplot(softshrink, -2, 2, height=7)\n           ┌────────────────────────────────────────┐              \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀│ softshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡤⠔⠒⠉⠁│              \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⠒⠋⠁⠀⠀⠀⠀⠀⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⡤⠤⠤⠤⠤⠤⠤⡧⠤⠤⠤⠤⠶⠮⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⠀⠀⠀⠀⠀⢀⣀⠤⠖⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⠀⣀⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -2 │⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀              \n\njulia> lineplot!(ans, tanhshrink)\n           ┌────────────────────────────────────────┐              \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀│ softshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡤⠔⠒⣉⡡│ tanhshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⣒⣋⠥⠤⠒⠊⠉⠁⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⣤⣤⣤⡤⠤⠤⠤⠤⠤⠤⡷⠶⠶⠶⠶⠶⠾⠿⠯⠭⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⢀⣀⡠⠤⠖⢒⣋⠭⠗⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⠊⣉⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -2 │⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀\n\njulia> softshrink.((-10f0, 10f0))\n(-9.5f0, 9.5f0)"},{"id":131,"pagetitle":"Activation Functions","title":"NNlib.softsign","ref":"/flux/stable/models/activation/#NNlib.softsign","content":" NNlib.softsign  —  Function softsign(x) = x / (1 + |x|) See  \"Quadratic Polynomials Learn Better Image Features\"  (2009). julia> lineplot(softsign, -5, 5, height=7)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⠤⠤⠤⠤⠤│ softsign(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⡤⠖⠒⠋⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⡔⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⠤⠤⠒⠋⠁⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⠒⠒⠒⠒⠒⠊⠉⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> lineplot!(ans, tanh)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡤⠖⠊⠉⠉⠉⣉⣉⣉⣉⣉⠭⠭⠭⠭⠭│ softsign(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡔⣃⡤⠖⠒⠋⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│ tanh(x)    \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣧⡞⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡴⠃⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⠤⠤⠒⢋⠕⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⣒⣒⣒⣒⣒⣊⣉⣉⣉⣉⣁⣀⣀⡠⠤⠒⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> softsign(1f0)\n0.5f0\n\njulia> softsign(100f0)\n0.990099f0"},{"id":132,"pagetitle":"Activation Functions","title":"NNlib.swish","ref":"/flux/stable/models/activation/#NNlib.swish","content":" NNlib.swish  —  Function swish(x) = x * σ(x) Self-gated activation function. See  \"Swish: a Self-Gated Activation Function\" . julia> lineplot(swish, -2, 2, height=7)\n           ┌────────────────────────────────────────┐         \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤│ swish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋⠁⠀│         \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀│         \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⢀⣀⡤⠔⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⣤⣤⡤⡧⠴⠶⠯⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│         \n           │⠉⠑⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠉⠉⠉⠉⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n           └────────────────────────────────────────┘         \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀         \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         "},{"id":133,"pagetitle":"Activation Functions","title":"NNlib.tanhshrink","ref":"/flux/stable/models/activation/#NNlib.tanhshrink","content":" NNlib.tanhshrink  —  Function tanhshrink(x) = x - tanh(x) See  \"Tanhshrink Activation Function\" . julia> lineplot(tanhshrink, -3, 3, height=7)\n           ┌────────────────────────────────────────┐              \n         3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ tanhshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠊│              \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⣀⡠⠤⠒⠊⠉⠁⠀⠀⠀⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⣤⡤⠤⠤⠤⠤⠤⠤⡷⠶⠶⠶⠶⠶⠮⠭⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⠀⠀⠀⠀⣀⡠⠴⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⡠⠴⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀              \n\njulia> tanhshrink.((-10f0, 10f0))\n(-9.0f0, 9.0f0)"},{"id":134,"pagetitle":"Activation Functions","title":"NNlib.tanh_fast","ref":"/flux/stable/models/activation/#NNlib.tanh_fast","content":" NNlib.tanh_fast  —  Function tanh_fast(x) This is a faster but slighly less accurate version of  tanh . Where Julia's  tanh  function has an error under 2 eps, this may be wrong by 5 eps, a reduction by less than one decimal digit.  For  x::Float32  this is usually about 10 times faster, with a smaller speedup for  x::Float64 . For any other number types, it just calls  tanh . See also  sigmoid_fast . julia> tanh(0.5f0)\n0.46211717f0\n\njulia> tanh_fast(0.5f0)\n0.46211714f0\n\njulia> hard_tanh(0.5f0)\n0.5f0"},{"id":135,"pagetitle":"Activation Functions","title":"NNlib.trelu","ref":"/flux/stable/models/activation/#NNlib.trelu","content":" NNlib.trelu  —  Function trelu(x, theta=1) = x > theta ? x : 0 Threshold gated rectified linear activation function. See  \"Zero-bias autoencoders and the benefits of co-adapting features\" julia> lineplot(trelu, -2, 4, height=7)\n          ┌────────────────────────────────────────┐         \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋│ trelu(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠴⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣠⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⣀⣀⣀⣀⣀⣀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀4⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         "},{"id":136,"pagetitle":"Activation Functions","title":"One More","ref":"/flux/stable/models/activation/#One-More","content":" One More Julia's  Base.Math  also provides  tanh , which can be used as an activation function. Note that many Flux layers will automatically replace this with  NNlib.tanh_fast  when called, as Base's  tanh  is slow enough to sometimes be a bottleneck. julia> using UnicodePlots\n\njulia> lineplot(tanh, -3, 3, height=7)\n           ┌────────────────────────────────────────┐        \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⣀⠤⠔⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉│ tanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⡰⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⡤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠎⠁⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠴⠊⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⡤⠤⠔⠒⠉⠁⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        "},{"id":139,"pagetitle":"Custom Layers","title":"Defining Customised Layers","ref":"/flux/stable/models/advanced/#man-advanced","content":" Defining Customised Layers Here we will try and describe usage of some more advanced features that Flux provides to give more control over model building."},{"id":140,"pagetitle":"Custom Layers","title":"Custom Model Example","ref":"/flux/stable/models/advanced/#Custom-Model-Example","content":" Custom Model Example Here is a basic example of a custom model. It simply adds the input to the result from the neural network. struct CustomModel\n  chain::Chain\nend\n\nfunction (m::CustomModel)(x)\n  # Arbitrary code can go here, but note that everything will be differentiated.\n  # Zygote does not allow some operations, like mutating arrays.\n\n  return m.chain(x) + x\nend\n\n# Call @functor to allow for training. Described below in more detail.\nFlux.@functor CustomModel You can then use the model like: chain = Chain(Dense(10, 10))\nmodel = CustomModel(chain)\nmodel(rand(10)) For an intro to Flux and automatic differentiation, see this  tutorial ."},{"id":141,"pagetitle":"Custom Layers","title":"Customising Parameter Collection for a Model","ref":"/flux/stable/models/advanced/#Customising-Parameter-Collection-for-a-Model","content":" Customising Parameter Collection for a Model Taking reference from our example  Affine  layer from the  basics . By default all the fields in the  Affine  type are collected as its parameters, however, in some cases it may be desired to hold other metadata in our \"layers\" that may not be needed for training, and are hence supposed to be ignored while the parameters are collected. With Flux, the way to mark some fields of our layer as trainable is through overloading the  trainable  function: julia> Flux.@functor Affine\n\njulia> a = Affine(Float32[1 2; 3 4; 5 6], Float32[7, 8, 9])\nAffine(Float32[1.0 2.0; 3.0 4.0; 5.0 6.0], Float32[7.0, 8.0, 9.0])\n\njulia> Flux.params(a) # default behavior\nParams([Float32[1.0 2.0; 3.0 4.0; 5.0 6.0], Float32[7.0, 8.0, 9.0]])\n\njulia> Flux.trainable(a::Affine) = (; a.W)  # returns a NamedTuple using the field's name\n\njulia> Flux.params(a)\nParams([Float32[1.0 2.0; 3.0 4.0; 5.0 6.0]]) Only the fields returned by  trainable  will be collected as trainable parameters of the layer when calling  Flux.params , and only these fields will be seen by  Flux.setup  and  Flux.update!  for training. But all fields wil be seen by  gpu  and similar functions, for example: julia> a |> f16\nAffine(Float16[1.0 2.0; 3.0 4.0; 5.0 6.0], Float16[7.0, 8.0, 9.0]) Note that there is no need to overload  trainable  to hide fields which do not contain trainable parameters. (For example, activation functions, or Boolean flags.) These are always ignored by  params  and by training: julia> Flux.params(Affine(true, [10, 11, 12.0]))\nParams([]) It is also possible to further restrict what fields are seen by writing  @functor Affine (W,) . However, this is not recommended. This requires the  struct  to have a corresponding constructor that accepts only  W  as an argument, and the ignored fields will not be seen by functions like  gpu  (which is usually undesired)."},{"id":142,"pagetitle":"Custom Layers","title":"Freezing Layer Parameters","ref":"/flux/stable/models/advanced/#Freezing-Layer-Parameters","content":" Freezing Layer Parameters When it is desired to not include all the model parameters (for e.g. transfer learning), we can simply not pass in those layers into our call to  params . Flux ≤ 0.14 The mechanism described here is for Flux's old \"implicit\" training style. When upgrading for Flux 0.15, it should be replaced by  freeze!  and  thaw! . Consider a simple multi-layer perceptron model where we want to avoid optimising the first two  Dense  layers. We can obtain this using the slicing features  Chain  provides: m = Chain(\n      Dense(784 => 64, relu),\n      Dense(64 => 64, relu),\n      Dense(32 => 10)\n    );\n\nps = Flux.params(m[3:end]) The  Zygote.Params  object  ps  now holds a reference to only the parameters of the layers passed to it. During training, the gradients will only be computed for (and applied to) the last  Dense  layer, therefore only that would have its parameters changed. Flux.params  also takes multiple inputs to make it easy to collect parameters from heterogenous models with a single call. A simple demonstration would be if we wanted to omit optimising the second  Dense  layer in the previous example. It would look something like this: Flux.params(m[1], m[3:end]) Sometimes, a more fine-tuned control is needed. We can freeze a specific parameter of a specific layer which already entered a  Params  object  ps , by simply deleting it from  ps : ps = Flux.params(m)\ndelete!(ps, m[2].bias) "},{"id":143,"pagetitle":"Custom Layers","title":"Custom multiple input or output layer","ref":"/flux/stable/models/advanced/#Custom-multiple-input-or-output-layer","content":" Custom multiple input or output layer Sometimes a model needs to receive several separate inputs at once or produce several separate outputs at once. In other words, there multiple paths within this high-level layer, each processing a different input or producing a different output. A simple example of this in machine learning literature is the  inception module . Naively, we could have a struct that stores the weights of along each path and implement the joining/splitting in the forward pass function. But that would mean a new struct any time the operations along each path changes. Instead, this guide will show you how to construct a high-level layer (like  Chain ) that is made of multiple sub-layers for each path."},{"id":144,"pagetitle":"Custom Layers","title":"Multiple inputs: a custom  Join  layer","ref":"/flux/stable/models/advanced/#Multiple-inputs:-a-custom-Join-layer","content":" Multiple inputs: a custom  Join  layer Our custom  Join  layer will accept multiple inputs at once, pass each input through a separate path, then combine the results together. Note that this layer can already be constructed using  Parallel , but we will first walk through how do this manually. We start by defining a new struct,  Join , that stores the different paths and a combine operation as its fields. using Flux\nusing CUDA\n\n# custom join layer\nstruct Join{T, F}\n  combine::F\n  paths::T\nend\n\n# allow Join(op, m1, m2, ...) as a constructor\nJoin(combine, paths...) = Join(combine, paths) Notice that we parameterized the type of the  paths  field. This is necessary for fast Julia code; in general,  T  might be a  Tuple  or  Vector , but we don't need to pay attention to what it specifically is. The same goes for the  combine  field. The next step is to use  Functors.@functor  to make our struct behave like a Flux layer. This is important so that calling  params  on a  Join  returns the underlying weight arrays on each path. Flux.@functor Join Finally, we define the forward pass. For  Join , this means applying each  path  in  paths  to each input array, then using  combine  to merge the results. (m::Join)(xs::Tuple) = m.combine(map((f, x) -> f(x), m.paths, xs)...)\n(m::Join)(xs...) = m(xs) Lastly, we can test our new layer. Thanks to the proper abstractions in Julia, our layer works on GPU arrays out of the box! model = Chain(\n              Join(vcat,\n                   Chain(Dense(1 => 5, relu), Dense(5 => 1)), # branch 1\n                   Dense(1 => 2),                             # branch 2\n                   Dense(1 => 1)                              # branch 3\n                  ),\n              Dense(4 => 1)\n             ) |> gpu\n\nxs = map(gpu, (rand(1), rand(1), rand(1)))\n\nmodel(xs)\n# returns a single float vector with one value Note This  Join  layer is available from the  Fluxperimental.jl  package."},{"id":145,"pagetitle":"Custom Layers","title":"Using  Parallel","ref":"/flux/stable/models/advanced/#Using-Parallel","content":" Using  Parallel Flux already provides  Parallel  that can offer the same functionality. In this case,  Join  is going to just be syntactic sugar for  Parallel . Join(combine, paths) = Parallel(combine, paths)\nJoin(combine, paths...) = Join(combine, paths)\n\n# use vararg/tuple version of Parallel forward pass\nmodel = Chain(\n              Join(vcat,\n                   Chain(Dense(1 => 5, relu), Dense(5 => 1)),\n                   Dense(1 => 2),\n                   Dense(1 => 1)\n                  ),\n              Dense(4 => 1)\n             ) |> gpu\n\nxs = map(gpu, (rand(1), rand(1), rand(1)))\n\nmodel(xs)\n# returns a single float vector with one value"},{"id":146,"pagetitle":"Custom Layers","title":"Multiple outputs: a custom  Split  layer","ref":"/flux/stable/models/advanced/#Multiple-outputs:-a-custom-Split-layer","content":" Multiple outputs: a custom  Split  layer Our custom  Split  layer will accept a single input, then pass the input through a separate path to produce multiple outputs. We start by following the same steps as the  Join  layer: define a struct, use  Functors.@functor , and define the forward pass. using Flux\nusing CUDA\n\n# custom split layer\nstruct Split{T}\n  paths::T\nend\n\nSplit(paths...) = Split(paths)\n\nFlux.@functor Split\n\n(m::Split)(x::AbstractArray) = map(f -> f(x), m.paths) Now we can test to see that our  Split  does indeed produce multiple outputs. model = Chain(\n              Dense(10 => 5),\n              Split(Dense(5 => 1, tanh), Dense(5 => 3, tanh), Dense(5 => 2))\n             ) |> gpu\n\nmodel(gpu(rand(10)))\n# returns a tuple with three float vectors A custom loss function for the multiple outputs may look like this: using Statistics\n\n# assuming model returns the output of a Split\n# x is a single input\n# ys is a tuple of outputs\nfunction loss(x, ys, model)\n  # rms over all the mse\n  ŷs = model(x)\n  return sqrt(mean(Flux.mse(y, ŷ) for (y, ŷ) in zip(ys, ŷs)))\nend Note This  Split  layer is available from the  Fluxperimental.jl  package."},{"id":149,"pagetitle":"Gradients and Layers","title":"How Flux Works: Gradients and Layers","ref":"/flux/stable/models/basics/#man-basics","content":" How Flux Works: Gradients and Layers"},{"id":150,"pagetitle":"Gradients and Layers","title":"Taking Gradients","ref":"/flux/stable/models/basics/#man-taking-gradients","content":" Taking Gradients Flux's core feature is taking gradients of Julia code. The  gradient  function takes another Julia function  f  and a set of arguments, and returns the gradient with respect to each argument. (It's a good idea to try pasting these examples in the Julia terminal.) julia> using Flux\n\njulia> f(x) = 3x^2 + 2x + 1;\n\njulia> df(x) = gradient(f, x)[1]; # df/dx = 6x + 2\n\njulia> df(2)\n14.0\n\njulia> d2f(x) = gradient(df, x)[1]; # d²f/dx² = 6\n\njulia> d2f(2)\n6.0 When a function has many parameters, we can get gradients of each one at the same time: julia> f(x, y) = sum((x .- y).^2);\n\njulia> gradient(f, [2, 1], [2, 0])\n([0.0, 2.0], [-0.0, -2.0]) These gradients are based on  x  and  y . Flux works by instead taking gradients based on the weights and biases that make up the parameters of a model. Machine learning often can have  hundreds  of parameter arrays. Instead of passing them to  gradient  individually, we can store them together in a structure. The simplest example is a named tuple, created by the following syntax: julia> nt = (a = [2, 1], b = [2, 0], c = tanh);\n\njulia> g(x::NamedTuple) = sum(abs2, x.a .- x.b);\n\njulia> g(nt)\n1\n\njulia> dg_nt = gradient(g, nt)[1]\n(a = [0.0, 2.0], b = [-0.0, -2.0], c = nothing) Notice that  gradient  has returned a matching structure. The field  dg_nt.a  is the gradient for  nt.a , and so on. Some fields have no gradient, indicated by  nothing .  Rather than define a function like  g  every time (and think up a name for it), it is often useful to use anonymous functions: this one is  x -> sum(abs2, x.a .- x.b) . Anonymous functions can be defined either with  ->  or with  do , and such  do  blocks are often useful if you have a few steps to perform: julia> gradient((x, y) -> sum(abs2, x.a ./ y .- x.b), nt, [1, 2])\n((a = [0.0, 0.5], b = [-0.0, -1.0], c = nothing), [-0.0, -0.25])\n\njulia> gradient(nt, [1, 2]) do x, y\n         z = x.a ./ y\n         sum(abs2, z .- x.b)\n       end\n((a = [0.0, 0.5], b = [-0.0, -1.0], c = nothing), [-0.0, -0.25]) Sometimes you may want to know the value of the function, as well as its gradient. Rather than calling the function a second time, you can call  withgradient  instead: julia> Flux.withgradient(g, nt)\n(val = 1, grad = ((a = [0.0, 2.0], b = [-0.0, -2.0], c = nothing),)) Implicit gradients Flux used to handle many parameters in a different way, using the  params  function. This uses a method of  gradient  which takes a zero-argument function, and returns a dictionary through which the resulting gradients can be looked up: julia> x = [2, 1];\n\njulia> y = [2, 0];\n\njulia> gs = gradient(Flux.params(x, y)) do\n         f(x, y)\n       end\nGrads(...)\n\njulia> gs[x]\n2-element Vector{Float64}:\n 0.0\n 2.0\n\njulia> gs[y]\n2-element Vector{Float64}:\n -0.0\n -2.0"},{"id":151,"pagetitle":"Gradients and Layers","title":"Building Simple Models","ref":"/flux/stable/models/basics/#Building-Simple-Models","content":" Building Simple Models Consider a simple linear regression, which tries to predict an output array  y  from an input  x . W = rand(2, 5)\nb = rand(2)\n\npredict(x) = W*x .+ b\n\nfunction loss(x, y)\n  ŷ = predict(x)\n  sum((y .- ŷ).^2)\nend\n\nx, y = rand(5), rand(2) # Dummy data\nloss(x, y) # ~ 3 To improve the prediction we can take the gradients of the loss with respect to  W  and  b  and perform gradient descent. using Flux\n\ngs = gradient(() -> loss(x, y), Flux.params(W, b)) Now that we have gradients, we can pull them out and update  W  to train the model. W̄ = gs[W]\n\nW .-= 0.1 .* W̄\n\nloss(x, y) # ~ 2.5 The loss has decreased a little, meaning that our prediction  x  is closer to the target  y . If we have some data we can already try  training the model . All deep learning in Flux, however complex, is a simple generalisation of this example. Of course, models can  look  very different – they might have millions of parameters or complex control flow. Let's see how Flux handles more complex models."},{"id":152,"pagetitle":"Gradients and Layers","title":"Building Layers","ref":"/flux/stable/models/basics/#Building-Layers","content":" Building Layers It's common to create more complex models than the linear regression above. For example, we might want to have two linear layers with a nonlinearity like  sigmoid  ( σ ) in between them. In the above style we could write this as: using Flux\n\nW1 = rand(3, 5)\nb1 = rand(3)\nlayer1(x) = W1 * x .+ b1\n\nW2 = rand(2, 3)\nb2 = rand(2)\nlayer2(x) = W2 * x .+ b2\n\nmodel(x) = layer2(σ.(layer1(x)))\n\nmodel(rand(5)) # => 2-element vector This works but is fairly unwieldy, with a lot of repetition – especially as we add more layers. One way to factor this out is to create a function that returns linear layers. function linear(in, out)\n  W = randn(out, in)\n  b = randn(out)\n  x -> W * x .+ b\nend\n\nlinear1 = linear(5, 3) # we can access linear1.W etc\nlinear2 = linear(3, 2)\n\nmodel(x) = linear2(σ.(linear1(x)))\n\nmodel(rand(5)) # => 2-element vector Another (equivalent) way is to create a struct that explicitly represents the affine layer. struct Affine\n  W\n  b\nend\n\nAffine(in::Integer, out::Integer) =\n  Affine(randn(out, in), randn(out))\n\n# Overload call, so the object can be used as a function\n(m::Affine)(x) = m.W * x .+ m.b\n\na = Affine(10, 5)\n\na(rand(10)) # => 5-element vector Congratulations! You just built the  Dense  layer that comes with Flux. Flux has many interesting layers available, but they're all things you could have built yourself very easily. (There is one small difference with  Dense  – for convenience it also takes an activation function, like  Dense(10 => 5, σ) .)"},{"id":153,"pagetitle":"Gradients and Layers","title":"Stacking It Up","ref":"/flux/stable/models/basics/#Stacking-It-Up","content":" Stacking It Up It's pretty common to write models that look something like: layer1 = Dense(10 => 5, σ)\n# ...\nmodel(x) = layer3(layer2(layer1(x))) For long chains, it might be a bit more intuitive to have a list of layers, like this: using Flux\n\nlayers = [Dense(10 => 5, σ), Dense(5 => 2), softmax]\n\nmodel(x) = foldl((x, m) -> m(x), layers, init = x)\n\nmodel(rand(10)) # => 2-element vector Handily, this is also provided for in Flux: model2 = Chain(\n  Dense(10 => 5, σ),\n  Dense(5 => 2),\n  softmax)\n\nmodel2(rand(10)) # => 2-element vector This quickly starts to look like a high-level deep learning library; yet you can see how it falls out of simple abstractions, and we lose none of the power of Julia code. A nice property of this approach is that because \"models\" are just functions (possibly with trainable parameters), you can also see this as simple function composition. m = Dense(5 => 2) ∘ Dense(10 => 5, σ)\n\nm(rand(10)) Likewise,  Chain  will happily work with any Julia function. m = Chain(x -> x^2, x -> x+1)\n\nm(5) # => 26"},{"id":154,"pagetitle":"Gradients and Layers","title":"Layer Helpers","ref":"/flux/stable/models/basics/#Layer-Helpers","content":" Layer Helpers There is still one problem with this  Affine  layer, that Flux does not know to look inside it. This means that  Flux.train!  won't see its parameters, nor will  gpu  be able to move them to your GPU. These features are enabled by the  @functor  macro: Flux.@functor Affine Finally, most Flux layers make bias optional, and allow you to supply the function used for generating random weights. We can easily add these refinements to the  Affine  layer as follows, using the helper function  create_bias : function Affine((in, out)::Pair; bias=true, init=Flux.randn32)\n  W = init(out, in)\n  b = Flux.create_bias(W, bias, out)\n  Affine(W, b)\nend\n\nAffine(3 => 1, bias=false, init=ones) |> gpu"},{"id":157,"pagetitle":"Nested Structures – Functors.jl","title":"Recursive transformations from Functors.jl","ref":"/flux/stable/models/functors/#Recursive-transformations-from-Functors.jl","content":" Recursive transformations from Functors.jl Flux models are deeply nested structures, and  Functors.jl  provides tools needed to explore such objects, apply functions to the parameters they contain, and re-build them. New layers should be annotated using the  Functors.@functor  macro. This will enable  params  to see the parameters inside, and  gpu  to move them to the GPU. Functors.jl  has its own  notes on basic usage  for more details. Additionally, the  Advanced Model Building and Customisation  page covers the use cases of  Functors  in greater details."},{"id":158,"pagetitle":"Nested Structures – Functors.jl","title":"Functors.@functor","ref":"/flux/stable/models/functors/#Functors.@functor","content":" Functors.@functor  —  Macro @functor T\n@functor T (x,) Adds methods to  functor  allowing recursion into objects of type  T , and reconstruction. Assumes that  T  has a constructor accepting all of its fields, which is true unless you have provided an inner constructor which does not. By default all fields of  T  are considered  children ;  this can be restricted be restructed by providing a tuple of field names. Examples julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> Functors.children(Foo(1,2))\n(x = 1, y = 2)\n\njulia> _, re = Functors.functor(Foo(1,2));\n\njulia> re((10, 20))\nFoo(10, 20)\n\njulia> struct TwoThirds a; b; c; end\n\njulia> @functor TwoThirds (a, c)\n\njulia> ch2, re3 = Functors.functor(TwoThirds(10,20,30));\n\njulia> ch2\n(a = 10, c = 30)\n\njulia> re3((\"ten\", \"thirty\"))\nTwoThirds(\"ten\", 20, \"thirty\")\n\njulia> fmap(x -> 10x, TwoThirds(Foo(1,2), Foo(3,4), 56))\nTwoThirds(Foo(10, 20), Foo(3, 4), 560)"},{"id":159,"pagetitle":"Nested Structures – Functors.jl","title":"Functors.fmap","ref":"/flux/stable/models/functors/#Functors.fmap","content":" Functors.fmap  —  Function fmap(f, x, ys...; exclude = Functors.isleaf, walk = Functors.DefaultWalk()[, prune]) A structure and type preserving  map . By default it transforms every leaf node (identified by  exclude , default  isleaf ) by applying  f , and otherwise traverses  x  recursively using  functor . Optionally, it may also be associated with objects  ys  with the same tree structure. In that case,  f  is applied to the corresponding leaf nodes in  x  and  ys . Examples julia> fmap(string, (x=1, y=(2, 3)))\n(x = \"1\", y = (\"2\", \"3\"))\n\njulia> nt = (a = [1,2], b = [23, (45,), (x=6//7, y=())], c = [8,9]);\n\njulia> fmap(println, nt)\n[1, 2]\n23\n45\n6//7\n()\n[8, 9]\n(a = nothing, b = Any[nothing, (nothing,), (x = nothing, y = nothing)], c = nothing)\n\njulia> fmap(println, nt; exclude = x -> x isa Array)\n[1, 2]\nAny[23, (45,), (x = 6//7, y = ())]\n[8, 9]\n(a = nothing, b = nothing, c = nothing)\n\njulia> twice = [1, 2];  # println only acts once on this\n\njulia> fmap(println, (i = twice, ii = 34, iii = [5, 6], iv = (twice, 34), v = 34.0))\n[1, 2]\n34\n[5, 6]\n34\n34.0\n(i = nothing, ii = nothing, iii = nothing, iv = (nothing, nothing), v = nothing)\n\njulia> d1 = Dict(\"x\" => [1,2], \"y\" => 3);\n\njulia> d2 = Dict(\"x\" => [4,5], \"y\" => 6, \"z\" => \"an_extra_value\");\n\njulia> fmap(+, d1, d2) == Dict(\"x\" => [5, 7], \"y\" => 9) # Note that \"z\" is ignored\ntrue Mutable objects which appear more than once are only handled once (by caching  f(x)  in an  IdDict ). Thus the relationship  x.i === x.iv[1]  will be preserved. An immutable object which appears twice is not stored in the cache, thus  f(34)  will be called twice, and the results will agree only if  f  is pure. By default,  Tuple s,  NamedTuple s, and some other container-like types in Base have children to recurse into. Arrays of numbers do not. To enable recursion into new types, you must provide a method of  functor , which can be done using the macro  @functor : julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> struct Bar; x; end\n\njulia> @functor Bar\n\njulia> m = Foo(Bar([1,2,3]), (4, 5, Bar(Foo(6, 7))));\n\njulia> fmap(x -> 10x, m)\nFoo(Bar([10, 20, 30]), (40, 50, Bar(Foo(60, 70))))\n\njulia> fmap(string, m)\nFoo(Bar(\"[1, 2, 3]\"), (\"4\", \"5\", Bar(Foo(\"6\", \"7\"))))\n\njulia> fmap(string, m, exclude = v -> v isa Bar)\nFoo(\"Bar([1, 2, 3])\", (4, 5, \"Bar(Foo(6, 7))\")) To recurse into custom types without reconstructing them afterwards, use  fmapstructure . For advanced customization of the traversal behaviour, pass a custom  walk  function that subtypes  Functors.AbstractWalk . The call  fmap(f, x, ys...; walk = mywalk)  will wrap  mywalk  in  ExcludeWalk  then  CachedWalk . Here,  ExcludeWalk  is responsible for applying  f  at excluded nodes. For a low-level interface for executing a user-constructed walk, see  execute . julia> struct MyWalk <: Functors.AbstractWalk end\n\njulia> (::MyWalk)(recurse, x) = x isa Bar ? \"hello\" :\n                                            Functors.DefaultWalk()(recurse, x)\n\njulia> fmap(x -> 10x, m; walk = MyWalk())\nFoo(\"hello\", (40, 50, \"hello\")) The behaviour when the same node appears twice can be altered by giving a value to the  prune  keyword, which is then used in place of all but the first: julia> twice = [1, 2];\n\njulia> fmap(float, (x = twice, y = [1,2], z = twice); prune = missing)\n(x = [1.0, 2.0], y = [1.0, 2.0], z = missing)"},{"id":160,"pagetitle":"Nested Structures – Functors.jl","title":"Functors.isleaf","ref":"/flux/stable/models/functors/#Functors.isleaf","content":" Functors.isleaf  —  Function Functors.isleaf(x) Return true if  x  has no  children  according to  functor . Examples julia> Functors.isleaf(1)\ntrue\n\njulia> Functors.isleaf([2, 3, 4])\ntrue\n\njulia> Functors.isleaf([\"five\", [6, 7]])\nfalse\n\njulia> Functors.isleaf([])\nfalse\n\njulia> Functors.isleaf((8, 9))\nfalse\n\njulia> Functors.isleaf(())\ntrue"},{"id":161,"pagetitle":"Nested Structures – Functors.jl","title":"Functors.children","ref":"/flux/stable/models/functors/#Functors.children","content":" Functors.children  —  Function Functors.children(x) Return the children of  x  as defined by  functor . Equivalent to  functor(x)[1] ."},{"id":162,"pagetitle":"Nested Structures – Functors.jl","title":"Functors.fcollect","ref":"/flux/stable/models/functors/#Functors.fcollect","content":" Functors.fcollect  —  Function fcollect(x; exclude = v -> false) Traverse  x  by recursing each child of  x  as defined by  functor  and collecting the results into a flat array, ordered by a breadth-first traversal of  x , respecting the iteration order of  children  calls. Doesn't recurse inside branches rooted at nodes  v  for which  exclude(v) == true . In such cases, the root  v  is also excluded from the result. By default,  exclude  always yields  false . See also  children . Examples julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> struct Bar; x; end\n\njulia> @functor Bar\n\njulia> struct TypeWithNoChildren; x; y; end\n\njulia> m = Foo(Bar([1,2,3]), TypeWithNoChildren(:a, :b))\nFoo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n\njulia> fcollect(m)\n4-element Vector{Any}:\n Foo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n Bar([1, 2, 3])\n [1, 2, 3]\n TypeWithNoChildren(:a, :b)\n\njulia> fcollect(m, exclude = v -> v isa Bar)\n2-element Vector{Any}:\n Foo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n TypeWithNoChildren(:a, :b)\n\njulia> fcollect(m, exclude = v -> Functors.isleaf(v))\n2-element Vector{Any}:\n Foo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n Bar([1, 2, 3])"},{"id":163,"pagetitle":"Nested Structures – Functors.jl","title":"Functors.functor","ref":"/flux/stable/models/functors/#Functors.functor","content":" Functors.functor  —  Function Functors.functor(x) = functor(typeof(x), x) Returns a tuple containing, first, a  NamedTuple  of the children of  x  (typically its fields), and second, a reconstruction funciton. This controls the behaviour of  fmap . Methods should be added to  functor(::Type{T}, x)  for custom types, usually using the macro  @functor ."},{"id":164,"pagetitle":"Nested Structures – Functors.jl","title":"Functors.fmapstructure","ref":"/flux/stable/models/functors/#Functors.fmapstructure","content":" Functors.fmapstructure  —  Function fmapstructure(f, x; exclude = isleaf) Like  fmap , but doesn't preserve the type of custom structs. Instead, it returns a  NamedTuple  (or a  Tuple , or an array), or a nested set of these. Useful for when the output must not contain custom structs. Examples julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> m = Foo([1,2,3], [4, (5, 6), Foo(7, 8)]);\n\njulia> fmapstructure(x -> 2x, m)\n(x = [2, 4, 6], y = Any[8, (10, 12), (x = 14, y = 16)])\n\njulia> fmapstructure(println, m)\n[1, 2, 3]\n4\n5\n6\n7\n8\n(x = nothing, y = Any[nothing, (nothing, nothing), (x = nothing, y = nothing)])"},{"id":165,"pagetitle":"Nested Structures – Functors.jl","title":"Moving models, or data, to the GPU","ref":"/flux/stable/models/functors/#Moving-models,-or-data,-to-the-GPU","content":" Moving models, or data, to the GPU Flux provides some convenience functions based on  fmap . Some ( f16 ,  f32 ,  f64 ) change the precision of all arrays in a model. Others are used for moving a model to of from GPU memory:"},{"id":166,"pagetitle":"Nested Structures – Functors.jl","title":"Flux.cpu","ref":"/flux/stable/models/functors/#Flux.cpu","content":" Flux.cpu  —  Function cpu(m) Copies  m  onto the CPU, the opposite of  gpu . Recurses into structs marked  @functor . Example julia> m_gpu = Dense(CUDA.randn(2, 5))\nDense(5 => 2)       # 12 parameters\n\njulia> m_gpu.bias  # matches the given weight matrix\n2-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n 0.0\n 0.0\n\njulia> m = m_gpu |> cpu\nDense(5 => 2)       # 12 parameters\n\njulia> m.bias\n2-element Vector{Float32}:\n 0.0\n 0.0 source"},{"id":167,"pagetitle":"Nested Structures – Functors.jl","title":"Flux.gpu","ref":"/flux/stable/models/functors/#Flux.gpu-Tuple{Any}","content":" Flux.gpu  —  Method gpu(m) Copies  m  to the current GPU device (using current GPU backend), if one is available. If no GPU is available, it does nothing (but prints a warning the first time). On arrays, this calls CUDA's  cu , which also changes arrays with Float64 elements to Float32 while copying them to the device (same for AMDGPU). To act on arrays within a struct, the struct type must be marked with  @functor . Use  cpu  to copy back to ordinary  Array s. See also  f32  and  f16  to change element type only. See the  CUDA.jl docs   to help identify the current device. Example julia> m = Dense(rand(2, 3))  # constructed with Float64 weight matrix\nDense(3 => 2)       # 8 parameters\n\njulia> typeof(m.weight)\nMatrix{Float64} (alias for Array{Float64, 2})\n\njulia> m_gpu = gpu(m)  # can equivalently be written m_gpu = m |> gpu\nDense(3 => 2)       # 8 parameters\n\njulia> typeof(m_gpu.weight)\nCUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer} source"},{"id":168,"pagetitle":"Nested Structures – Functors.jl","title":"Flux.gpu","ref":"/flux/stable/models/functors/#Flux.gpu-Tuple{DataLoader}","content":" Flux.gpu  —  Method gpu(data::DataLoader) Transforms a given  DataLoader  to apply  gpu  to each batch of data, when iterated over. (If no GPU is available, this does nothing.) Example julia> dl = Flux.DataLoader((x = ones(2,10), y='a':'j'), batchsize=3)\n4-element DataLoader(::NamedTuple{(:x, :y), Tuple{Matrix{Float64}, StepRange{Char, Int64}}}, batchsize=3)\n  with first element:\n  (; x = 2×3 Matrix{Float64}, y = 3-element StepRange{Char, Int64})\n\njulia> first(dl)\n(x = [1.0 1.0 1.0; 1.0 1.0 1.0], y = 'a':1:'c')\n\njulia> c_dl = gpu(dl)\n4-element DataLoader(::MLUtils.MappedData{:auto, typeof(gpu), NamedTuple{(:x, :y), Tuple{Matrix{Float64}, StepRange{Char, Int64}}}}, batchsize=3)\n  with first element:\n  (; x = 2×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, y = 3-element StepRange{Char, Int64})\n\njulia> first(c_dl).x\n2×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0 For large datasets, this is preferred over moving all the data to the GPU before creating the  DataLoader , like this: julia> Flux.DataLoader((x = ones(2,10), y=2:11) |> gpu, batchsize=3)\n4-element DataLoader(::NamedTuple{(:x, :y), Tuple{CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, UnitRange{Int64}}}, batchsize=3)\n  with first element:\n  (; x = 2×3 CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, y = 3-element UnitRange{Int64}) Warning This only works if  gpu  is applied directly to the  DataLoader . While  gpu  acts recursively on Flux models and many basic Julia structs, it will not work on (say) a tuple of  DataLoader s. source"},{"id":171,"pagetitle":"Built-in Layers","title":"Built-in Layer Types","ref":"/flux/stable/models/layers/#Built-in-Layer-Types","content":" Built-in Layer Types If you started at the beginning of the guide, then you have already met the basic  Dense  layer, and seen  Chain  for combining layers. These core layers form the foundation of almost all neural networks. The  Dense  exemplifies several features: It contains an an  activation function , which is broadcasted over the output. Because this broadcast can be fused with other operations, doing so is more efficient than applying the activation function separately. It take an  init  keyword, which accepts a function acting like  rand . That is,  init(2,3,4)  should create an array of this size. Flux has  many such functions  built-in. All make a CPU array, moved later with  gpu  if desired. The bias vector is always initialised  Flux.zeros32 . The keyword  bias=false  will turn this off, i.e. keeping the bias permanently zero. It is annotated with  @functor , which means that  params  will see the contents, and  gpu  will move their arrays to the GPU. By contrast,  Chain  itself contains no parameters, but connects other layers together. The section on  dataflow layers  introduces others like this."},{"id":172,"pagetitle":"Built-in Layers","title":"Fully Connected","ref":"/flux/stable/models/layers/#Fully-Connected","content":" Fully Connected"},{"id":173,"pagetitle":"Built-in Layers","title":"Flux.Dense","ref":"/flux/stable/models/layers/#Flux.Dense","content":" Flux.Dense  —  Type Dense(in => out, σ=identity; bias=true, init=glorot_uniform)\nDense(W::AbstractMatrix, [bias, σ]) Create a traditional fully connected layer, whose forward pass is given by: y = σ.(W * x .+ bias) The input  x  should be a vector of length  in , or batch of vectors represented as an  in × N  matrix, or any array with  size(x,1) == in . The out  y  will be a vector  of length  out , or a batch with  size(y) == (out, size(x)[2:end]...) Keyword  bias=false  will switch off trainable bias for the layer. The initialisation of the weight matrix is  W = init(out, in) , calling the function given to keyword  init , with default  glorot_uniform . The weight matrix and/or the bias vector (of length  out ) may also be provided explicitly. Examples julia> d = Dense(5 => 2)\nDense(5 => 2)       # 12 parameters\n\njulia> d(rand32(5, 64)) |> size\n(2, 64)\n\njulia> d(rand32(5, 6, 4, 64)) |> size  # treated as three batch dimensions\n(2, 6, 4, 64)\n\njulia> d1 = Dense(ones(2, 5), false, tanh)  # using provided weight matrix\nDense(5 => 2, tanh; bias=false)  # 10 parameters\n\njulia> d1(ones(5))\n2-element Vector{Float64}:\n 0.9999092042625951\n 0.9999092042625951\n\njulia> Flux.params(d1)  # no trainable bias\nParams([[1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]]) source"},{"id":174,"pagetitle":"Built-in Layers","title":"Flux.Bilinear","ref":"/flux/stable/models/layers/#Flux.Bilinear","content":" Flux.Bilinear  —  Type Bilinear((in1, in2) => out, σ=identity; bias=true, init=glorot_uniform)\nBilinear(W::AbstractArray, [bias, σ]) Creates a layer which is fully connected between two inputs and the output, and otherwise similar to  Dense . Its output, given vectors  x  &  y , is another vector  z  with, for all  i ∈ 1:out : z[i] = σ(x' * W[i,:,:] * y + bias[i]) If  x  and  y  are matrices, then each column of the output  z = B(x, y)  is of this form, with  B  the Bilinear layer. If the second input  y  is not given, it is taken to be equal to  x , i.e.  B(x) == B(x, x) The two inputs may also be provided as a tuple,  B((x, y)) == B(x, y) , which is accepted as the input to a  Chain . If the two input sizes are the same,  in1 == in2 , then you may write  Bilinear(in => out, σ) . The initialisation works as for  Dense  layer, with  W = init(out, in1, in2) . By default the bias vector is  zeros(Float32, out) , option  bias=false  will switch off trainable bias. Either of these may be provided explicitly. Examples julia> x, y = randn(Float32, 5, 32), randn(Float32, 5, 32);\n\njulia> B = Flux.Bilinear((5, 5) => 7)\nBilinear(5 => 7)    # 182 parameters\n\njulia> B(x) |> size  # interactions based on one input\n(7, 32)\n\njulia> B(x,y) == B((x,y))  # two inputs, may be given as a tuple\ntrue\n\njulia> sc = SkipConnection(\n                Chain(Dense(5 => 20, tanh), Dense(20 => 9, tanh)),\n                Flux.Bilinear((9, 5) => 3, bias=false),\n            );  # used as the recombinator, with skip as the second input\n\njulia> sc(x) |> size\n(3, 32)\n\njulia> Flux.Bilinear(rand(4,8,16), false, tanh)  # first dim of weight is the output\nBilinear((8, 16) => 4, tanh; bias=false)  # 512 parameters source"},{"id":175,"pagetitle":"Built-in Layers","title":"Flux.Scale","ref":"/flux/stable/models/layers/#Flux.Scale","content":" Flux.Scale  —  Type Scale(size::Integer..., σ=identity; bias=true, init=ones32)\nScale(scale::AbstractArray, [bias, σ]) Create an element-wise layer, whose forward pass is given by: y = σ.(scale .* x .+ bias) This uses  .*  instead of matrix multiplication  *  of  Dense . The learnable scale & bias are initialised  init(size...)  and  zeros32(size...) , with  init=ones32  by default. You may specify the function  init ,  turn off trainable bias with  bias=false , or provide the array(s) explicitly. Used by  LayerNorm  with  affine=true . Examples julia> a = Flux.Scale(2)\nScale(2)            # 4 parameters\n\njulia> Flux.params(a)\nParams([Float32[1.0, 1.0], Float32[0.0, 0.0]])\n\njulia> a([1 2 3])\n2×3 Matrix{Float32}:\n 1.0  2.0  3.0\n 1.0  2.0  3.0\n\njulia> b = Flux.Scale([1 2 3 4], false, abs2)\nScale(1, 4, abs2; bias=false)  # 4 parameters\n\njulia> b([1, 10])\n2×4 Matrix{Int64}:\n   1    4    9    16\n 100  400  900  1600\n\njulia> Flux.params(b)\nParams([[1 2 3 4]]) source Perhaps  Scale  isn't quite fully connected, but it may be thought of as  Dense(Diagonal(s.weights), s.bias) , and LinearAlgebra's  Diagonal  is a matrix which just happens to contain many zeros. Flux ≤ 0.12 Old versions of Flux accepted only  Dense(in, out, act)  and not  Dense(in => out, act) . This notation makes a  Pair  object. If you get an error like  MethodError: no method matching Dense(::Pair{Int64,Int64}) , this means that you should upgrade to newer Flux versions."},{"id":176,"pagetitle":"Built-in Layers","title":"Convolution Models","ref":"/flux/stable/models/layers/#Convolution-Models","content":" Convolution Models These layers are used to build convolutional neural networks (CNNs). They all expect images in what is called WHCN order: a batch of 32 colour images, each 50 x 50 pixels, will have  size(x) == (50, 50, 3, 32) . A single grayscale image might instead have  size(x) == (28, 28, 1, 1) . Besides images, 2D data, they also work with 1D data, where for instance stereo sound recording with 1000 samples might have  size(x) == (1000, 2, 1) . They will also work with 3D data,  ndims(x) == 5 , where again the last two dimensions are channel and batch. To understand how strides and padding work, the article by  Dumoulin & Visin  has great illustrations."},{"id":177,"pagetitle":"Built-in Layers","title":"Flux.Conv","ref":"/flux/stable/models/layers/#Flux.Conv","content":" Flux.Conv  —  Type Conv(filter, in => out, σ = identity;\n     stride = 1, pad = 0, dilation = 1, groups = 1, [bias, init]) Standard convolutional layer.  filter  is a tuple of integers specifying the size of the convolutional kernel;  in  and  out  specify the number of input and output channels. Image data should be stored in WHCN order (width, height, channels, batch). In other words, a 100×100 RGB image would be a  100×100×3×1  array, and a batch of 50 would be a  100×100×3×50  array. This has  N = 2  spatial dimensions, and needs a kernel size like  (5,5) , a 2-tuple of integers. To take convolutions along  N  feature dimensions, this layer expects as input an array with  ndims(x) == N+2 , where  size(x, N+1) == in  is the number of input channels, and  size(x, ndims(x))  is (as always) the number of observations in a batch. Then: filter  should be a tuple of  N  integers. Keywords  stride  and  dilation  should each be either single integer, or a tuple with  N  integers. Keyword  pad  specifies the number of elements added to the borders of the data array. It can be a single integer for equal padding all around, a tuple of  N  integers, to apply the same padding at begin/end of each spatial dimension, a tuple of  2*N  integers, for asymmetric padding, or the singleton  SamePad() , to calculate padding such that  size(output,d) == size(x,d) / stride  (possibly rounded) for each spatial dimension. Keyword  groups  is expected to be an  Int . It specifies the number of groups to divide a convolution into. Keywords to control initialization of the layer: init  - Function used to generate initial weights. Defaults to  glorot_uniform . bias  - The initial bias vector is all zero by default. Trainable bias can be disabled entirely by setting this to  false , or another vector can be provided such as  bias = randn(Float32, out) . See also  ConvTranspose ,  DepthwiseConv ,  CrossCor . Examples julia> xs = rand32(100, 100, 3, 50); # a batch of 50 RGB images\n\njulia> layer = Conv((5,5), 3 => 7, relu; bias = false)\nConv((5, 5), 3 => 7, relu, bias=false)  # 525 parameters\n\njulia> layer(xs) |> size\n(96, 96, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2)(xs) |> size\n(48, 48, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2, pad = SamePad())(xs) |> size\n(50, 50, 7, 50)\n\njulia> Conv((1,1), 3 => 7; pad = (20,10,0,0))(xs) |> size\n(130, 100, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2, dilation = 4)(xs) |> size\n(42, 42, 7, 50) source"},{"id":178,"pagetitle":"Built-in Layers","title":"Flux.Conv","ref":"/flux/stable/models/layers/#Flux.Conv-Tuple{AbstractArray}","content":" Flux.Conv  —  Method Conv(weight::AbstractArray, [bias, activation; stride, pad, dilation]) Constructs a convolutional layer with the given weight and bias. Accepts the same keywords and has the same defaults as  Conv(k::NTuple{N,Integer}, ch::Pair{<:Integer,<:Integer}, σ; ...) . julia> weight = rand(3, 4, 5);\n\njulia> bias = zeros(5);\n\njulia> layer = Conv(weight, bias, sigmoid)  # expects 1 spatial dimension\nConv((3,), 4 => 5, σ)  # 65 parameters\n\njulia> layer(randn(100, 4, 64)) |> size\n(98, 5, 64)\n\njulia> Flux.params(layer) |> length\n2 source"},{"id":179,"pagetitle":"Built-in Layers","title":"Flux.ConvTranspose","ref":"/flux/stable/models/layers/#Flux.ConvTranspose","content":" Flux.ConvTranspose  —  Type ConvTranspose(filter, in => out, σ=identity; stride=1, pad=0, dilation=1, [bias, init]) Standard convolutional transpose layer.  filter  is a tuple of integers specifying the size of the convolutional kernel, while  in  and  out  specify the number of input and output channels. Note that  pad=SamePad()  here tries to ensure  size(output,d) == size(x,d) * stride . Parameters are controlled by additional keywords, with defaults  init=glorot_uniform  and  bias=true . See also  Conv  for more detailed description of keywords. Examples julia> xs = rand32(100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> layer = ConvTranspose((5,5), 3 => 7, relu)\nConvTranspose((5, 5), 3 => 7, relu)  # 532 parameters\n\njulia> layer(xs) |> size\n(104, 104, 7, 50)\n\njulia> ConvTranspose((5,5), 3 => 7, stride=2)(xs) |> size\n(203, 203, 7, 50)\n\njulia> ConvTranspose((5,5), 3 => 7, stride=3, pad=SamePad())(xs) |> size\n(300, 300, 7, 50) source"},{"id":180,"pagetitle":"Built-in Layers","title":"Flux.ConvTranspose","ref":"/flux/stable/models/layers/#Flux.ConvTranspose-Tuple{AbstractArray}","content":" Flux.ConvTranspose  —  Method ConvTranspose(weight::AbstractArray, [bias, activation; stride, pad, dilation, groups]) Constructs a ConvTranspose layer with the given weight and bias. Accepts the same keywords and has the same defaults as  ConvTranspose(k::NTuple{N,Integer}, ch::Pair{<:Integer,<:Integer}, σ; ...) . Examples julia> weight = rand(3, 4, 5);\n\njulia> bias = zeros(4);\n\njulia> layer = ConvTranspose(weight, bias, sigmoid)\nConvTranspose((3,), 5 => 4, σ)  # 64 parameters\n\njulia> layer(randn(100, 5, 64)) |> size  # transposed convolution will increase the dimension size (upsampling)\n(102, 4, 64)\n\njulia> Flux.params(layer) |> length\n2 source"},{"id":181,"pagetitle":"Built-in Layers","title":"Flux.CrossCor","ref":"/flux/stable/models/layers/#Flux.CrossCor","content":" Flux.CrossCor  —  Type CrossCor(filter, in => out, σ=identity; stride=1, pad=0, dilation=1, [bias, init]) Standard cross correlation layer.  filter  is a tuple of integers specifying the size of the convolutional kernel;  in  and  out  specify the number of input and output channels. Parameters are controlled by additional keywords, with defaults  init=glorot_uniform  and  bias=true . See also  Conv  for more detailed description of keywords. Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> layer = CrossCor((5,5), 3 => 6, relu; bias=false)\nCrossCor((5, 5), 3 => 6, relu, bias=false)  # 450 parameters\n\njulia> layer(xs) |> size\n(96, 96, 6, 50)\n\njulia> CrossCor((5,5), 3 => 7, stride=3, pad=(2,0))(xs) |> size\n(34, 32, 7, 50) source"},{"id":182,"pagetitle":"Built-in Layers","title":"Flux.CrossCor","ref":"/flux/stable/models/layers/#Flux.CrossCor-Tuple{AbstractArray}","content":" Flux.CrossCor  —  Method CrossCor(weight::AbstractArray, [bias, activation; stride, pad, dilation]) Constructs a CrossCor layer with the given weight and bias. Accepts the same keywords and has the same defaults as  CrossCor(k::NTuple{N,Integer}, ch::Pair{<:Integer,<:Integer}, σ; ...) . Examples julia> weight = rand(3, 4, 5);\n\njulia> bias = zeros(5);\n\njulia> layer = CrossCor(weight, bias, relu)\nCrossCor((3,), 4 => 5, relu)  # 65 parameters\n\njulia> layer(randn(100, 4, 64)) |> size\n(98, 5, 64) source"},{"id":183,"pagetitle":"Built-in Layers","title":"Flux.DepthwiseConv","ref":"/flux/stable/models/layers/#Flux.DepthwiseConv","content":" Flux.DepthwiseConv  —  Function DepthwiseConv(filter, in => out, σ=identity; stride=1, pad=0, dilation=1, [bias, init])\nDepthwiseConv(weight::AbstractArray, [bias, activation; stride, pad, dilation]) Return a depthwise convolutional layer, that is a  Conv  layer with number of groups equal to the number of input channels. See  Conv  for a description of the arguments. Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> layer = DepthwiseConv((5,5), 3 => 6, relu; bias=false)\nConv((5, 5), 3 => 6, relu, groups=3, bias=false)  # 150 parameters \n\njulia> layer(xs) |> size\n(96, 96, 6, 50)\n\njulia> DepthwiseConv((5, 5), 3 => 9, stride=2, pad=2)(xs) |> size\n(50, 50, 9, 50) source"},{"id":184,"pagetitle":"Built-in Layers","title":"Flux.SamePad","ref":"/flux/stable/models/layers/#Flux.SamePad","content":" Flux.SamePad  —  Type SamePad() Passed as an option to convolutional layers (and friends), this causes the padding to be chosen such that the input and output sizes agree (on the first  N  dimensions, the kernel or window) when  stride==1 . When  stride≠1 , the output size equals  ceil(input_size/stride) . See also  Conv ,  MaxPool . Examples julia> xs = rand32(100, 100, 3, 50);  # a batch of images\n\njulia> layer = Conv((2,2), 3 => 7, pad=SamePad())\nConv((2, 2), 3 => 7, pad=(1, 0, 1, 0))  # 91 parameters\n\njulia> layer(xs) |> size  # notice how the dimensions stay the same with this padding\n(100, 100, 7, 50)\n\njulia> layer2 = Conv((2,2), 3 => 7)\nConv((2, 2), 3 => 7)  # 91 parameters\n\njulia> layer2(xs) |> size  # the output dimension changes as the padding was not \"same\"\n(99, 99, 7, 50)\n\njulia> layer3 = Conv((5, 5), 3 => 7, stride=2, pad=SamePad())\nConv((5, 5), 3 => 7, pad=2, stride=2)  # 532 parameters\n\njulia> layer3(xs) |> size  # output size = `ceil(input_size/stride)` = 50\n(50, 50, 7, 50) source"},{"id":185,"pagetitle":"Built-in Layers","title":"Flux.flatten","ref":"/flux/stable/models/layers/#Flux.flatten","content":" Flux.flatten  —  Function flatten(x) Same as  MLUtils.flatten , which  should be prefered to this method existing  only for backward compatibility. source"},{"id":186,"pagetitle":"Built-in Layers","title":"MultiHeadAttention","ref":"/flux/stable/models/layers/#MultiHeadAttention","content":" MultiHeadAttention The basic blocks needed to implement  Transformer  architectures. See also the functional counterparts documented in NNlib's  Attention  section."},{"id":187,"pagetitle":"Built-in Layers","title":"Flux.MultiHeadAttention","ref":"/flux/stable/models/layers/#Flux.MultiHeadAttention","content":" Flux.MultiHeadAttention  —  Type MultiHeadAttention(dims; [nheads, bias, init, dropout_prob]) The multi-head dot-product attention layer used in Transformer architectures [1]. Returns the transformed input sequnce and the attention scores. [1] Vaswani et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017. Arguments dims : The embedding dimensions of inputs, intermediate tensors and outputs.         In the most general case, it is given as          a)  (q_in_dim, k_in_dim, v_in_dim) => (qk_dim, v_dim) => out_dim .         Can take also simpler forms as         b)  dims::Int ;         c)  in_dim::Int => (qk_dim, v_dim) => out_dim ;         d)  in_dim::Int => qkv_dim => out_dim . nheads : number of heads. Default  8 . init : weight initializer for the Dense layers. Default  glorot_uniform . bias  : whether pointwise QKVO dense transforms use bias. Default  false . dropout_prob : dropout probability for the attention scores. Default  0.0 . Forward (mha::MultiHeadAttention)(q_in, k_in, v_in, [bias]; [mask]) The arguments of the forward pass are: q_in : Input query array of size  (q_in_dim, q_len, batch_size) . k_in : Input key array of size  (k_in_dim, kv_len, batch_size) . v_in : Input value array of size  (v_in_dim, kv_len, batch_size) . bias : Bias array broadcastable to size  (kv_len, q_len, nheads, batch_size) .          It will be added to the attention scores before the softmax.         Default  nothing . mask : Input array broadcastable to size           (kv_len, q_len, nheads, batch_size) .          The mask is applied to the attention scores just before the softmax.          See  NNlib.make_causal_mask  for creating causal masks.          Default  nothing . Alternative calling signatures are  mha(q_in) , equivalent to  mha(q_in, q_in, q_in)  (self-attention), and  mha(q_in, k_in) , equivalent to  mha(q_in, k_in, k_in)  (key and value are the same). See also  NNlib.dot_product_attention . Examples mha = MultiHeadAttention(64, nheads = 8)\nq = rand(Float32, (64, 10, 32))\nk = rand(Float32, (64, 20, 32))\nv = rand(Float32, (64, 20, 32))\ny, α = mha(q, k, v) \n# [y] = [64, 10, 32]\n# [α] = [20, 10, 8, 32]\n\nmha = MultiHeadAttention(64 => 1024 => 1024, nheads = 8)\ny, α = mha(q) # self-attention\n# [y] = [1024, 10, 32]\n# [α] = [10, 10, 8, 32] source"},{"id":188,"pagetitle":"Built-in Layers","title":"Pooling","ref":"/flux/stable/models/layers/#Pooling","content":" Pooling These layers are commonly used after a convolution layer, and reduce the size of its output. They have no trainable parameters."},{"id":189,"pagetitle":"Built-in Layers","title":"Flux.AdaptiveMaxPool","ref":"/flux/stable/models/layers/#Flux.AdaptiveMaxPool","content":" Flux.AdaptiveMaxPool  —  Type AdaptiveMaxPool(out::NTuple) Adaptive max pooling layer. Calculates the necessary window size such that its output has  size(y)[1:N] == out . Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(out) . See also  MaxPool ,  AdaptiveMeanPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> AdaptiveMaxPool((25, 25))(xs) |> size\n(25, 25, 3, 50)\n\njulia> MaxPool((4,4))(xs) ≈ AdaptiveMaxPool((25, 25))(xs)\ntrue source"},{"id":190,"pagetitle":"Built-in Layers","title":"Flux.MaxPool","ref":"/flux/stable/models/layers/#Flux.MaxPool","content":" Flux.MaxPool  —  Type MaxPool(window::NTuple; pad=0, stride=window) Max pooling layer, which replaces all pixels in a block of size  window  with one. Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(window) . By default the window size is also the stride in each dimension. The keyword  pad  accepts the same options as for the  Conv  layer, including  SamePad() . See also  Conv ,  MeanPool ,  AdaptiveMaxPool ,  GlobalMaxPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> m = Chain(Conv((5, 5), 3 => 7, pad=SamePad()), MaxPool((5, 5), pad=SamePad()))\nChain(\n  Conv((5, 5), 3 => 7, pad=2),          # 532 parameters\n  MaxPool((5, 5), pad=2),\n)\n\njulia> m[1](xs) |> size\n(100, 100, 7, 50)\n\njulia> m(xs) |> size\n(20, 20, 7, 50)\n\njulia> layer = MaxPool((5,), pad=2, stride=(3,))  # one-dimensional window\nMaxPool((5,), pad=2, stride=3)\n\njulia> layer(rand(Float32, 100, 7, 50)) |> size\n(34, 7, 50) source"},{"id":191,"pagetitle":"Built-in Layers","title":"Flux.GlobalMaxPool","ref":"/flux/stable/models/layers/#Flux.GlobalMaxPool","content":" Flux.GlobalMaxPool  —  Type GlobalMaxPool() Global max pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing max pooling on the complete (w,h)-shaped feature maps. See also  MaxPool ,  GlobalMeanPool . julia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((3,3), 3 => 7), GlobalMaxPool());\n\njulia> m(xs) |> size\n(1, 1, 7, 50)\n\njulia> GlobalMaxPool()(rand(3,5,7)) |> size  # preserves 2 dimensions\n(1, 5, 7) source"},{"id":192,"pagetitle":"Built-in Layers","title":"Flux.AdaptiveMeanPool","ref":"/flux/stable/models/layers/#Flux.AdaptiveMeanPool","content":" Flux.AdaptiveMeanPool  —  Type AdaptiveMeanPool(out::NTuple) Adaptive mean pooling layer. Calculates the necessary window size such that its output has  size(y)[1:N] == out . Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(out) . See also  MaxPool ,  AdaptiveMaxPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> AdaptiveMeanPool((25, 25))(xs) |> size\n(25, 25, 3, 50)\n\njulia> MeanPool((4,4))(xs) ≈ AdaptiveMeanPool((25, 25))(xs)\ntrue source"},{"id":193,"pagetitle":"Built-in Layers","title":"Flux.MeanPool","ref":"/flux/stable/models/layers/#Flux.MeanPool","content":" Flux.MeanPool  —  Type MeanPool(window::NTuple; pad=0, stride=window) Mean pooling layer, averaging all pixels in a block of size  window . Expects as input an array with  ndims(x) == N+2 , i.e. channel and batch dimensions, after the  N  feature dimensions, where  N = length(window) . By default the window size is also the stride in each dimension. The keyword  pad  accepts the same options as for the  Conv  layer, including  SamePad() . See also  Conv ,  MaxPool ,  AdaptiveMeanPool . Examples julia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((5,5), 3 => 7), MeanPool((5,5), pad=SamePad()))\nChain(\n  Conv((5, 5), 3 => 7),                 # 532 parameters\n  MeanPool((5, 5), pad=2),\n)\n\njulia> m[1](xs) |> size\n(96, 96, 7, 50)\n\njulia> m(xs) |> size\n(20, 20, 7, 50) source"},{"id":194,"pagetitle":"Built-in Layers","title":"Flux.GlobalMeanPool","ref":"/flux/stable/models/layers/#Flux.GlobalMeanPool","content":" Flux.GlobalMeanPool  —  Type GlobalMeanPool() Global mean pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing mean pooling on the complete (w,h)-shaped feature maps. julia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((3,3), 3 => 7), GlobalMeanPool());\n\njulia> m(xs) |> size\n(1, 1, 7, 50) source"},{"id":195,"pagetitle":"Built-in Layers","title":"Upsampling","ref":"/flux/stable/models/layers/#Upsampling","content":" Upsampling The opposite of pooling, these layers increase the size of an array. They have no trainable parameters. "},{"id":196,"pagetitle":"Built-in Layers","title":"Flux.Upsample","ref":"/flux/stable/models/layers/#Flux.Upsample","content":" Flux.Upsample  —  Type Upsample(mode = :nearest; [scale, size]) \nUpsample(scale, mode = :nearest) An upsampling layer. One of two keywords must be given: If  scale  is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually. Alternatively, keyword   size  accepts a tuple, to directly specify the leading dimensions of the output. Currently supported upsampling  mode s  and corresponding NNlib's methods are: :nearest  ->  NNlib.upsample_nearest :bilinear  ->  NNlib.upsample_bilinear :trilinear  ->  NNlib.upsample_trilinear Examples julia> m = Upsample(scale = (2, 3))\nUpsample(:nearest, scale = (2, 3))\n\njulia> m(ones(2, 2, 1, 1)) |> size\n(4, 6, 1, 1)\n\njulia> m = Upsample(:bilinear, size = (4, 5))\nUpsample(:bilinear, size = (4, 5))\n\njulia> m(ones(2, 2, 1, 1)) |> size\n(4, 5, 1, 1) source"},{"id":197,"pagetitle":"Built-in Layers","title":"Flux.PixelShuffle","ref":"/flux/stable/models/layers/#Flux.PixelShuffle","content":" Flux.PixelShuffle  —  Type PixelShuffle(r::Int) Pixel shuffling layer with upscale factor  r . Usually used for generating higher resolution images while upscaling them. See  NNlib.pixel_shuffle . Examples julia> p = PixelShuffle(2);\n\njulia> xs = [2row + col + channel/10 for row in 1:2, col in 1:2, channel in 1:4, n in 1:1]\n2×2×4×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 3.1  4.1\n 5.1  6.1\n\n[:, :, 2, 1] =\n 3.2  4.2\n 5.2  6.2\n\n[:, :, 3, 1] =\n 3.3  4.3\n 5.3  6.3\n\n[:, :, 4, 1] =\n 3.4  4.4\n 5.4  6.4\n\njulia> p(xs)\n4×4×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 3.1  3.3  4.1  4.3\n 3.2  3.4  4.2  4.4\n 5.1  5.3  6.1  6.3\n 5.2  5.4  6.2  6.4\n\njulia> xs = [3row + col + channel/10 for row in 1:2, col in 1:3, channel in 1:4, n in 1:1]\n2×3×4×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 4.1  5.1  6.1\n 7.1  8.1  9.1\n\n[:, :, 2, 1] =\n 4.2  5.2  6.2\n 7.2  8.2  9.2\n\n[:, :, 3, 1] =\n 4.3  5.3  6.3\n 7.3  8.3  9.3\n\n[:, :, 4, 1] =\n 4.4  5.4  6.4\n 7.4  8.4  9.4\n\njulia> p(xs)\n4×6×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 4.1  4.3  5.1  5.3  6.1  6.3\n 4.2  4.4  5.2  5.4  6.2  6.4\n 7.1  7.3  8.1  8.3  9.1  9.3\n 7.2  7.4  8.2  8.4  9.2  9.4 source"},{"id":198,"pagetitle":"Built-in Layers","title":"Embedding Vectors","ref":"/flux/stable/models/layers/#Embedding-Vectors","content":" Embedding Vectors These layers accept an index, and return a vector (or several indices, and several vectors). The possible embedding vectors are learned parameters."},{"id":199,"pagetitle":"Built-in Layers","title":"Flux.Embedding","ref":"/flux/stable/models/layers/#Flux.Embedding","content":" Flux.Embedding  —  Type Embedding(in => out; init=randn32) A lookup table that stores embeddings of dimension  out   for a vocabulary of size  in , as a trainable matrix. This layer is often used to store word embeddings and retrieve them using indices.  The input to the layer can be a vocabulary index in  1:in , an array of indices, or the corresponding  onehot encoding . For indices  x , the result is of size  (out, size(x)...) , allowing several batch dimensions. For one-hot  ohx , the result is of size  (out, size(ohx)[2:end]...) . Examples julia> emb = Embedding(26 => 4, init=Flux.identity_init(gain=22))\nEmbedding(26 => 4)  # 104 parameters\n\njulia> emb(2)  # one column of e.weight (here not random!)\n4-element Vector{Float32}:\n  0.0\n 22.0\n  0.0\n  0.0\n\njulia> emb([3, 1, 20, 14, 4, 15, 7])  # vocabulary indices, in 1:26\n4×7 Matrix{Float32}:\n  0.0  22.0  0.0  0.0   0.0  0.0  0.0\n  0.0   0.0  0.0  0.0   0.0  0.0  0.0\n 22.0   0.0  0.0  0.0   0.0  0.0  0.0\n  0.0   0.0  0.0  0.0  22.0  0.0  0.0\n\njulia> ans == emb(Flux.onehotbatch(\"cat&dog\", 'a':'z', 'n'))\ntrue\n\njulia> emb(rand(1:26, (10, 1, 12))) |> size  # three batch dimensions\n(4, 10, 1, 12) source"},{"id":200,"pagetitle":"Built-in Layers","title":"Flux.EmbeddingBag","ref":"/flux/stable/models/layers/#Flux.EmbeddingBag","content":" Flux.EmbeddingBag  —  Type EmbeddingBag(in => out, reduction=mean; init=Flux.randn32) A lookup table that stores embeddings of dimension  out  for a vocabulary of size  in . Differs from  Embedding  in that, instead of acting on a single vocabulary index, it always acts a vector of indices which it calls a \"bag\". Their individual embedding vectors are reduced to one, using  mean  or some other function. Instead of acting on one \"bag\", such as  x::Vector{Int} , the layer can also act on several: Acting on a vector of \"bags\", it produces a matrix whose columns are the reduced vectors. More generally on  x::Array{Vector{Int}} , its output is of size  (out, size(x)...) . Any higher-rank array of integers is interpreted as a collection of \"bags\" each along the first dimension. Thus the output is  mapslices(e, x; dims=1)  when  e::EmbeddingBag  and  x::Array{Int,N} . This method is more efficient, but requires that all \"bags\" have the same length. A vector of \"bags\" may also be produced by splitting a vector of indices at specified points. For this case the layer takes two inputs, both vectors of integers. See details below. The \"bag\" may equivalently be represented as a  OneHotMatrix . A collection of these, or one higher-rank  OneHotArray , again produce a stack of embeddings. See details below. Examples julia> vocab_size = 26;  # embed into 3 dimensions, with non-random vectors:\n\njulia> eb = EmbeddingBag(vocab_size => 3, init=Flux.identity_init(gain=100))\nEmbeddingBag(26 => 3)  # 78 parameters\n\njulia> eb([2])  # one bag of 1 item\n3-element Vector{Float32}:\n   0.0\n 100.0\n   0.0\n\njulia> eb([3,3,1])  # one bag of 3 items, one mean embedding\n3-element Vector{Float32}:\n 33.333332\n  0.0\n 66.666664\n\njulia> eb([[3,1,3], [2,1]])  # two bags\n3×2 Matrix{Float32}:\n 33.3333  50.0\n  0.0     50.0\n 66.6667   0.0\n\njulia> eb([1 1 1 1; 1 2 3 4])  # 4 bags each of 2 items, eachcol([1 1 1 1; 1 2 3 4])\n3×4 Matrix{Float32}:\n 100.0  50.0  50.0  50.0\n   0.0  50.0   0.0   0.0\n   0.0   0.0  50.0   0.0\n\njulia> eb(rand(1:26, 10, 5, 5)) |> size  # 25 bags each of 10 items\n(3, 5, 5) Another way to specify \"many bags of many items\" is to provide a vector  data  (each in  1:in ) and a vector  at  stating where to split that up into \"bags\". The first bag starts with  data[at[1]] , the second at  data[at[2]] , and so on,  with no overlaps and nothing left out (thus it requires  at[1]==1 ). julia> data = [11, 1, 12, 2, 13, 3, 14];\n\njulia> Flux._splitat(data, [1, 4]) |> println  # internal function, makes data[1:3], data[4:end]\n[[11, 1, 12], [2, 13, 3, 14]]\n\njulia> eb(data, [1, 4])  # two bags, of 3 and 4 items\n3×2 Matrix{Float32}:\n 33.3333   0.0\n  0.0     25.0\n  0.0     25.0 Finally, each bag may also be also be represented as a  OneHotMatrix . julia> eb(Flux.onehotbatch(\"bba\", 'a':'z'))  # same as [2,2,1], one bag of 3 items\n3-element Vector{Float32}:\n 33.333332\n 66.666664\n  0.0\n\njulia> eb([Flux.onehotbatch(\"bba\", 'a':'z'), Flux.onehotbatch(\"cc\", 'a':'z')])  # two bags\n3×2 Matrix{Float32}:\n 33.3333    0.0\n 66.6667    0.0\n  0.0     100.0 source"},{"id":201,"pagetitle":"Built-in Layers","title":"Dataflow Layers, or Containers","ref":"/flux/stable/models/layers/#man-dataflow-layers","content":" Dataflow Layers, or Containers The basic  Chain(F, G, H)  applies the layers it contains in sequence, equivalent to  H ∘ G ∘ F . Flux has some other layers which contain layers, but connect them up in a more complicated way:  SkipConnection  allows ResNet's residual connection."},{"id":202,"pagetitle":"Built-in Layers","title":"Flux.Chain","ref":"/flux/stable/models/layers/#Flux.Chain","content":" Flux.Chain  —  Type Chain(layers...)\nChain(name = layer, ...) Collects multiple layers / functions to be called in sequence on a given input. Supports indexing and slicing,  m[2]  or  m[1:end-1] , and if names are given,  m[:name] == m[1]  etc. Examples julia> m = Chain(x -> x^2, x -> x+1);\n\njulia> m(5) == 26\ntrue\n\njulia> m = Chain(Dense(10 => 5, tanh), Dense(5 => 2));\n\njulia> x = rand32(10, 32);\n\njulia> m(x) == m[2](m[1](x))\ntrue\n\njulia> m2 = Chain(enc = Chain(Flux.flatten, Dense(10 => 5, tanh)), \n                  dec = Dense(5 => 2));\n\njulia> m2(x) == (m2[:dec] ∘ m2[:enc])(x)\ntrue For large models, there is a special type-unstable path which can reduce compilation times. This can be used by supplying a vector of layers  Chain([layer1, layer2, ...]) . This feature is somewhat experimental, beware! source"},{"id":203,"pagetitle":"Built-in Layers","title":"Flux.activations","ref":"/flux/stable/models/layers/#Flux.activations","content":" Flux.activations  —  Function activations(c::Chain, input) Like calling a  Chain , but saves the result of each layer as an output. Examples julia> using Flux: activations\n\njulia> c = Chain(x -> x + 1, x -> x * 2, x -> x ^ 3);\n\njulia> activations(c, 1)\n(2, 4, 64) source"},{"id":204,"pagetitle":"Built-in Layers","title":"Flux.Maxout","ref":"/flux/stable/models/layers/#Flux.Maxout","content":" Flux.Maxout  —  Type Maxout(layers...)\nMaxout(f, n_alts) This contains a number of internal layers, each of which receives the same input. Its output is the elementwise maximum of the the internal layers' outputs. Instead of defining layers individually, you can provide a zero-argument function which constructs them, and the number to construct. Maxout over linear dense layers satisfies the univeral approximation theorem. See Goodfellow, Warde-Farley, Mirza, Courville & Bengio \"Maxout Networks\"   https://arxiv.org/abs/1302.4389 . See also  Parallel  to reduce with other operators. Examples julia> m = Maxout(x -> abs2.(x), x -> x .* 3);\n\njulia> m([-2 -1 0 1 2])\n1×5 Matrix{Int64}:\n 4  1  0  3  6\n\njulia> m3 = Maxout(() -> Dense(5 => 7, tanh), 3)\nMaxout(\n  Dense(5 => 7, tanh),                  # 42 parameters\n  Dense(5 => 7, tanh),                  # 42 parameters\n  Dense(5 => 7, tanh),                  # 42 parameters\n)                   # Total: 6 arrays, 126 parameters, 888 bytes.\n\njulia> Flux.outputsize(m3, (5, 11))\n(7, 11) source"},{"id":205,"pagetitle":"Built-in Layers","title":"Flux.SkipConnection","ref":"/flux/stable/models/layers/#Flux.SkipConnection","content":" Flux.SkipConnection  —  Type SkipConnection(layer, connection) Create a skip connection which consists of a layer or  Chain  of consecutive layers and a shortcut connection linking the block's input to the output through a user-supplied 2-argument callable. The first argument to the callable will be propagated through the given  layer  while the second is the unchanged, \"skipped\" input. The simplest \"ResNet\"-type connection is just  SkipConnection(layer, +) . Here is a more complicated example: julia> m = Conv((3,3), 4 => 7, pad=(1,1));\n\njulia> x = ones(Float32, 5, 5, 4, 10);\n\njulia> size(m(x)) == (5, 5, 7, 10)\ntrue\n\njulia> sm = SkipConnection(m, (mx, x) -> cat(mx, x, dims=3));\n\njulia> size(sm(x)) == (5, 5, 11, 10)\ntrue See also  Parallel ,  Maxout . source"},{"id":206,"pagetitle":"Built-in Layers","title":"Flux.Parallel","ref":"/flux/stable/models/layers/#Flux.Parallel","content":" Flux.Parallel  —  Type Parallel(connection, layers...)\nParallel(connection; name = layer, ...) Create a layer which passes an input array to each path in  layers , before reducing the output with  connection . Called with one input  x , this is equivalent to  connection([l(x) for l in layers]...) . If called with multiple inputs, one is passed to each layer, thus  Parallel(+, f, g)(x, y) = f(x) + g(y) . Like  Chain , its sub-layers may be given names using the keyword constructor. These can be accessed by indexing:  m[1] == m[:name]  is the first layer. See also  SkipConnection  which is  Parallel  with one  identity , and  Maxout  which reduces by broadcasting  max . Examples julia> model = Chain(Dense(3 => 5),\n                     Parallel(vcat, Dense(5 => 4), Chain(Dense(5 => 7), Dense(7 => 4))),\n                     Dense(8 => 17));\n\njulia> model(rand32(3)) |> size\n(17,)\n\njulia> model2 = Parallel(+; α = Dense(10, 2, tanh), β = Dense(5, 2))\nParallel(\n  +,\n  α = Dense(10 => 2, tanh),             # 22 parameters\n  β = Dense(5 => 2),                    # 12 parameters\n)                   # Total: 4 arrays, 34 parameters, 392 bytes.\n\njulia> model2(rand32(10), rand32(5)) |> size\n(2,)\n\njulia> model2[:α](rand32(10)) |> size\n(2,)\n\njulia> model2[:β] == model2[2]\ntrue source"},{"id":207,"pagetitle":"Built-in Layers","title":"Flux.PairwiseFusion","ref":"/flux/stable/models/layers/#Flux.PairwiseFusion","content":" Flux.PairwiseFusion  —  Type PairwiseFusion(connection, layers...) Arguments connection : A function taking 2 inputs and combining them into a single output  layers : The layers whose outputs are combined Inputs This layer behaves differently based on input type: If input  x  is a tuple of length N (or the input is  xs  with N  x 's), matching the number of  layers ,  then each layer receives a new input  x[i]  combined with the previous output  y[i-1]  using  connection .   Thus  (y1, y2, y3) = PairwiseFusion(connection, layer1, layer2, layer3)((x1, x2, x3))    may be drawn as: x1 → layer1 → y1 ↘\n                  connection → layer2 → y2 ↘\n              x2 ↗                          connection → layer3 → y3\n                                        x3 ↗ ... or written as: y1 = layer1(x1)\ny2 = layer2(connection(y1, x2))\ny3 = layer3(connection(y2, x3)) With just one input, each layer receives the same  x  combined with the previous output. Thus  y = PairwiseFusion(connection, layers...)(x)  obeys: y[1] == layers[1](x)\nfor i in 2:length(layers)\n    y[i] == connection(layers[i](y[i-1]), x)\nend Returns A tuple of length N with the output of each fusion (( y1 ,  y2 , ...,  yN ) in the example above). source"},{"id":208,"pagetitle":"Built-in Layers","title":"Recurrent Models","ref":"/flux/stable/models/layers/#Recurrent-Models","content":" Recurrent Models Much like the core layers above, but can be used to process sequence data (as well as other kinds of structured data)."},{"id":209,"pagetitle":"Built-in Layers","title":"Flux.RNN","ref":"/flux/stable/models/layers/#Flux.RNN","content":" Flux.RNN  —  Function RNN(in => out, σ = tanh) The most basic recurrent layer; essentially acts as a  Dense  layer, but with the output fed back into the input each time step. The arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(RNNCell(a...)) , and so RNNs are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. Examples julia> r = RNN(3 => 5)\nRecur(\n  RNNCell(3 => 5, tanh),                # 50 parameters\n)         # Total: 4 trainable arrays, 50 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 432 bytes.\n\njulia> r(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(r);\n\njulia> r(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10) Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the following example: julia> r = RNN(3 => 5)\nRecur(\n  RNNCell(3 => 5, tanh),                # 50 parameters\n)         # Total: 4 trainable arrays, 50 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 432 bytes.\n\njulia> r.state |> size\n(5, 1)\n\njulia> r(rand(Float32, 3)) |> size\n(5,)\n\njulia> r.state |> size\n(5, 1)\n\njulia> r(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n\njulia> r.state |> size # state shape has changed\n(5, 10)\n\njulia> r(rand(Float32, 3)) |> size # erroneously outputs a length 5*10 = 50 vector.\n(50,) Note: RNNCell s can be constructed directly by specifying the non-linear function, the  Wi  and  Wh  internal matrices, a bias vector  b , and a learnable initial state  state0 . The   Wi  and  Wh  matrices do not need to be the same type, but if  Wh  is  dxd , then  Wi  should be of shape  dxN . julia> using LinearAlgebra\n\njulia> r = Flux.Recur(Flux.RNNCell(tanh, rand(5, 4), Tridiagonal(rand(5, 5)), rand(5), rand(5, 1)))\n\njulia> r(rand(4, 10)) |> size # batch size of 10\n(5, 10) source"},{"id":210,"pagetitle":"Built-in Layers","title":"Flux.LSTM","ref":"/flux/stable/models/layers/#Flux.LSTM","content":" Flux.LSTM  —  Function LSTM(in => out) Long Short Term Memory  recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. The arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(LSTMCell(a...)) , and so LSTMs are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. See  this article  for a good overview of the internals. Examples julia> l = LSTM(3 => 5)\nRecur(\n  LSTMCell(3 => 5),                     # 190 parameters\n)         # Total: 5 trainable arrays, 190 parameters,\n          # plus 2 non-trainable, 10 parameters, summarysize 1.062 KiB.\n\njulia> l(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(l);\n\njulia> l(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10) Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the example in  RNN . Note: LSTMCell s can be constructed directly by specifying the non-linear function, the  Wi  and  Wh  internal matrices, a bias vector  b , and a learnable initial state  state0 . The   Wi  and  Wh  matrices do not need to be the same type. See the example in  RNN . source"},{"id":211,"pagetitle":"Built-in Layers","title":"Flux.GRU","ref":"/flux/stable/models/layers/#Flux.GRU","content":" Flux.GRU  —  Function GRU(in => out) Gated Recurrent Unit  layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. This implements the variant proposed in v1 of the referenced paper. The integer arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(GRUCell(a...)) , and so GRUs are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. See  this article  for a good overview of the internals. Examples julia> g = GRU(3 => 5)\nRecur(\n  GRUCell(3 => 5),                      # 140 parameters\n)         # Total: 4 trainable arrays, 140 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 792 bytes.\n\njulia> g(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(g);\n\njulia> g(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10) Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the example in  RNN . Note: GRUCell s can be constructed directly by specifying the non-linear function, the  Wi  and  Wh  internal matrices, a bias vector  b , and a learnable initial state  state0 . The   Wi  and  Wh  matrices do not need to be the same type. See the example in  RNN . source"},{"id":212,"pagetitle":"Built-in Layers","title":"Flux.GRUv3","ref":"/flux/stable/models/layers/#Flux.GRUv3","content":" Flux.GRUv3  —  Function GRUv3(in => out) Gated Recurrent Unit  layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. This implements the variant proposed in v3 of the referenced paper. The arguments  in  and  out  describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B . This constructor is syntactic sugar for  Recur(GRUv3Cell(a...)) , and so GRUv3s are stateful. Note that the state shape can change depending on the inputs, and so it is good to  reset!  the model between inference calls if the batch size changes. See the examples below. See  this article  for a good overview of the internals. Examples julia> g = GRUv3(3 => 5)\nRecur(\n  GRUv3Cell(3 => 5),                    # 140 parameters\n)         # Total: 5 trainable arrays, 140 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 848 bytes.\n\njulia> g(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(g);\n\njulia> g(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10) Batch size changes Failing to call  reset!  when the input batch size changes can lead to unexpected behavior. See the example in  RNN . Note: GRUv3Cell s can be constructed directly by specifying the non-linear function, the  Wi ,  Wh , and  Wh_h  internal matrices, a bias vector  b , and a learnable initial state  state0 . The   Wi ,  Wh , and  Wh_h  matrices do not need to be the same type. See the example in  RNN . source"},{"id":213,"pagetitle":"Built-in Layers","title":"Flux.Recur","ref":"/flux/stable/models/layers/#Flux.Recur","content":" Flux.Recur  —  Type Recur(cell) Recur  takes a recurrent cell and makes it stateful, managing the hidden state in the background.  cell  should be a model of the form: h, y = cell(h, x...) For example, here's a recurrent network that keeps a running total of its inputs: Examples julia> accum(h, x) = (h + x, x)\naccum (generic function with 1 method)\n\njulia> rnn = Flux.Recur(accum, 0)\nRecur(accum)\n\njulia> rnn(2) \n2\n\njulia> rnn(3)\n3\n\njulia> rnn.state\n5 Folding over a 3d Array of dimensions  (features, batch, time)  is also supported: julia> accum(h, x) = (h .+ x, x)\naccum (generic function with 1 method)\n\njulia> rnn = Flux.Recur(accum, zeros(Int, 1, 1))\nRecur(accum)\n\njulia> rnn([2])\n1-element Vector{Int64}:\n 2\n\njulia> rnn([3])\n1-element Vector{Int64}:\n 3\n\njulia> rnn.state\n1×1 Matrix{Int64}:\n 5\n\njulia> out = rnn(reshape(1:10, 1, 1, :));  # apply to a sequence of (features, batch, time)\n\njulia> out |> size\n(1, 1, 10)\n\njulia> vec(out)\n10-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n\njulia> rnn.state\n1×1 Matrix{Int64}:\n 60 source"},{"id":214,"pagetitle":"Built-in Layers","title":"Flux.reset!","ref":"/flux/stable/models/layers/#Flux.reset!","content":" Flux.reset!  —  Function reset!(rnn) Reset the hidden state of a recurrent layer back to its original value. Assuming you have a  Recur  layer  rnn , this is roughly equivalent to: rnn.state = hidden(rnn.cell) Examples julia> r = Flux.RNNCell(relu, ones(1,1), zeros(1,1), ones(1,1), zeros(1,1));  # users should use the RNN wrapper struct instead\n\njulia> y = Flux.Recur(r, ones(1,1));\n\njulia> y.state\n1×1 Matrix{Float64}:\n 1.0\n\njulia> y(ones(1,1))  # relu(1*1 + 1)\n1×1 Matrix{Float64}:\n 2.0\n\njulia> y.state\n1×1 Matrix{Float64}:\n 2.0\n\njulia> Flux.reset!(y)\n1×1 Matrix{Float64}:\n 0.0\n\njulia> y.state\n1×1 Matrix{Float64}:\n 0.0 source"},{"id":215,"pagetitle":"Built-in Layers","title":"Normalisation & Regularisation","ref":"/flux/stable/models/layers/#Normalisation-and-Regularisation","content":" Normalisation & Regularisation These layers don't affect the structure of the network but may improve training times or reduce overfitting. Some of them contain trainable parameters, while others do not."},{"id":216,"pagetitle":"Built-in Layers","title":"Flux.BatchNorm","ref":"/flux/stable/models/layers/#Flux.BatchNorm","content":" Flux.BatchNorm  —  Type BatchNorm(channels::Integer, λ=identity;\n          initβ=zeros32, initγ=ones32,\n          affine=true, track_stats=true, active=nothing,\n          eps=1f-5, momentum= 0.1f0) Batch Normalization  layer.  channels  should be the size of the channel dimension in your data (see below). Given an array with  N  dimensions, call the  N-1 th the channel dimension. For a batch of feature vectors this is just the data dimension, for  WHCN  images it's the usual channel dimension. BatchNorm  computes the mean and variance for each  D_1×...×D_{N-2}×1×D_N  input slice and normalises the input accordingly. If  affine=true , it also applies  a shift and a rescale to the input through to learnable per-channel bias β and scale γ parameters. After normalisation, elementwise activation  λ  is applied. If  track_stats=true , accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase. Use  testmode!  during inference. Examples julia> using Statistics\n\njulia> xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels\n\njulia> m = BatchNorm(3);\n\njulia> Flux.trainmode!(m);\n\njulia> isapprox(std(m(xs)), 1, atol=0.1) && std(xs) != std(m(xs))\ntrue source"},{"id":217,"pagetitle":"Built-in Layers","title":"Flux.Dropout","ref":"/flux/stable/models/layers/#Flux.Dropout","content":" Flux.Dropout  —  Type Dropout(p; [dims, rng, active]) Layer implementing  dropout  with the given probability. This is used as a regularisation, i.e. to reduce overfitting. While training, it sets each input to  0  (with probability  p ) or else scales it by  1 / (1 - p) , using the  NNlib.dropout  function. While testing, it has no effect. By default the mode will switch automatically, but it can also be controlled manually via  Flux.testmode! , or by passing keyword  active=true  for training mode. By default every input is treated independently. With the  dims  keyword, instead it takes a random choice only along that dimension. For example  Dropout(p; dims = 3)  will randomly zero out entire channels on WHCN input (also called 2D dropout). Keyword  rng  lets you specify a custom random number generator. (Only supported on the CPU.) Examples julia> m = Chain(Dense(ones(3,2)), Dropout(0.4))\nChain(\n  Dense(2 => 3),                        # 9 parameters\n  Dropout(0.4),\n)\n\njulia> m(ones(2, 7))  # test mode, no effect\n3×7 Matrix{Float64}:\n 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n 2.0  2.0  2.0  2.0  2.0  2.0  2.0\n\njulia> Flux.trainmode!(m)  # equivalent to use within gradient\nChain(\n  Dense(2 => 3),                        # 9 parameters\n  Dropout(0.4, active=true),\n)\n\njulia> m(ones(2, 7))\n3×7 Matrix{Float64}:\n 0.0      0.0      3.33333  0.0      0.0      0.0  0.0\n 3.33333  0.0      3.33333  0.0      3.33333  0.0  3.33333\n 3.33333  3.33333  0.0      3.33333  0.0      0.0  3.33333\n\njulia> y = m(ones(2, 10_000));\n\njulia> using Statistics\n\njulia> mean(y)  # is about 2.0, same as in test mode\n1.9989999999999961\n\njulia> mean(iszero, y)  # is about 0.4\n0.4003 source"},{"id":218,"pagetitle":"Built-in Layers","title":"Flux.AlphaDropout","ref":"/flux/stable/models/layers/#Flux.AlphaDropout","content":" Flux.AlphaDropout  —  Type AlphaDropout(p; [rng, active]) A dropout layer. Used in  Self-Normalizing Neural Networks . The AlphaDropout layer ensures that mean and variance of activations remain the same as before. Does nothing to the input once  testmode!  is true. Examples julia> using Statistics\n\njulia> x = randn32(1000,1);\n\njulia> m = Chain(Dense(1000 => 1000, selu), AlphaDropout(0.2));\n\njulia> Flux.trainmode!(m);\n\njulia> y = m(x);\n\njulia> isapprox(std(x), std(y), atol=0.2)\ntrue source"},{"id":219,"pagetitle":"Built-in Layers","title":"Flux.LayerNorm","ref":"/flux/stable/models/layers/#Flux.LayerNorm","content":" Flux.LayerNorm  —  Type LayerNorm(size..., λ=identity; affine=true, eps=1f-5) A  normalisation layer  designed to be used with recurrent hidden states. The argument  size  should be an integer or a tuple of integers. In the forward pass, the layer normalises the mean and standard deviation of the input, then applies the elementwise activation  λ . The input is normalised along the first  length(size)  dimensions for tuple  size , and along the first dimension for integer  size . The input is expected to have first dimensions' size equal to  size . If  affine=true , it also applies a learnable shift and rescaling using the  Scale  layer. See also  BatchNorm ,  InstanceNorm ,  GroupNorm , and  normalise . Examples julia> using Statistics\n\njulia> xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels\n\njulia> m = LayerNorm(3);\n\njulia> y = m(xs);\n\njulia> isapprox(std(y, dims=1:3), ones(1, 1, 1, 2), atol=0.1) && std(y, dims=1:3) != std(xs, dims=1:3)\ntrue source"},{"id":220,"pagetitle":"Built-in Layers","title":"Flux.InstanceNorm","ref":"/flux/stable/models/layers/#Flux.InstanceNorm","content":" Flux.InstanceNorm  —  Type InstanceNorm(channels::Integer, λ=identity;\n             initβ=zeros32, initγ=ones32,\n             affine=false, track_stats=false,\n             eps=1f-5, momentum=0.1f0) Instance Normalization  layer.  channels  should be the size of the channel dimension in your data (see below). Given an array with  N > 2  dimensions, call the  N-1 th the channel dimension. For  WHCN  images it's the usual channel dimension. InstanceNorm  computes the mean and variance for each  D_1×...×D_{N-2}×1×1  input slice and normalises the input accordingly. If  affine=true , it also applies  a shift and a rescale to the input through to learnable per-channel bias  β  and scale  γ  parameters. If  track_stats=true , accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase. Warning : the defaults for  affine  and  track_stats  used to be  true  in previous Flux versions (< v0.12). Examples julia> using Statistics\n\njulia> xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels\n\njulia> m = InstanceNorm(3);\n\njulia> y = m(xs);\n\njulia> isapprox(std(y, dims=1:2), ones(1, 1, 3, 2), atol=0.2) && std(y, dims=1:2) != std(xs, dims=1:2)\ntrue source"},{"id":221,"pagetitle":"Built-in Layers","title":"Flux.GroupNorm","ref":"/flux/stable/models/layers/#Flux.GroupNorm","content":" Flux.GroupNorm  —  Type GroupNorm(channels::Int, G::Int, λ = identity;\n          initβ = zeros32, \n          initγ = ones32,\n          affine = true, \n          eps = 1f-5, \n          momentum = 0.1f0) Group Normalization  layer. chs  is the number of channels, the channel dimension of your input. For an array of N dimensions, the  N-1 th index is the channel dimension. G  is the number of groups along which the statistics are computed. The number of channels must be an integer multiple of the number of groups. channels  should be the size of the channel dimension in your data (see below). Given an array with  N > 2  dimensions, call the  N-1 th the channel dimension. For  WHCN  images it's the usual channel dimension. If  affine=true , it also applies  a shift and a rescale to the input through to learnable per-channel bias  β  and scale  γ  parameters. Examples julia> using Statistics\n\njulia> xs = rand(3, 3, 4, 2);  # a batch of 2 images, each having 4 channels\n\njulia> m = GroupNorm(4, 2);\n\njulia> y = m(xs);\n\njulia> isapprox(std(y[:, :, 1:2, 1]), 1, atol=0.1) && std(xs[:, :, 1:2, 1]) != std(y[:, :, 1:2, 1])\ntrue\n\njulia> isapprox(std(y[:, :, 3:4, 2]), 1, atol=0.1) && std(xs[:, :, 3:4, 2]) != std(y[:, :, 3:4, 2])\ntrue source"},{"id":222,"pagetitle":"Built-in Layers","title":"Flux.normalise","ref":"/flux/stable/models/layers/#Flux.normalise","content":" Flux.normalise  —  Function normalise(x; dims=ndims(x), eps=1e-5) Normalise  x  to mean 0 and standard deviation 1 across the dimension(s) given by  dims . Per default,  dims  is the last dimension.   eps  is a small term added to the denominator for numerical stability. Examples julia> using Statistics\n\njulia> x = [90, 100, 110, 130, 70];\n\njulia> mean(x), std(x; corrected=false)\n(100.0, 20.0)\n\njulia> y = Flux.normalise(x)\n5-element Vector{Float64}:\n -0.49999975000012503\n  0.0\n  0.49999975000012503\n  1.499999250000375\n -1.499999250000375\n\njulia> isapprox(std(y; corrected=false), 1, atol=1e-5)\ntrue\n\njulia> x = rand(10:100, 10, 10);\n\njulia> y = Flux.normalise(x, dims=1);\n\njulia> isapprox(std(y; dims=1, corrected=false), ones(1, 10), atol=1e-5)\ntrue source"},{"id":223,"pagetitle":"Built-in Layers","title":"Test vs. Train","ref":"/flux/stable/models/layers/#Test-vs.-Train","content":" Test vs. Train Several normalisation layers behave differently under training and inference (testing). By default, Flux will automatically determine when a layer evaluation is part of training or inference.  Warning This automatic train/test detection works best with Zygote, the default automatic differentiation package. It may not work with other packages such as Tracker, Yota, or ForwardDiff. The functions  Flux.trainmode!  and  Flux.testmode!  let you manually specify which behaviour you want. When called on a model, they will place all layers within the model into the specified mode."},{"id":224,"pagetitle":"Built-in Layers","title":"Flux.testmode!","ref":"/flux/stable/models/layers/#Flux.testmode!-Tuple{Any}","content":" Flux.testmode!  —  Method testmode!(model, [mode]) -> model Set a layer, or all layers in a model, to test mode. This disables the effect of  Dropout  and some other regularisation layers. If you manually set a model into test mode, you need to manually place it back into train mode during training phase, using  trainmode! . There is an optional second argument, which takes a symbol  :auto  to reset all layers back to the default automatic mode. Example julia> d = Dropout(0.3)\nDropout(0.3)\n\njulia> testmode!(d)   # dropout is now always disabled\nDropout(0.3, active=false)\n\njulia> trainmode!(d)  # dropout is now always enabled\nDropout(0.3, active=true)\n\njulia> testmode!(d, :auto)  # back to default\nDropout(0.3) source"},{"id":225,"pagetitle":"Built-in Layers","title":"Flux.testmode!","ref":"/flux/stable/models/layers/#Flux.testmode!-Tuple{Any, Any}","content":" Flux.testmode!  —  Method testmode!(model, inactive) This two-argument method is largely internal. It recurses into the  model , and until a method like  testmode!(d::Dropout, inactive)  alters the activity of a layer. Custom layers can support manual  testmode!  /  trainmode!  switching by defining such a method. Possible values of   inactive  are: true  for testing, i.e.  active=false false  for training, same as  trainmode! (m) :auto  or  nothing  for Flux to detect training automatically. Compat This method may be removed in a future breaking change, to separate the user-facing  testmode!  from the internal recursion. source"},{"id":226,"pagetitle":"Built-in Layers","title":"Flux.trainmode!","ref":"/flux/stable/models/layers/#Flux.trainmode!","content":" Flux.trainmode!  —  Function trainmode!(model) -> model Set a layer, or all layers in a model, to training mode. Opposite to  testmode! , see further details there. source trainmode!(m, active) Warning This two-argument method is deprecated. Possible values of   active  are: true  for training, or  false  for testing, same as  testmode! (m) :auto  or  nothing  for Flux to detect training automatically. source"},{"id":229,"pagetitle":"Loss Functions","title":"Loss Functions","ref":"/flux/stable/models/losses/#man-losses","content":" Loss Functions Flux provides a large number of common loss functions used for training machine learning models. They are grouped together in the  Flux.Losses  module. Loss functions for supervised learning typically expect as inputs a target  y , and a prediction  ŷ  from your model. In Flux's convention, the order of the arguments is the following loss(ŷ, y) Most loss functions in Flux have an optional argument  agg , denoting the type of aggregation performed over the batch: loss(ŷ, y)                         # defaults to `mean`\nloss(ŷ, y, agg=sum)                # use `sum` for reduction\nloss(ŷ, y, agg=x->sum(x, dims=2))  # partial reduction\nloss(ŷ, y, agg=x->mean(w .* x))    # weighted mean\nloss(ŷ, y, agg=identity)           # no aggregation."},{"id":230,"pagetitle":"Loss Functions","title":"Function listing","ref":"/flux/stable/models/losses/#Function-listing","content":" Function listing"},{"id":231,"pagetitle":"Loss Functions","title":"Flux.Losses.mae","ref":"/flux/stable/models/losses/#Flux.Losses.mae","content":" Flux.Losses.mae  —  Function mae(ŷ, y; agg = mean) Return the loss corresponding to mean absolute error: agg(abs.(ŷ .- y)) Example julia> y_model = [1.1, 1.9, 3.1];\n\njulia> Flux.mae(y_model, 1:3)\n0.10000000000000009 source"},{"id":232,"pagetitle":"Loss Functions","title":"Flux.Losses.mse","ref":"/flux/stable/models/losses/#Flux.Losses.mse","content":" Flux.Losses.mse  —  Function mse(ŷ, y; agg = mean) Return the loss corresponding to mean square error: agg((ŷ .- y) .^ 2) See also:  mae ,  msle ,  crossentropy . Example julia> y_model = [1.1, 1.9, 3.1];\n\njulia> y_true = 1:3;\n\njulia> Flux.mse(y_model, y_true)\n0.010000000000000018 source"},{"id":233,"pagetitle":"Loss Functions","title":"Flux.Losses.msle","ref":"/flux/stable/models/losses/#Flux.Losses.msle","content":" Flux.Losses.msle  —  Function msle(ŷ, y; agg = mean, eps = eps(eltype(ŷ))) The loss corresponding to mean squared logarithmic errors, calculated as agg((log.(ŷ .+ ϵ) .- log.(y .+ ϵ)) .^ 2) The  ϵ == eps  term provides numerical stability. Penalizes an under-estimation more than an over-estimatation. Example julia> Flux.msle(Float32[1.1, 2.2, 3.3], 1:3)\n0.009084041f0\n\njulia> Flux.msle(Float32[0.9, 1.8, 2.7], 1:3)\n0.011100831f0 source"},{"id":234,"pagetitle":"Loss Functions","title":"Flux.Losses.huber_loss","ref":"/flux/stable/models/losses/#Flux.Losses.huber_loss","content":" Flux.Losses.huber_loss  —  Function huber_loss(ŷ, y; delta = 1, agg = mean) Return the mean of the  Huber loss  given the prediction  ŷ  and true values  y .              | 0.5 * |ŷ - y|^2,            for |ŷ - y| <= δ\nHuber loss = |\n             |  δ * (|ŷ - y| - 0.5 * δ), otherwise Example julia> ŷ = [1.1, 2.1, 3.1];\n\njulia> Flux.huber_loss(ŷ, 1:3)  # default δ = 1 > |ŷ - y|\n0.005000000000000009\n\njulia> Flux.huber_loss(ŷ, 1:3, delta=0.05)  # changes behaviour as |ŷ - y| > δ\n0.003750000000000005 source"},{"id":235,"pagetitle":"Loss Functions","title":"Flux.Losses.label_smoothing","ref":"/flux/stable/models/losses/#Flux.Losses.label_smoothing","content":" Flux.Losses.label_smoothing  —  Function label_smoothing(y::Union{Number, AbstractArray}, α; dims::Int=1) Returns smoothed labels, meaning the confidence on label values are relaxed. When  y  is given as one-hot vector or batch of one-hot, its calculated as y .* (1 - α) .+ α / size(y, dims) when  y  is given as a number or batch of numbers for binary classification, its calculated as y .* (1 - α) .+ α / 2 in which case the labels are squeezed towards  0.5 . α is a number in interval (0, 1) called the smoothing factor. Higher the value of α larger the smoothing of  y . dims  denotes the one-hot dimension, unless  dims=0  which denotes the application of label smoothing to binary distributions encoded in a single number. Example julia> y = Flux.onehotbatch([1, 1, 1, 0, 1, 0], 0:1)\n2×6 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n ⋅  ⋅  ⋅  1  ⋅  1\n 1  1  1  ⋅  1  ⋅\n\njulia> y_smoothed = Flux.label_smoothing(y, 0.2f0)\n2×6 Matrix{Float32}:\n 0.1  0.1  0.1  0.9  0.1  0.9\n 0.9  0.9  0.9  0.1  0.9  0.1\n\njulia> y_sim = softmax(y .* log(2f0))\n2×6 Matrix{Float32}:\n 0.333333  0.333333  0.333333  0.666667  0.333333  0.666667\n 0.666667  0.666667  0.666667  0.333333  0.666667  0.333333\n\njulia> y_dis = vcat(y_sim[2,:]', y_sim[1,:]')\n2×6 Matrix{Float32}:\n 0.666667  0.666667  0.666667  0.333333  0.666667  0.333333\n 0.333333  0.333333  0.333333  0.666667  0.333333  0.666667\n\njulia> Flux.crossentropy(y_sim, y) < Flux.crossentropy(y_sim, y_smoothed)\ntrue\n\njulia> Flux.crossentropy(y_dis, y) > Flux.crossentropy(y_dis, y_smoothed)\ntrue source"},{"id":236,"pagetitle":"Loss Functions","title":"Flux.Losses.crossentropy","ref":"/flux/stable/models/losses/#Flux.Losses.crossentropy","content":" Flux.Losses.crossentropy  —  Function crossentropy(ŷ, y; dims = 1, eps = eps(eltype(ŷ)), agg = mean) Return the cross entropy between the given probability distributions; calculated as agg(-sum(y .* log.(ŷ .+ ϵ); dims)) Cross entropy is typically used as a loss in multi-class classification, in which case the labels  y  are given in a one-hot format.  dims  specifies the dimension (or the dimensions) containing the class probabilities. The prediction  ŷ  is supposed to sum to one across  dims , as would be the case with the output of a  softmax  operation. For numerical stability, it is recommended to use  logitcrossentropy  rather than  softmax  followed by  crossentropy  . Use  label_smoothing  to smooth the true labels as preprocessing before computing the loss. See also:  logitcrossentropy ,  binarycrossentropy ,  logitbinarycrossentropy . Example julia> y_label = Flux.onehotbatch([0, 1, 2, 1, 0], 0:2)\n3×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  ⋅  1\n ⋅  1  ⋅  1  ⋅\n ⋅  ⋅  1  ⋅  ⋅\n\njulia> y_model = softmax(reshape(-7:7, 3, 5) .* 1f0)\n3×5 Matrix{Float32}:\n 0.0900306  0.0900306  0.0900306  0.0900306  0.0900306\n 0.244728   0.244728   0.244728   0.244728   0.244728\n 0.665241   0.665241   0.665241   0.665241   0.665241\n\njulia> sum(y_model; dims=1)\n1×5 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0\n\njulia> Flux.crossentropy(y_model, y_label)\n1.6076053f0\n\njulia> 5 * ans ≈ Flux.crossentropy(y_model, y_label; agg=sum)\ntrue\n\njulia> y_smooth = Flux.label_smoothing(y_label, 0.15f0)\n3×5 Matrix{Float32}:\n 0.9   0.05  0.05  0.05  0.9\n 0.05  0.9   0.05  0.9   0.05\n 0.05  0.05  0.9   0.05  0.05\n\njulia> Flux.crossentropy(y_model, y_smooth)\n1.5776052f0 source"},{"id":237,"pagetitle":"Loss Functions","title":"Flux.Losses.logitcrossentropy","ref":"/flux/stable/models/losses/#Flux.Losses.logitcrossentropy","content":" Flux.Losses.logitcrossentropy  —  Function logitcrossentropy(ŷ, y; dims = 1, agg = mean) Return the cross entropy calculated by agg(-sum(y .* logsoftmax(ŷ; dims); dims)) This is mathematically equivalent to  crossentropy(softmax(ŷ), y) , but is more numerically stable than using functions  crossentropy  and  softmax  separately. See also:  binarycrossentropy ,  logitbinarycrossentropy ,  label_smoothing . Example julia> y_label = Flux.onehotbatch(collect(\"abcabaa\"), 'a':'c')\n3×7 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  1  ⋅  1  1\n ⋅  1  ⋅  ⋅  1  ⋅  ⋅\n ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n\njulia> y_model = reshape(vcat(-9:0, 0:9, 7.5f0), 3, 7)\n3×7 Matrix{Float32}:\n -9.0  -6.0  -3.0  0.0  2.0  5.0  8.0\n -8.0  -5.0  -2.0  0.0  3.0  6.0  9.0\n -7.0  -4.0  -1.0  1.0  4.0  7.0  7.5\n\njulia> Flux.logitcrossentropy(y_model, y_label)\n1.5791205f0\n\njulia> Flux.crossentropy(softmax(y_model), y_label)\n1.5791197f0 source"},{"id":238,"pagetitle":"Loss Functions","title":"Flux.Losses.binarycrossentropy","ref":"/flux/stable/models/losses/#Flux.Losses.binarycrossentropy","content":" Flux.Losses.binarycrossentropy  —  Function binarycrossentropy(ŷ, y; agg = mean, eps = eps(eltype(ŷ))) Return the binary cross-entropy loss, computed as agg(@.(-y * log(ŷ + ϵ) - (1 - y) * log(1 - ŷ + ϵ))) Where typically, the prediction  ŷ  is given by the output of a  sigmoid  activation. The  ϵ == eps  term is included to avoid infinity. Using  logitbinarycrossentropy  is recomended over  binarycrossentropy  for numerical stability. Use  label_smoothing  to smooth the  y  value as preprocessing before computing the loss. See also:  crossentropy ,  logitcrossentropy . Examples julia> y_bin = Bool[1,0,1]\n3-element Vector{Bool}:\n 1\n 0\n 1\n\njulia> y_prob = softmax(reshape(vcat(1:3, 3:5), 2, 3) .* 1f0)\n2×3 Matrix{Float32}:\n 0.268941  0.5  0.268941\n 0.731059  0.5  0.731059\n\njulia> Flux.binarycrossentropy(y_prob[2,:], y_bin)\n0.43989f0\n\njulia> all(p -> 0 < p < 1, y_prob[2,:])  # else DomainError\ntrue\n\njulia> y_hot = Flux.onehotbatch(y_bin, 0:1)\n2×3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n ⋅  1  ⋅\n 1  ⋅  1\n\njulia> Flux.crossentropy(y_prob, y_hot)\n0.43989f0 source"},{"id":239,"pagetitle":"Loss Functions","title":"Flux.Losses.logitbinarycrossentropy","ref":"/flux/stable/models/losses/#Flux.Losses.logitbinarycrossentropy","content":" Flux.Losses.logitbinarycrossentropy  —  Function logitbinarycrossentropy(ŷ, y; agg = mean) Mathematically equivalent to  binarycrossentropy(σ(ŷ), y)  but is more numerically stable. See also:  crossentropy ,  logitcrossentropy . Examples julia> y_bin = Bool[1,0,1];\n\njulia> y_model = Float32[2, -1, pi]\n3-element Vector{Float32}:\n  2.0\n -1.0\n  3.1415927\n\njulia> Flux.logitbinarycrossentropy(y_model, y_bin)\n0.160832f0\n\njulia> Flux.binarycrossentropy(sigmoid.(y_model), y_bin)\n0.16083185f0 source"},{"id":240,"pagetitle":"Loss Functions","title":"Flux.Losses.kldivergence","ref":"/flux/stable/models/losses/#Flux.Losses.kldivergence","content":" Flux.Losses.kldivergence  —  Function kldivergence(ŷ, y; agg = mean, eps = eps(eltype(ŷ))) Return the  Kullback-Leibler divergence  between the given probability distributions. The KL divergence is a measure of how much one probability distribution is different from the other. It is always non-negative, and zero only when both the distributions are equal. Example julia> p1 = [1 0; 0 1]\n2×2 Matrix{Int64}:\n 1  0\n 0  1\n\njulia> p2 = fill(0.5, 2, 2)\n2×2 Matrix{Float64}:\n 0.5  0.5\n 0.5  0.5\n\njulia> Flux.kldivergence(p2, p1) ≈ log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p1; agg = sum) ≈ 2log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p2; eps = 0)  # about -2e-16 with the regulator\n0.0\n\njulia> Flux.kldivergence(p1, p2; eps = 0)  # about 17.3 with the regulator\nInf source"},{"id":241,"pagetitle":"Loss Functions","title":"Flux.Losses.poisson_loss","ref":"/flux/stable/models/losses/#Flux.Losses.poisson_loss","content":" Flux.Losses.poisson_loss  —  Function poisson_loss(ŷ, y; agg = mean) Return how much the predicted distribution  ŷ  diverges from the expected Poisson distribution  y ; calculated as - sum(ŷ .- y .* log.(ŷ)) / size(y, 2) More information. . Example julia> y_model = [1, 3, 3];  # data should only take integral values\n\njulia> Flux.poisson_loss(y_model, 1:3)\n0.5023128522198171 source"},{"id":242,"pagetitle":"Loss Functions","title":"Flux.Losses.hinge_loss","ref":"/flux/stable/models/losses/#Flux.Losses.hinge_loss","content":" Flux.Losses.hinge_loss  —  Function hinge_loss(ŷ, y; agg = mean) Return the  hinge_loss  given the prediction  ŷ  and true labels  y  (containing 1 or -1); calculated as sum(max.(0, 1 .- ŷ .* y)) / size(y, 2) Usually used with classifiers like Support Vector Machines. See also:  squared_hinge_loss Example julia> y_true = [1, -1, 1, 1];\n\njulia> y_pred = [0.1, 0.3, 1, 1.5];\n\njulia> Flux.hinge_loss(y_pred, y_true)\n0.55\n\njulia> Flux.hinge_loss(y_pred[1], y_true[1]) != 0  # same sign but |ŷ| < 1\ntrue\n\njulia> Flux.hinge_loss(y_pred[end], y_true[end]) == 0  # same sign but |ŷ| >= 1\ntrue\n\njulia> Flux.hinge_loss(y_pred[2], y_true[2]) != 0 # opposite signs\ntrue source"},{"id":243,"pagetitle":"Loss Functions","title":"Flux.Losses.squared_hinge_loss","ref":"/flux/stable/models/losses/#Flux.Losses.squared_hinge_loss","content":" Flux.Losses.squared_hinge_loss  —  Function squared_hinge_loss(ŷ, y) Return the squared hinge_loss loss given the prediction  ŷ  and true labels  y  (containing 1 or -1); calculated as sum((max.(0, 1 .- ŷ .* y)).^2) / size(y, 2) Usually used with classifiers like Support Vector Machines. See also:  hinge_loss Example julia> y_true = [1, -1, 1, 1];\n\njulia> y_pred = [0.1, 0.3, 1, 1.5];\n\njulia> Flux.squared_hinge_loss(y_pred, y_true)\n0.625\n\njulia> Flux.squared_hinge_loss(y_pred[1], y_true[1]) != 0\ntrue\n\njulia> Flux.squared_hinge_loss(y_pred[end], y_true[end]) == 0\ntrue\n\njulia> Flux.squared_hinge_loss(y_pred[2], y_true[2]) != 0\ntrue source"},{"id":244,"pagetitle":"Loss Functions","title":"Flux.Losses.dice_coeff_loss","ref":"/flux/stable/models/losses/#Flux.Losses.dice_coeff_loss","content":" Flux.Losses.dice_coeff_loss  —  Function dice_coeff_loss(ŷ, y; smooth = 1) Return a loss based on the dice coefficient. Used in the  V-Net  image segmentation architecture. The dice coefficient is similar to the F1_score. Loss calculated as: 1 - 2*sum(|ŷ .* y| + smooth) / (sum(ŷ.^2) + sum(y.^2) + smooth) Example julia> y_pred = [1.1, 2.1, 3.1];\n\njulia> Flux.dice_coeff_loss(y_pred, 1:3)\n0.000992391663909964\n\njulia> 1 - Flux.dice_coeff_loss(y_pred, 1:3)  # ~ F1 score for image segmentation\n0.99900760833609 source"},{"id":245,"pagetitle":"Loss Functions","title":"Flux.Losses.tversky_loss","ref":"/flux/stable/models/losses/#Flux.Losses.tversky_loss","content":" Flux.Losses.tversky_loss  —  Function tversky_loss(ŷ, y; beta = 0.7) Return the  Tversky loss . Used with imbalanced data to give more weight to false negatives. Larger  β == beta  weigh recall more than precision (by placing more emphasis on false negatives). Calculated as: 1 - sum(|y .* ŷ| + 1) / (sum(y .* ŷ + (1 - β)*(1 .- y) .* ŷ + β*y .* (1 .- ŷ)) + 1) source"},{"id":246,"pagetitle":"Loss Functions","title":"Flux.Losses.binary_focal_loss","ref":"/flux/stable/models/losses/#Flux.Losses.binary_focal_loss","content":" Flux.Losses.binary_focal_loss  —  Function binary_focal_loss(ŷ, y; agg=mean, gamma=2, eps=eps(eltype(ŷ))) Return the  binary focal loss  The input, 'ŷ', is expected to be normalized (i.e.  softmax  output). For  gamma = 0 , the loss is mathematically equivalent to  Losses.binarycrossentropy . See also:  Losses.focal_loss  for multi-class setting Example julia> y = [0  1  0\n            1  0  1]\n2×3 Matrix{Int64}:\n 0  1  0\n 1  0  1\n\njulia> ŷ = [0.268941  0.5  0.268941\n            0.731059  0.5  0.731059]\n2×3 Matrix{Float64}:\n 0.268941  0.5  0.268941\n 0.731059  0.5  0.731059\n\njulia> Flux.binary_focal_loss(ŷ, y) ≈ 0.0728675615927385\ntrue source"},{"id":247,"pagetitle":"Loss Functions","title":"Flux.Losses.focal_loss","ref":"/flux/stable/models/losses/#Flux.Losses.focal_loss","content":" Flux.Losses.focal_loss  —  Function focal_loss(ŷ, y; dims=1, agg=mean, gamma=2, eps=eps(eltype(ŷ))) Return the  focal_loss  which can be used in classification tasks with highly imbalanced classes. It down-weights well-classified examples and focuses on hard examples. The input, 'ŷ', is expected to be normalized (i.e.  softmax  output). The modulating factor,  γ == gamma , controls the down-weighting strength. For  γ == 0 , the loss is mathematically equivalent to  Losses.crossentropy . Example julia> y = [1  0  0  0  1\n            0  1  0  1  0\n            0  0  1  0  0]\n3×5 Matrix{Int64}:\n 1  0  0  0  1\n 0  1  0  1  0\n 0  0  1  0  0\n\njulia> ŷ = softmax(reshape(-7:7, 3, 5) .* 1f0)\n3×5 Matrix{Float32}:\n 0.0900306  0.0900306  0.0900306  0.0900306  0.0900306\n 0.244728   0.244728   0.244728   0.244728   0.244728\n 0.665241   0.665241   0.665241   0.665241   0.665241\n\njulia> Flux.focal_loss(ŷ, y) ≈ 1.1277571935622628\ntrue See also:  Losses.binary_focal_loss  for binary (not one-hot) labels source"},{"id":248,"pagetitle":"Loss Functions","title":"Flux.Losses.siamese_contrastive_loss","ref":"/flux/stable/models/losses/#Flux.Losses.siamese_contrastive_loss","content":" Flux.Losses.siamese_contrastive_loss  —  Function siamese_contrastive_loss(ŷ, y; margin = 1, agg = mean) Return the  contrastive loss  which can be useful for training Siamese Networks. It is given by agg(@. (1 - y) * ŷ^2 + y * max(0, margin - ŷ)^2) Specify  margin  to set the baseline for distance at which pairs are dissimilar. Example julia> ŷ = [0.5, 1.5, 2.5];\n\njulia> Flux.siamese_contrastive_loss(ŷ, 1:3)\n-4.833333333333333\n\njulia> Flux.siamese_contrastive_loss(ŷ, 1:3, margin = 2)\n-4.0 source"},{"id":251,"pagetitle":"Low-level Operations – NNlib.jl","title":"Neural Network primitives from NNlib.jl","ref":"/flux/stable/models/nnlib/#Neural-Network-primitives-from-NNlib.jl","content":" Neural Network primitives from NNlib.jl Flux re-exports all of the functions exported by the  NNlib  package. This includes activation functions, described on  their own page . Many of the functions on this page exist primarily as the internal implementation of Flux layer, but can also be used independently."},{"id":252,"pagetitle":"Low-level Operations – NNlib.jl","title":"Attention","ref":"/flux/stable/models/nnlib/#Attention","content":" Attention Primitives for the  MultiHeadAttention  layer."},{"id":253,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.dot_product_attention","ref":"/flux/stable/models/nnlib/#NNlib.dot_product_attention","content":" NNlib.dot_product_attention  —  Function dot_product_attention(query, key, value, [bias]; [fdrop, mask, nheads]) Multihead dot product attention used in transformer architectures. The input arrays must have the first two dimensions given by the number of features and the sequence length, then an arbitrary number of batch dimensions or none. Returns the attention output array of size  (v_dim, q_len, batch_size...)  and the attention scores of size  (kv_len, q_len, nheads, batch_size...) . See also  dot_product_attention_scores  if you only need the attention scores. Arguments query : Query array of size  (qk_dim, q_len, batch_size...) . key : Key array of size  (qk_dim, kv_len, batch_size...) . value : Value array of size  (v_dim, kv_len, batch_size...) . bias : Either  nothing  or an array broadcastable to size  (kv_len, q_len, nheads, batch_size) .         It will be added to the attention scores before applying the softmax. Default  nothing . fdrop : A dropout function or layer to be applied on the attention scores right after the softmax.          Default  identity  (no dropout). mask : Either  nothing  or a boolean array broadcastable to size  (kv_len, q_len, nheads, batch_size) .         The mask is applied to the attention scores just before the softmax.         See  make_causal_mask  fore creating causal masks. Default  nothing . nheads : Number of heads to split the input arrays into. Default  1 . Examples q, k, v = rand(10, 20, 2), rand(10, 30, 2), rand(20, 30, 2)\ny, α = dot_product_attention(q, k, v)"},{"id":254,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.dot_product_attention_scores","ref":"/flux/stable/models/nnlib/#NNlib.dot_product_attention_scores","content":" NNlib.dot_product_attention_scores  —  Function dot_product_attention_scores(query, key, [bias]; [fdrop, mask]) Return the attention scores for the  dot_product_attention . Input arrays must have dimensions  (num_features ÷ nheads, nheads, sequence_length, batch_size) . See  dot_product_attention  for more details."},{"id":255,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.make_causal_mask","ref":"/flux/stable/models/nnlib/#NNlib.make_causal_mask","content":" NNlib.make_causal_mask  —  Function make_causal_mask(x, dims=2) Return a boolean square matrix  m  of the same type as  x  and of side  size(x, dims) . Its elements are set such that  m[i, j] == i ≤ j . Can be used to mask the attention scores in  dot_product_attention ."},{"id":256,"pagetitle":"Low-level Operations – NNlib.jl","title":"Softmax","ref":"/flux/stable/models/nnlib/#Softmax","content":" Softmax Flux 's  Flux.logitcrossentropy  uses  NNlib.logsoftmax  internally."},{"id":257,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.softmax","ref":"/flux/stable/models/nnlib/#NNlib.softmax","content":" NNlib.softmax  —  Function softmax(x; dims = 1) Softmax  turns input array  x  into probability distributions that sum to 1 along the dimensions specified by  dims . It is semantically equivalent to the following: softmax(x; dims = 1) = exp.(x) ./ sum(exp.(x), dims = dims) with additional manipulations enhancing numerical stability. For a matrix input  x  it will by default ( dims = 1 ) treat it as a batch of vectors, with each column independent. Keyword  dims = 2  will instead treat rows independently, and so on. See also  logsoftmax . Examples julia> softmax([1, 2, 3])\n3-element Vector{Float64}:\n 0.09003057317038046\n 0.24472847105479764\n 0.6652409557748218\n\njulia> softmax([1 2 3; 2 2 2])  # dims=1\n2×3 Matrix{Float64}:\n 0.268941  0.5  0.731059\n 0.731059  0.5  0.268941\n\njulia> softmax([1 2 3; 2 2 2]; dims=2)\n2×3 Matrix{Float64}:\n 0.0900306  0.244728  0.665241\n 0.333333   0.333333  0.333333 Note that, when used with Flux.jl,  softmax  must not be passed to layers like  Dense  which accept an activation function. The activation is broadcasted over the result, thus applies to individual numbers. But  softmax  always needs to see the whole column. julia> using Flux\n\njulia> x = randn(Float32, 4, 4, 3, 13);\n\njulia> model = Chain(Conv((4, 4), 3 => 8, tanh), Flux.flatten, Dense(8 => 7), softmax);\n\njulia> model(x) |> size\n(7, 13)\n\njulia> Dense(4 => 7, softmax)(x)\nERROR: `softmax(x)` called with a number, but it expects an array. "},{"id":258,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.logsoftmax","ref":"/flux/stable/models/nnlib/#NNlib.logsoftmax","content":" NNlib.logsoftmax  —  Function logsoftmax(x; dims = 1) Computes the log of softmax in a more numerically stable way than directly taking  log.(softmax(xs)) . Commonly used in computing cross entropy loss. It is semantically equivalent to the following: logsoftmax(x; dims = 1) = x .- log.(sum(exp.(x), dims = dims)) See also  softmax ."},{"id":259,"pagetitle":"Low-level Operations – NNlib.jl","title":"Pooling","ref":"/flux/stable/models/nnlib/#Pooling","content":" Pooling Flux 's  AdaptiveMaxPool ,  AdaptiveMeanPool ,  GlobalMaxPool ,  GlobalMeanPool ,   MaxPool , and  MeanPool  use  NNlib.PoolDims ,  NNlib.maxpool , and  NNlib.meanpool  as their backend."},{"id":260,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.PoolDims","ref":"/flux/stable/models/nnlib/#NNlib.PoolDims","content":" NNlib.PoolDims  —  Type PoolDims(x_size::NTuple{M}, k::Union{NTuple{L, Int}, Int};\n        stride=k, padding=0, dilation=1)  where {M, L} Dimensions for a \"pooling\" operation that can have an arbitrary input size, kernel size, stride, dilation, and channel count.  Used to dispatch onto efficient implementations at compile-time."},{"id":261,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.lpnormpool","ref":"/flux/stable/models/nnlib/#NNlib.lpnormpool","content":" NNlib.lpnormpool  —  Function lpnormpool(x, p::Real, k::NTuple{N, Integer}; pad=0, stride=k) Perform Lp pool operation with value of the Lp norm  p  and window size  k  on input tensor  x , also known as LPPool in pytorch. This pooling operator from  Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always length(k) == ndim(x) - 2` p  is restricted to  0 < p < Inf . pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k . For all elements  x  in a size  k  window, lpnormpool computes  (∑ᵢ xᵢ^p)^(1 / p)  as an element of the output. Thus  lpnormpool(x, 1, k) ./ prod(k) ≈ meanpool(x, k)  and  lpnormpool(x, 2, k).^2 ./ prod(k) ≈ meanpool(x.^2, k) ."},{"id":262,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.maxpool","ref":"/flux/stable/models/nnlib/#NNlib.maxpool","content":" NNlib.maxpool  —  Function maxpool(x, k::NTuple{N, Integer}; pad=0, stride=k) Perform max pool operation with window size  k  on input tensor  x . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always  length(k) == ndim(x) - 2 pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k ."},{"id":263,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.meanpool","ref":"/flux/stable/models/nnlib/#NNlib.meanpool","content":" NNlib.meanpool  —  Function meanpool(x, k::NTuple{N, Integer}; pad=0, stride=k) Perform mean pool operation with window size  k  on input tensor  x . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always length(k) == ndim(x) - 2` pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k ."},{"id":264,"pagetitle":"Low-level Operations – NNlib.jl","title":"Padding","ref":"/flux/stable/models/nnlib/#Padding","content":" Padding"},{"id":265,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.pad_circular","ref":"/flux/stable/models/nnlib/#NNlib.pad_circular","content":" NNlib.pad_circular  —  Function pad_circular(x, pad::Tuple; [dims])\npad_circular(x, pad::Int; [dims]) Pad the array  x  \"circularly\" across the border by wrapping around values from the opposite side of  x .  pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  The pad length on either side in any dimension must not exceed the size of  x  in that dimension, i.e.  pad_circular  is not able to create abitrary sized tilings of  x . See also  pad_repeat ,  pad_reflect ,  pad_symmetric , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_circular(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 9  3  6  9  3  6\n 7  1  4  7  1  4\n 8  2  5  8  2  5\n 9  3  6  9  3  6\n 7  1  4  7  1  4\n 8  2  5  8  2  5"},{"id":266,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.pad_constant","ref":"/flux/stable/models/nnlib/#NNlib.pad_constant","content":" NNlib.pad_constant  —  Function pad_constant(x, pad::Tuple, val = 0; [dims = :])\npad_constant(x, pad::Int, val = 0; [dims = :]) Pad the array  x  with the constant value  val . pad  can be a tuple of integers. If it is of some length  2 * length(dims)  that specifies the left and right padding size for each of the dimensions in  dims  as  (l1, r1, ..., ln, rn) .  If supplied with a tuple of length  length(dims)  instead, it applies symmetric padding. If  dims  is not given, it defaults to all dimensions. For integer  pad  input, it is applied on both sides on every dimension in  dims . See also  pad_zeros ,  pad_repeat ,  pad_reflect ,  pad_symmetric , and  pad_circular . julia> r = reshape(1:4, 2, 2)\n2×2 reshape(::UnitRange{Int64}, 2, 2) with eltype Int64:\n 1  3\n 2  4\n\njulia> pad_constant(r, (1, 2, 3, 4), 8)\n5×9 Matrix{Int64}:\n 8  8  8  8  8  8  8  8  8\n 8  8  8  1  3  8  8  8  8\n 8  8  8  2  4  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n\njulia> pad_constant(r, 1, 8)\n4×4 Matrix{Int64}:\n 8  8  8  8\n 8  1  3  8\n 8  2  4  8\n 8  8  8  8\n\njulia> r = reshape(1:27, 3, 3, 3)\n3×3×3 reshape(::UnitRange{Int64}, 3, 3, 3) with eltype Int64:\n[:, :, 1] =\n 1  4  7\n 2  5  8\n 3  6  9\n\n[:, :, 2] =\n 10  13  16\n 11  14  17\n 12  15  18\n\n[:, :, 3] =\n 19  22  25\n 20  23  26\n 21  24  27\n\njulia> pad_constant(r, (2,1), dims = 1) # assymetric padding\n6×3×3 Array{Int64, 3}:\n[:, :, 1] =\n 0  0  0\n 0  0  0\n 1  4  7\n 2  5  8\n 3  6  9\n 0  0  0\n\n[:, :, 2] =\n  0   0   0\n  0   0   0\n 10  13  16\n 11  14  17\n 12  15  18\n  0   0   0\n\n[:, :, 3] =\n  0   0   0\n  0   0   0\n 19  22  25\n 20  23  26\n 21  24  27\n  0   0   0\n\njulia> pad_constant(r, (2,1, 3), dims = (1,2)) # padding must always be either the same length as dims, or double it\nERROR: ArgumentError: Could not parse padding (2, 1, 3) and dims (1, 2)\nStacktrace:\n[...]"},{"id":267,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.pad_reflect","ref":"/flux/stable/models/nnlib/#NNlib.pad_reflect","content":" NNlib.pad_reflect  —  Function pad_reflect(x, pad::Tuple; [dims])\npad_reflect(x, pad::Int; [dims]) Pad the array  x  reflecting its values across the border. pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_repeat ,  pad_symmetric ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_reflect(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 5  2  5  8  5  2\n 4  1  4  7  4  1\n 5  2  5  8  5  2\n 6  3  6  9  6  3\n 5  2  5  8  5  2\n 4  1  4  7  4  1"},{"id":268,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.pad_repeat","ref":"/flux/stable/models/nnlib/#NNlib.pad_repeat","content":" NNlib.pad_repeat  —  Function pad_repeat(x, pad::Tuple; [dims])\npad_repeat(x, pad::Int; [dims]) Pad the array  x  repeating the values on the border. pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_reflect ,  pad_symmetric ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_repeat(r, (1,2,3,4))\n6×10 Matrix{Int64}:\n 1  1  1  1  4  7  7  7  7  7\n 1  1  1  1  4  7  7  7  7  7\n 2  2  2  2  5  8  8  8  8  8\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9"},{"id":269,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.pad_symmetric","ref":"/flux/stable/models/nnlib/#NNlib.pad_symmetric","content":" NNlib.pad_symmetric  —  Function pad_symmetric(x, pad::Tuple; [dims])\npad_symmetric(x, pad::Int; [dims]) Pad the array  x  reflecting its values symmetrically across the border, i.e. the border values of  x  are present in the padding values, in contrast to  pad_reflect . pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_repeat ,  pad_reflect ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_symmetric(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 1  1  4  7  7  4\n 1  1  4  7  7  4\n 2  2  5  8  8  5\n 3  3  6  9  9  6\n 3  3  6  9  9  6\n 2  2  5  8  8  5"},{"id":270,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.pad_zeros","ref":"/flux/stable/models/nnlib/#NNlib.pad_zeros","content":" NNlib.pad_zeros  —  Function pad_zeros(x, pad::Tuple; [dims])\npad_zeros(x, pad::Int; [dims]) Pad the array  x  with zeros. Equivalent to  pad_constant  with the constant equal to 0. "},{"id":271,"pagetitle":"Low-level Operations – NNlib.jl","title":"Convolution","ref":"/flux/stable/models/nnlib/#Convolution","content":" Convolution Flux 's  Conv  and  CrossCor  layers use  NNlib.DenseConvDims  and  NNlib.conv  internally. "},{"id":272,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.conv","ref":"/flux/stable/models/nnlib/#NNlib.conv","content":" NNlib.conv  —  Function conv(x, w; stride = 1, pad = 0, dilation = 1, flipped = false, groups = 1) Apply convolution filter  w  to input  x .  x  and  w  are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively.  x  and  w  may have real or complex element types."},{"id":273,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.ConvDims","ref":"/flux/stable/models/nnlib/#NNlib.ConvDims","content":" NNlib.ConvDims  —  Type ConvDims Type system-level information about convolution dimensions. Critical for things like  im2col!()  to generate efficient code, and helpful to reduce the number of kwargs getting passed around."},{"id":274,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.depthwiseconv","ref":"/flux/stable/models/nnlib/#NNlib.depthwiseconv","content":" NNlib.depthwiseconv  —  Function depthwiseconv(x, w; stride=1, pad=0, dilation=1, flipped=false) Depthwise convolution operation with filter  w  on input  x .  x  and  w  are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively."},{"id":275,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.DepthwiseConvDims","ref":"/flux/stable/models/nnlib/#NNlib.DepthwiseConvDims","content":" NNlib.DepthwiseConvDims  —  Type DepthwiseConvDims Concrete subclass of  ConvDims  for a depthwise convolution.  Differs primarily due to characterization by C in, C mult, rather than C in, C out.  Useful to be separate from DenseConvDims primarily for channel calculation differences."},{"id":276,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.DenseConvDims","ref":"/flux/stable/models/nnlib/#NNlib.DenseConvDims","content":" NNlib.DenseConvDims  —  Type DenseConvDims Concrete subclass of  ConvDims  for a normal, dense, conv2d/conv3d."},{"id":277,"pagetitle":"Low-level Operations – NNlib.jl","title":"Dropout","ref":"/flux/stable/models/nnlib/#Dropout","content":" Dropout"},{"id":278,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.dropout","ref":"/flux/stable/models/nnlib/#NNlib.dropout","content":" NNlib.dropout  —  Function dropout([rng], A, p; [dims]) Returns an array in which each element of  A  is either replaced with zero, with probability  p , or else multiplied by  1/(1-p) . By default every element is treated independently. With keyword  dims=1 , a choice is made for every value of the 1st index i.e. each row of a matrix is either zero or not. Optional first argument is the random number generator used. Examples julia> dropout(ones(2, 10), 0.2)\n2×10 Matrix{Float64}:\n 1.25  1.25  0.0   1.25  1.25  1.25  1.25  1.25  1.25  1.25\n 1.25  1.25  1.25  0.0   1.25  1.25  0.0   1.25  1.25  1.25\n\njulia> mean(dropout(ones(10^4, 5), 0.2), dims=1)\n1×5 Matrix{Float64}:\n 0.998  1.00075  0.99125  0.99575  1.00075\n\njulia> dropout(ones(5, 5), 0.7, dims=1)  # whole row the same\n5×5 Matrix{Float64}:\n 3.33333  3.33333  3.33333  3.33333  3.33333\n 0.0      0.0      0.0      0.0      0.0\n 0.0      0.0      0.0      0.0      0.0\n 3.33333  3.33333  3.33333  3.33333  3.33333\n 0.0      0.0      0.0      0.0      0.0\n\njulia> mean(dropout(ones(10^4, 5), 0.3, dims=1), dims=1)\n1×5 Matrix{Float64}:\n 1.00571  1.00571  1.00571  1.00571  1.00571"},{"id":279,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.dropout!","ref":"/flux/stable/models/nnlib/#NNlib.dropout!","content":" NNlib.dropout!  —  Function dropout!(B, A, p; [dims]) This does exactly  B .= dropout(A, p; dims) , or rather, it's the implementation of out-of-place  dropout ."},{"id":280,"pagetitle":"Low-level Operations – NNlib.jl","title":"Upsampling","ref":"/flux/stable/models/nnlib/#Upsampling","content":" Upsampling Flux 's  Upsample  layer uses  NNlib.upsample_nearest ,  NNlib.upsample_bilinear , and  NNlib.upsample_trilinear  as its backend. Additionally,  Flux 's  PixelShuffle  layer uses  NNlib.pixel_shuffle  as its backend."},{"id":281,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.upsample_nearest","ref":"/flux/stable/models/nnlib/#NNlib.upsample_nearest","content":" NNlib.upsample_nearest  —  Function upsample_nearest(x, scale::NTuple{S,Int})\nupsample_nearest(x; size::NTuple{S,Int}) Upsamples the array  x  by integer multiples along the first  S  dimensions. Subsequent dimensions of  x  are not altered. Either the  scale  factors or the final output  size  can be specified. See also  upsample_bilinear , for two dimensions of an  N=4  array. Example julia> upsample_nearest([1 2 3; 4 5 6], (2, 3))\n4×9 Matrix{Int64}:\n 1  1  1  2  2  2  3  3  3\n 1  1  1  2  2  2  3  3  3\n 4  4  4  5  5  5  6  6  6\n 4  4  4  5  5  5  6  6  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6]; size=(4, 9))  # equivalent\ntrue\n\njulia> upsample_nearest([1 2 3; 4 5 6], (2,))\n4×3 Matrix{Int64}:\n 1  2  3\n 1  2  3\n 4  5  6\n 4  5  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6], size=(4,))\ntrue"},{"id":282,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.upsample_linear","ref":"/flux/stable/models/nnlib/#NNlib.upsample_linear","content":" NNlib.upsample_linear  —  Function upsample_linear(x::AbstractArray{T,3}, scale::Real; align_corners::Bool = true)\nupsample_linear(x::AbstractArray{T,3}; size::Integer, align_corners::Bool = true) Upsamples the first dimension of the array  x  by the upsample provided  scale , using linear interpolation. As an alternative to using  scale , the resulting array  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale*S1, S2, S3) , where  S1, S2, S3 = size(x) ."},{"id":283,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.∇upsample_linear","ref":"/flux/stable/models/nnlib/#NNlib.∇upsample_linear","content":" NNlib.∇upsample_linear  —  Function ∇upsample_linear(Δ::AbstractArray{T,3}; size::Integer, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Size of the image upsampled in the first place Outputs dx : Downsampled version of  Δ"},{"id":284,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.upsample_bilinear","ref":"/flux/stable/models/nnlib/#NNlib.upsample_bilinear","content":" NNlib.upsample_bilinear  —  Function upsample_bilinear(x::AbstractArray{T,4}, scale::NTuple{2,Real}; align_corners::Bool = true)\nupsample_bilinear(x::AbstractArray{T,4}; size::NTuple{2,Integer}, align_corners::Bool = true) Upsamples the first 2 dimensions of the array  x  by the upsample factors stored in  scale , using bilinear interpolation. As an alternative to using  scale , the resulting image  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale[1]*S1, scale[2]*S2, S3, S4) , where  S1, S2, S3, S4 = size(x) . Examples julia> x = reshape(Float32[1 2 3; 4 5 6], (2,3,1,1))\n2×3×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  2.0  3.0\n 4.0  5.0  6.0\n\njulia> upsample_bilinear(x, (2, 3))\n4×9×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  1.25  1.5  1.75  2.0  2.25  2.5  2.75  3.0\n 2.0  2.25  2.5  2.75  3.0  3.25  3.5  3.75  4.0\n 3.0  3.25  3.5  3.75  4.0  4.25  4.5  4.75  5.0\n 4.0  4.25  4.5  4.75  5.0  5.25  5.5  5.75  6.0\n\njulia> ans == upsample_bilinear(x; size=(4, 9))  # specify ouput size instead\ntrue\n\njulia> upsample_bilinear(x, (2.5, 3.5))  # non-integer scaling factors are allowed\n5×10×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0   1.22222  1.44444  1.66667  1.88889  …  2.33333  2.55556  2.77778  3.0\n 1.75  1.97222  2.19444  2.41667  2.63889     3.08333  3.30556  3.52778  3.75\n 2.5   2.72222  2.94444  3.16667  3.38889     3.83333  4.05556  4.27778  4.5\n 3.25  3.47222  3.69444  3.91667  4.13889     4.58333  4.80556  5.02778  5.25\n 4.0   4.22222  4.44444  4.66667  4.88889     5.33333  5.55556  5.77778  6.0"},{"id":285,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.∇upsample_bilinear","ref":"/flux/stable/models/nnlib/#NNlib.∇upsample_bilinear","content":" NNlib.∇upsample_bilinear  —  Function ∇upsample_bilinear(Δ::AbstractArray{T,4}; size::NTuple{2,Integer}, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Lateral (W,H) size of the image upsampled in the first place Outputs dx : Downsampled version of  Δ"},{"id":286,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.upsample_trilinear","ref":"/flux/stable/models/nnlib/#NNlib.upsample_trilinear","content":" NNlib.upsample_trilinear  —  Function upsample_trilinear(x::AbstractArray{T,5}, scale::NTuple{3,Real}; align_corners::Bool = true)\nupsample_trilinear(x::AbstractArray{T,5}; size::NTuple{3,Integer}, align_corners::Bool = true) Upsamples the first 3 dimensions of the array  x  by the upsample factors stored in  scale , using trilinear interpolation. As an alternative to using  scale , the resulting image  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale[1]*S1, scale[2]*S2, scale[3]*S3, S4, S5) , where  S1, S2, S3, S4, S5 = size(x) . Examples upsample_trilinear(x, (2, 3, 4))\nupsample_trilinear(x; size=(4, 9, 11))  # specify ouput size instead\nupsample_trilinear(x, (2.5, 3.5, pi))  # non-integer scaling factors are allowed"},{"id":287,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.∇upsample_trilinear","ref":"/flux/stable/models/nnlib/#NNlib.∇upsample_trilinear","content":" NNlib.∇upsample_trilinear  —  Function ∇upsample_trilinear(Δ::AbstractArray{T,5}; size::NTuple{3,Integer}, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Lateral size & depth (W,H,D) of the image upsampled in the first place Outputs dx : Downsampled version of  Δ"},{"id":288,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.pixel_shuffle","ref":"/flux/stable/models/nnlib/#NNlib.pixel_shuffle","content":" NNlib.pixel_shuffle  —  Function pixel_shuffle(x, r::Integer) Pixel shuffling operation, upscaling by a factor  r . For 4-arrays representing  N  images, the operation converts input  size(x) == (W, H, r^2*C, N)  to output of size  (r*W, r*H, C, N) . For  D -dimensional data, it expects  ndims(x) == D+2  with channel and batch dimensions, and divides the number of channels by  r^D . Used in super-resolution networks to upsample towards high resolution features. Reference: Shi et. al., \"Real-Time Single Image and Video Super-Resolution ...\", CVPR 2016, https://arxiv.org/abs/1609.05158 Examples julia> x = [10i + j + channel/10 for i in 1:2, j in 1:3, channel in 1:4, batch in 1:1]\n2×3×4×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  12.1  13.1\n 21.1  22.1  23.1\n\n[:, :, 2, 1] =\n 11.2  12.2  13.2\n 21.2  22.2  23.2\n\n[:, :, 3, 1] =\n 11.3  12.3  13.3\n 21.3  22.3  23.3\n\n[:, :, 4, 1] =\n 11.4  12.4  13.4\n 21.4  22.4  23.4\n\njulia> pixel_shuffle(x, 2)  # 4 channels used up as 2x upscaling of image dimensions\n4×6×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  11.3  12.1  12.3  13.1  13.3\n 11.2  11.4  12.2  12.4  13.2  13.4\n 21.1  21.3  22.1  22.3  23.1  23.3\n 21.2  21.4  22.2  22.4  23.2  23.4\n\njulia> y = [i + channel/10 for i in 1:3, channel in 1:6, batch in 1:1]\n3×6×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.2  1.3  1.4  1.5  1.6\n 2.1  2.2  2.3  2.4  2.5  2.6\n 3.1  3.2  3.3  3.4  3.5  3.6\n\njulia> pixel_shuffle(y, 2)  # 1D image, with 6 channels reduced to 3\n6×3×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.3  1.5\n 1.2  1.4  1.6\n 2.1  2.3  2.5\n 2.2  2.4  2.6\n 3.1  3.3  3.5\n 3.2  3.4  3.6"},{"id":289,"pagetitle":"Low-level Operations – NNlib.jl","title":"Batched Operations","ref":"/flux/stable/models/nnlib/#Batched-Operations","content":" Batched Operations Flux 's  Flux.Bilinear  layer uses  NNlib.batched_mul  internally."},{"id":290,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.batched_mul","ref":"/flux/stable/models/nnlib/#NNlib.batched_mul","content":" NNlib.batched_mul  —  Function batched_mul(A, B) -> C\nA ⊠ B  # \\boxtimes Batched matrix multiplication. Result has  C[:,:,k...] == A[:,:,k...] * B[:,:,k...]  where  k...  represent  any indices in the last dimensions. If  ndims(A) == ndims(B) == 3  and  size(B,3) == 1  then instead  C[:,:,k] == A[:,:,k] * B[:,:,1] , and similarly for  A . To transpose each matrix, apply  batched_transpose  to the array, or  batched_adjoint  for conjugate-transpose: julia> A, B = randn(2,5,17), randn(5,9,17);\n\njulia> A ⊠ B |> size\n(2, 9, 17)\n\njulia> batched_adjoint(A) |> size\n(5, 2, 17)\n\njulia> batched_mul(A, batched_adjoint(randn(9,5,17))) |> size\n(2, 9, 17)\n\njulia> A ⊠ randn(5,9,1) |> size\n(2, 9, 17)\n\njulia> batched_transpose(A) == PermutedDimsArray(A, (2,1,3))\ntrue The equivalent  PermutedDimsArray  may be used in place of  batched_transpose . Other permutations are also handled by BLAS, provided that the batch index  k  is not the first dimension of the underlying array. Thus  PermutedDimsArray(::Array, (1,3,2))  and  PermutedDimsArray(::Array, (3,1,2))  are fine. However,  A = PermutedDimsArray(::Array, (3,2,1))  is not acceptable to BLAS, since the batch dimension is the contiguous one:  stride(A,3) == 1 . This will be copied, as doing so is faster than  batched_mul_generic! . Both this  copy  and  batched_mul_generic!  produce  @debug  messages, and setting for instance  ENV[\"JULIA_DEBUG\"] = NNlib  will display them. batched_mul(A::Array{T,3}, B::Matrix)\nbatched_mul(A::Matrix, B::Array{T,3})\nA ⊠ B This is always matrix-matrix multiplication, but either  A  or  B  may lack a batch index. When  B  is a matrix, result has  C[:,:,k] == A[:,:,k] * B[:,:]  for all  k . When  A  is a matrix, then  C[:,:,k] == A[:,:] * B[:,:,k] . This can also be done by reshaping and calling  * , for instance  A ⊡ B  using TensorCore.jl, but is implemented here using  batched_gemm  instead of  gemm . julia> randn(16,8,32) ⊠ randn(8,4) |> size\n(16, 4, 32)\n\njulia> randn(16,8,32) ⊠ randn(8,4,1) |> size  # equivalent\n(16, 4, 32)\n\njulia> randn(16,8) ⊠ randn(8,4,32) |> size\n(16, 4, 32) See also  batched_vec  to regard  B  as a batch of vectors,  A[:,:,k] * B[:,k] ."},{"id":291,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.batched_mul!","ref":"/flux/stable/models/nnlib/#NNlib.batched_mul!","content":" NNlib.batched_mul!  —  Function batched_mul!(C, A, B) -> C\nbatched_mul!(C, A, B, α=1, β=0) In-place batched matrix multiplication, equivalent to  mul!(C[:,:,k], A[:,:,k], B[:,:,k], α, β)  for all  k . If  size(B,3) == 1  then every batch uses  B[:,:,1]  instead. This will call  batched_gemm!  whenever possible. For real arrays this means that, for  X ∈ [A,B,C] , either  strides(X,1)==1  or  strides(X,2)==1 , the latter may be caused by  batched_transpose  or by for instance  PermutedDimsArray(::Array, (3,1,2)) . Unlike  batched_mul  this will never make a copy. For complex arrays, the wrapper made by  batched_adjoint  must be outermost to be seen. In this case the strided accepted by BLAS are more restricted, if  stride(C,1)==1  then only  stride(AorB::BatchedAdjoint,2) == 1  is accepted."},{"id":292,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.batched_adjoint","ref":"/flux/stable/models/nnlib/#NNlib.batched_adjoint","content":" NNlib.batched_adjoint  —  Function batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A) Equivalent to applying  transpose  or  adjoint  to each matrix  A[:,:,k] . These exist to control how  batched_mul  behaves, as it operates on such matrix slices of an array with  ndims(A)==3 . PermutedDimsArray(A, (2,1,3))  is equivalent to  batched_transpose(A) , and is also understood by  batched_mul  (and more widely supported elsewhere). BatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S} Lazy wrappers analogous to  Transpose  and  Adjoint , returned by  batched_transpose  etc."},{"id":293,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.batched_transpose","ref":"/flux/stable/models/nnlib/#NNlib.batched_transpose","content":" NNlib.batched_transpose  —  Function batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A) Equivalent to applying  transpose  or  adjoint  to each matrix  A[:,:,k] . These exist to control how  batched_mul  behaves, as it operates on such matrix slices of an array with  ndims(A)==3 . PermutedDimsArray(A, (2,1,3))  is equivalent to  batched_transpose(A) , and is also understood by  batched_mul  (and more widely supported elsewhere). BatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S} Lazy wrappers analogous to  Transpose  and  Adjoint , returned by  batched_transpose  etc."},{"id":294,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.batched_vec","ref":"/flux/stable/models/nnlib/#NNlib.batched_vec","content":" NNlib.batched_vec  —  Function batched_vec(A::Array{T,3}, B::Matrix)\nbatched_vec(A::Array{T,3}, b::Vector) Batched matrix-vector multiplication: the result has  C[:,:,k] == A[:,:,k] * B[:,k]  for all  k , or else  C[:,:,k] == A[:,:,k] * b  for  b::Vector . With the same argument types,  batched_mul(A, B)  would regard  B  as a fixed matrix, not a batch of vectors. Both reshape and then call  batched_mul(::Array{T,3}, ::Array{T,3}) . julia> A, B, b = randn(16,8,32), randn(8,32), randn(8);\n\njulia> batched_vec(A,B) |> size\n(16, 32)\n\njulia> batched_vec(A,b) |> size\n(16, 32)"},{"id":295,"pagetitle":"Low-level Operations – NNlib.jl","title":"Gather and Scatter","ref":"/flux/stable/models/nnlib/#Gather-and-Scatter","content":" Gather and Scatter Flux 's  Embedding  layer uses  NNlib.gather  as its backend."},{"id":296,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.gather","ref":"/flux/stable/models/nnlib/#NNlib.gather","content":" NNlib.gather  —  Function NNlib.gather(src, idx) -> dst Reverse operation of  scatter . Gathers data from source  src  and writes it in a destination  dst  according to the index array  idx . For each  k  in  CartesianIndices(idx) , assign values to  dst  according to dst[:, ... , k] .= src[:, ... , idx[k]...] Notice that if  idx  is a vector containing integers and  src  is a matrix, previous expression simplifies to dst[:, k] .= src[:, idx[k]] and  k  will run over  1:length(idx) . The elements of  idx  can be integers or integer tuples and may be repeated. A single  src  column can end up being copied into zero, one, or multiple  dst  columns. See  gather!  for an in-place version. Examples julia> NNlib.gather([1,20,300,4000], [2,4,2])\n3-element Vector{Int64}:\n   20\n 4000\n   20\n\njulia> NNlib.gather([1 2 3; 4 5 6], [1,3,1,3,1])\n2×5 Matrix{Int64}:\n 1  3  1  3  1\n 4  6  4  6  4 gather(src, IJK...) Convert the tuple of integer vectors  IJK  to a tuple of  CartesianIndex  and call  gather  on it:  gather(src, CartesianIndex.(IJK...)) . Examples julia> src = reshape([1:15;], 3, 5)\n3×5 Matrix{Int64}:\n 1  4  7  10  13\n 2  5  8  11  14\n 3  6  9  12  15\n\njulia> NNlib.gather(src, [1, 2], [2, 4])\n2-element Vector{Int64}:\n  4\n 11"},{"id":297,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.gather!","ref":"/flux/stable/models/nnlib/#NNlib.gather!","content":" NNlib.gather!  —  Function NNlib.gather!(dst, src, idx) Reverse operation of  scatter! . Gathers data from source  src  and writes it in destination  dst  according to the index array  idx . For each  k  in  CartesianIndices(idx) , assign values to  dst  according to dst[:, ... , k] .= src[:, ... , idx[k]...] Notice that if  idx  is a vector containing integers, and both  dst  and  src  are matrices, previous expression simplifies to dst[:, k] .= src[:, idx[k]] and  k  will run over  1:length(idx) . The elements of  idx  can be integers or integer tuples and may be repeated. A single  src  column can end up being copied into zero, one, or multiple  dst  columns. See  gather  for an allocating version."},{"id":298,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.scatter","ref":"/flux/stable/models/nnlib/#NNlib.scatter","content":" NNlib.scatter  —  Function NNlib.scatter(op, src, idx; [init, dstsize]) Scatter operation allocating a destination array  dst  and calling  scatter!(op, dst, src, idx)  on it. If keyword  init  is provided, it is used to initialize the content of  dst . Otherwise, the init values is inferred from the reduction operator  op  for some common operators (e.g.  init = 0  for  op = + ). If  dstsize  is provided, it will be used to define the size of destination array, otherwise it will be inferred by  src  and  idx . See  scatter!  for full details on how  idx  works. Examples julia> NNlib.scatter(+, [10,100,1000], [3,1,2])\n3-element Vector{Int64}:\n  100\n 1000\n   10\n\njulia> NNlib.scatter(+, [1 2 3 4; 5 6 7 8], [2,1,1,5])\n2×5 Matrix{Int64}:\n  5  1  0  0  4\n 13  5  0  0  8\n\njulia> NNlib.scatter(*, [10,200,3000], [1,4,2]; init = 10, dstsize = 6)\n6-element Vector{Int64}:\n   100\n 30000\n    10\n  2000\n    10\n    10"},{"id":299,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.scatter!","ref":"/flux/stable/models/nnlib/#NNlib.scatter!","content":" NNlib.scatter!  —  Function NNlib.scatter!(op, dst, src, idx) Scatter operation, which writes data in  src  into  dst  at locations  idx . A binary reduction operator  op  is applied during the scatter. For each index  k  in  idx , accumulates values in  dst  according to dst[:, ..., idx[k]...] = (op).(dst[:, ..., idx[k]...], src[:, ..., k...]) See also  scatter ,  gather . Arguments op : Operations to be applied on  dst  and  src , e.g.  + ,  - ,  * ,  / ,  max ,  min  and  mean . dst : The destination for  src  to aggregate to. This argument will be mutated. src : The source data for aggregating. idx : The mapping for aggregation from source (index) to destination (value).        The  idx  array can contain either integers or tuples. Examples julia> NNlib.scatter!(+, ones(3), [10,100], [1,3])\n3-element Vector{Float64}:\n  11.0\n   1.0\n 101.0\n\njulia> NNlib.scatter!(*, fill(0.5, 2, 4), [1 10; 100 1000], [3,2])\n2×4 Matrix{Float64}:\n 0.5    5.0   0.5  0.5\n 0.5  500.0  50.0  0.5"},{"id":300,"pagetitle":"Low-level Operations – NNlib.jl","title":"Sampling","ref":"/flux/stable/models/nnlib/#Sampling","content":" Sampling"},{"id":301,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.grid_sample","ref":"/flux/stable/models/nnlib/#NNlib.grid_sample","content":" NNlib.grid_sample  —  Function grid_sample(input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros) Given  input , compute output by sampling  input  values at pixel locations from  grid . Uses bilinear interpolation to calculate output values. This implementation assumes the extrema ( -1  and  1 ) are considered as referring to the center points of the input’s corner pixels (i.e. align corners is  true ). Arguments input : Input array in  (W_in, H_in, C, N)  shape. grid : Input grid in  (2, W_out, H_out, N)  shape.   Where for each  (W_out, H_out, N)  grid contains  (x, y)    coordinates that specify sampling locations normalized by the  input  shape. Therefore,  x  and  y  should have values in  [-1, 1]  range.   For example,  (x = -1, y = -1)  is the left-top pixel of  input ,   and  (x = 1, y = 1)  is the right-bottom pixel of  input . Out-of-bound values are handled according to the  padding_mode . padding_mode : Out-of-bound padding.    :zeros  to use  0  for out-of-bound grid locations.    :border  to use border values for out-of-bound grid locations.   Default is  :zeros . Returns (W_out, H_out, C, N)  sampled grid from  input . Examples In the example below, grid contains two out-of-bound sampling locations, which are handled differently, depending on the  padding_mode . julia> x = reshape(collect(1.0:4.0), (2, 2, 1, 1))\n2×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 2.0  4.0\n\njulia> grid = Array{Float64}(undef, 2, 3, 2, 1);\n\njulia> grid[:, 1, 1, 1] .= (-3, -1);\n\njulia> grid[:, 2, 1, 1] .= (0, -1);\n\njulia> grid[:, 3, 1, 1] .= (1, -1);\n\njulia> grid[:, 1, 2, 1] .= (-1, 1);\n\njulia> grid[:, 2, 2, 1] .= (0, 1);\n\njulia> grid[:, 3, 2, 1] .= (3, 1);\n\njulia> grid_sample(x, grid; padding_mode=:zeros)\n3×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 0.0  3.0\n 1.5  3.5\n 2.0  0.0\n\njulia> grid_sample(x, grid; padding_mode=:border)\n3×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 1.5  3.5\n 2.0  4.0"},{"id":302,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.∇grid_sample","ref":"/flux/stable/models/nnlib/#NNlib.∇grid_sample","content":" NNlib.∇grid_sample  —  Function ∇grid_sample(Δ::AbstractArray{T, 4}, input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros) where T Arguments Δ : Input gradient in  (W_out, H_out, C, N)  shape   (same as output of the primal computation). input : Input from primal computation in  (W_in, H_in, C, N)  shape. grid : Grid from primal computation in  (2, W_out, H_out, N)  shape. padding_mode : Out-of-bound padding.    :zeros  to use  0  for out-of-bound grid locations.    :border  to use border values for out-of-bound grid locations.   Should be the same as in primal computation.   Default is  :zeros . Returns dinput  (same shape as  input ) and  dgrid  (same shape as  grid ) gradients."},{"id":303,"pagetitle":"Low-level Operations – NNlib.jl","title":"Losses","ref":"/flux/stable/models/nnlib/#Losses","content":" Losses"},{"id":304,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.ctc_loss","ref":"/flux/stable/models/nnlib/#NNlib.ctc_loss","content":" NNlib.ctc_loss  —  Function ctc_loss(ŷ, y) Computes the connectionist temporal classification loss between  ŷ  and  y .  ŷ  must be a classes-by-time matrices, i.e., each row represents a class and each column represents a time step. Additionally, the  logsoftmax  function will be applied to  ŷ , so  ŷ  must be the raw activation values from the neural network and not, for example, the activations after being passed through a  softmax  activation function.  y  must be a 1D array of the labels associated with  ŷ . The blank label is assumed to be the last label category in  ŷ , so it is equivalent to  size(ŷ, 1) . Used for sequence-to-sequence classification problems such as speech recognition and handwriting recognition where the exact time-alignment of the output (e.g., letters) is not needed to solve the problem. See  Graves et al. (2006)  or  Graves (2012)  for mathematical details."},{"id":305,"pagetitle":"Low-level Operations – NNlib.jl","title":"Miscellaneous","ref":"/flux/stable/models/nnlib/#Miscellaneous","content":" Miscellaneous"},{"id":306,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.logsumexp","ref":"/flux/stable/models/nnlib/#NNlib.logsumexp","content":" NNlib.logsumexp  —  Function logsumexp(x; dims = :) Computes  log.(sum(exp.(x); dims))  in a numerically stable way. Without  dims  keyword this returns a scalar. See also  logsoftmax ."},{"id":307,"pagetitle":"Low-level Operations – NNlib.jl","title":"NNlib.glu","ref":"/flux/stable/models/nnlib/#NNlib.glu","content":" NNlib.glu  —  Function glu(x, dim = 1) The gated linear unit from the  \"Language Modeling with Gated Convolutional Networks\"  paper. Calculates  a .* sigmoid(b) , where  x  is split in half along given dimension  dim  to form  a  and  b ."},{"id":310,"pagetitle":"Fitting a Line","title":"Flux Overview: Fitting a Straight Line","ref":"/flux/stable/models/overview/#man-overview","content":" Flux Overview: Fitting a Straight Line Flux is a pure Julia ML stack that allows you to build predictive models. Here are the steps for a typical Flux program: Provide training and test data Build a model with configurable  parameters  to make predictions Iteratively train the model by tweaking the parameters to improve predictions Verify your model Under the hood, Flux uses a technique called automatic differentiation to take gradients that help improve predictions. Flux is also fully written in Julia so you can easily replace any layer of Flux with your own code to improve your understanding or satisfy special requirements. Here's how you'd use Flux to build and train the most basic of models, step by step."},{"id":311,"pagetitle":"Fitting a Line","title":"A Trivial Prediction","ref":"/flux/stable/models/overview/#A-Trivial-Prediction","content":" A Trivial Prediction This example will predict the output of the function  4x + 2 . Making such predictions is called \"linear regression\", and is really too simple to  need  a neural network. But it's a nice toy example. First, import  Flux  and define the function we want to simulate: julia> using Flux\n\njulia> actual(x) = 4x + 2\nactual (generic function with 1 method) This example will build a model to approximate the  actual  function."},{"id":312,"pagetitle":"Fitting a Line","title":"1. Provide Training and Test Data","ref":"/flux/stable/models/overview/#.-Provide-Training-and-Test-Data","content":" 1. Provide Training and Test Data Use the  actual  function to build sets of data for training and verification: julia> x_train, x_test = hcat(0:5...), hcat(6:10...)\n([0 1 … 4 5], [6 7 … 9 10])\n\njulia> y_train, y_test = actual.(x_train), actual.(x_test)\n([2 6 … 18 22], [26 30 … 38 42]) Normally, your training and test data come from real world observations, but here we simulate them."},{"id":313,"pagetitle":"Fitting a Line","title":"2. Build a Model to Make Predictions","ref":"/flux/stable/models/overview/#.-Build-a-Model-to-Make-Predictions","content":" 2. Build a Model to Make Predictions Now, build a model to make predictions with  1  input and  1  output: julia> model = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters\n\njulia> model.weight\n1×1 Matrix{Float32}:\n 0.95041317\n\njulia> model.bias\n1-element Vector{Float32}:\n 0.0 Under the hood, a dense layer is a struct with fields  weight  and  bias .  weight  represents a weights' matrix and  bias  represents a bias vector. There's another way to think about a model. In Flux,  models are conceptually predictive functions :  julia> predict = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters Dense(1 => 1)  also implements the function  σ(Wx+b)  where  W  and  b  are the weights and biases.  σ  is an activation function (more on activations later). Our model has one weight and one bias, but typical models will have many more. Think of weights and biases as knobs and levers Flux can use to tune predictions. Activation functions are transformations that tailor models to your needs.  This model will already make predictions, though not accurate ones yet: julia> predict(x_train)\n1×6 Matrix{Float32}:\n 0.0  0.906654  1.81331  2.71996  3.62662  4.53327 In order to make better predictions, you'll need to provide a  loss function  to tell Flux how to objectively  evaluate  the quality of a prediction. Loss functions compute the cumulative distance between actual values and predictions.  julia> using Statistics\n\njulia> loss(model, x, y) = mean(abs2.(model(x) .- y));\n\njulia> loss(predict, x_train, y_train)\n122.64734f0 More accurate predictions will yield a lower loss. You can write your own loss functions or rely on those already provided by Flux. This loss function is called  mean squared error  (and built-in as  mse ). Flux works by iteratively reducing the loss through  training ."},{"id":314,"pagetitle":"Fitting a Line","title":"3. Improve the Prediction","ref":"/flux/stable/models/overview/#.-Improve-the-Prediction","content":" 3. Improve the Prediction Under the hood, the Flux  Flux.train!  function uses  a loss function  and  training data  to improve the  parameters  of your model based on a pluggable  optimiser : julia> using Flux: train!\n\njulia> opt = Descent()\nDescent(0.1)\n\njulia> data = [(x_train, y_train)]\n1-element Vector{Tuple{Matrix{Int64}, Matrix{Int64}}}:\n ([0 1 … 4 5], [2 6 … 18 22]) Now, we have the optimiser and data we'll pass to  train! . All that remains are the parameters of the model. Remember, each model is a Julia struct with a function and configurable parameters. Remember, the dense layer has weights and biases that depend on the dimensions of the inputs and outputs:  julia> predict.weight\n1×1 Matrix{Float32}:\n 0.9066542\n\njulia> predict.bias\n1-element Vector{Float32}:\n 0.0 The dimensions of these model parameters depend on the number of inputs and outputs. Flux will adjust predictions by iteratively changing these parameters according to the optimiser. This optimiser implements the classic gradient descent strategy. Now improve the parameters of the model with a call to  Flux.train!  like this: julia> train!(loss, predict, data, opt) And check the loss: julia> loss(predict, x_train, y_train)\n116.38745f0 It went down. Why?  julia> predict.weight, predict.bias\n(Float32[7.246838;;], Float32[1.748103]) The parameters have changed. This single step is the essence of machine learning."},{"id":315,"pagetitle":"Fitting a Line","title":"3+. Iteratively Train the Model","ref":"/flux/stable/models/overview/#.-Iteratively-Train-the-Model","content":" 3+. Iteratively Train the Model In the previous section, we made a single call to  train!  which iterates over the data we passed in just once. An  epoch  refers to one pass over the dataset. Typically, we will run the training for multiple epochs to drive the loss down even further. Let's run it a few more times: julia> for epoch in 1:200\n         train!(loss, predict, data, opt)\n       end\n\njulia> loss(predict, x_train, y_train)\n0.00339581f0\n\njulia> predict.weight, predict.bias\n(Float32[4.0159144;;], Float32[2.004479]) After 200 training steps, the loss went down, and the parameters are getting close to those in the function the model is built to predict."},{"id":316,"pagetitle":"Fitting a Line","title":"4. Verify the Results","ref":"/flux/stable/models/overview/#.-Verify-the-Results","content":" 4. Verify the Results Now, let's verify the predictions: julia> predict(x_test)\n1×5 Matrix{Float32}:\n 26.1121  30.13  34.1479  38.1657  42.1836\n\njulia> y_test\n1×5 Matrix{Int64}:\n 26  30  34  38  42 The predictions are good. Here's how we got there.  First, we gathered real-world data into the variables  x_train ,  y_train ,  x_test , and  y_test . The  x_*  data defines inputs, and the  y_*  data defines outputs. The  *_train  data is for training the model, and the  *_test  data is for verifying the model. Our data was based on the function  4x + 2 . Then, we built a single input, single output predictive model,  predict = Dense(1 => 1) . The initial predictions weren't accurate, because we had not trained the model yet. After building the model, we trained it with  train!(loss, predict, data, opt) . The loss function is first, followed by the model itself, the training data, and the  Descent  optimiser provided by Flux. We ran the training step once, and observed that the parameters changed and the loss went down. Then, we ran the  train!  many times to finish the training process. After we trained the model, we verified it with the test data to verify the results.  This overall flow represents how Flux works. Let's drill down a bit to understand what's going on inside the individual layers of Flux."},{"id":319,"pagetitle":"Quick Start","title":"A Neural Network in One Minute","ref":"/flux/stable/models/quickstart/#man-quickstart","content":" A Neural Network in One Minute If you have used neural networks before, then this simple example might be helpful for seeing how the major parts of Flux work together. Try pasting the code into the REPL prompt. If you haven't, then you might prefer the  Fitting a Straight Line  page. # This will prompt if neccessary to install everything, including CUDA:\nusing Flux, CUDA, Statistics, ProgressMeter\n\n# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:\nnoisy = rand(Float32, 2, 1000)                                    # 2×1000 Matrix{Float32}\ntruth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]   # 1000-element Vector{Bool}\n\n# Define our model, a multi-layer perceptron with one hidden layer of size 3:\nmodel = Chain(\n    Dense(2 => 3, tanh),   # activation function inside layer\n    BatchNorm(3),\n    Dense(3 => 2),\n    softmax) |> gpu        # move model to GPU, if available\n\n# The model encapsulates parameters, randomly initialised. Its initial output is:\nout1 = model(noisy |> gpu) |> cpu                                 # 2×1000 Matrix{Float32}\n\n# To train the model, we use batches of 64 samples, and one-hot encoding:\ntarget = Flux.onehotbatch(truth, [true, false])                   # 2×1000 OneHotMatrix\nloader = Flux.DataLoader((noisy, target) |> gpu, batchsize=64, shuffle=true);\n# 16-element DataLoader with first element: (2×64 Matrix{Float32}, 2×64 OneHotMatrix)\n\noptim = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc.\n\n# Training loop, using the whole data set 1000 times:\nlosses = []\n@showprogress for epoch in 1:1_000\n    for (x, y) in loader\n        loss, grads = Flux.withgradient(model) do m\n            # Evaluate model and loss inside gradient context:\n            y_hat = m(x)\n            Flux.crossentropy(y_hat, y)\n        end\n        Flux.update!(optim, model, grads[1])\n        push!(losses, loss)  # logging, outside gradient context\n    end\nend\n\noptim # parameters, momenta and output have all changed\nout2 = model(noisy |> gpu) |> cpu  # first row is prob. of true, second row p(false)\n\nmean((out2[1,:] .> 0.5) .== truth)  # accuracy 94% so far! using Plots  # to draw the above figure\n\np_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=\"True classification\", legend=false)\np_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=out1[1,:], title=\"Untrained network\", label=\"\", clims=(0,1))\np_done = scatter(noisy[1,:], noisy[2,:], zcolor=out2[1,:], title=\"Trained network\", legend=false)\n\nplot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330)) Here's the loss during training: plot(losses; xaxis=(:log10, \"iteration\"),\n    yaxis=\"loss\", label=\"per batch\")\nn = length(loader)\nplot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n    label=\"epoch mean\", dpi=200) This XOR (\"exclusive or\") problem is a variant of the famous one which drove Minsky and Papert to invent deep neural networks in 1969. For small values of \"deep\" – this has one hidden layer, while earlier perceptrons had none. (What they call a hidden layer, Flux calls the output of the first layer,  model[1](noisy) .) Since then things have developed a little. "},{"id":320,"pagetitle":"Quick Start","title":"Features to Note","ref":"/flux/stable/models/quickstart/#Features-to-Note","content":" Features to Note Some things to notice in this example are: The batch dimension of data is always the last one. Thus a  2×1000 Matrix  is a thousand observations, each a column of length 2. Flux defaults to  Float32 , but most of Julia to  Float64 . The  model  can be called like a function,  y = model(x) . Each layer like  Dense  is an ordinary  struct , which encapsulates some arrays of parameters (and possibly other state, as for  BatchNorm ). But the model does not contain the loss function, nor the optimisation rule. The momenta needed by  Adam  are stored in the object returned by  setup . And  Flux.crossentropy  is an ordinary function. The  do  block creates an anonymous function, as the first argument of  gradient . Anything executed within this is differentiated. Instead of calling  gradient  and  update!  separately, there is a convenience function  train! . If we didn't want anything extra (like logging the loss), we could replace the training loop with the following: for epoch in 1:1_000\n    Flux.train!(model, loader, optim) do m, x, y\n        y_hat = m(x)\n        Flux.crossentropy(y_hat, y)\n    end\nend Implicit-style training, Flux ≤ 0.14 Until recently Flux's training worked a bit differently.  Any code which looks like  gradient(() -> loss(model, x, y), Flux.params(model)) (gradient of a zero-argument function) or train!((x,y) -> loss(model, x, y), Flux.params(model), loader, opt) (with  Flux.params ) is in the old \"implicit\" style. This still works on Flux 0.14, but will be removed from Flux 0.15. See the  training section  for more details."},{"id":323,"pagetitle":"Recurrence","title":"Recurrent Models","ref":"/flux/stable/models/recurrence/#Recurrent-Models","content":" Recurrent Models"},{"id":324,"pagetitle":"Recurrence","title":"Recurrent cells","ref":"/flux/stable/models/recurrence/#Recurrent-cells","content":" Recurrent cells To introduce Flux's recurrence functionalities, we will consider the following vanilla recurrent neural network structure: In the above, we have a sequence of length 3, where  x1  to  x3  represent the input at each step (could be a timestamp or a word in a sentence), and  y1  to  y3  are their respective outputs. An aspect to recognize is that in such a model, the recurrent cells  A  all refer to the same structure. What distinguishes it from a simple dense layer is that the cell  A  is fed, in addition to an input  x , with information from the previous state of the model (hidden state denoted as  h1  &  h2  in the diagram). In the most basic RNN case, cell A could be defined by the following:  output_size = 5\ninput_size = 2\nWxh = randn(Float32, output_size, input_size)\nWhh = randn(Float32, output_size, output_size)\nb   = randn(Float32, output_size)\n\nfunction rnn_cell(h, x)\n    h = tanh.(Wxh * x .+ Whh * h .+ b)\n    return h, h\nend\n\nx = rand(Float32, input_size) # dummy input data\nh = rand(Float32, output_size) # random initial hidden state\n\nh, y = rnn_cell(h, x) Notice how the above is essentially a  Dense  layer that acts on two inputs,  h  and  x . If you run the last line a few times, you'll notice the output  y  changing slightly even though the input  x  is the same. There are various recurrent cells available in Flux, notably  RNNCell ,  LSTMCell  and  GRUCell , which are documented in the  layer reference . The hand-written example above can be replaced with: using Flux\n\nrnn = Flux.RNNCell(2, 5)\n\nx = rand(Float32, 2) # dummy data\nh = rand(Float32, 5)  # initial hidden state\n\nh, y = rnn(h, x)"},{"id":325,"pagetitle":"Recurrence","title":"Stateful Models","ref":"/flux/stable/models/recurrence/#Stateful-Models","content":" Stateful Models For the most part, we don't want to manage hidden states ourselves, but to treat our models as being stateful. Flux provides the  Recur  wrapper to do this. x = rand(Float32, 2)\nh = rand(Float32, 5)\n\nm = Flux.Recur(rnn, h)\n\ny = m(x) The  Recur  wrapper stores the state between runs in the  m.state  field. If we use the  RNN(2, 5)  constructor – as opposed to  RNNCell  – you'll see that it's simply a wrapped cell. julia> using Flux\n\njulia> RNN(2, 5)  # or equivalently RNN(2 => 5)\nRecur(\n  RNNCell(2 => 5, tanh),                # 45 parameters\n)         # Total: 4 trainable arrays, 45 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 412 bytes. Equivalent to the  RNN  stateful constructor,  LSTM  and  GRU  are also available.  Using these tools, we can now build the model shown in the above diagram with:  julia> m = Chain(RNN(2 => 5), Dense(5 => 1))\nChain(\n  Recur(\n    RNNCell(2 => 5, tanh),              # 45 parameters\n  ),\n  Dense(5 => 1),                        # 6 parameters\n)         # Total: 6 trainable arrays, 51 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 580 bytes.    In this example, each output has only one component."},{"id":326,"pagetitle":"Recurrence","title":"Working with sequences","ref":"/flux/stable/models/recurrence/#Working-with-sequences","content":" Working with sequences Using the previously defined  m  recurrent model, we can now apply it to a single step from our sequence: julia> x = rand(Float32, 2);\n\njulia> m(x)\n1-element Vector{Float32}:\n 0.45860028 The  m(x)  operation would be represented by  x1 -> A -> y1  in our diagram. If we perform this operation a second time, it will be equivalent to  x2 -> A -> y2   since the model  m  has stored the state resulting from the  x1  step. Now, instead of computing a single step at a time, we can get the full  y1  to  y3  sequence in a single pass by  iterating the model on a sequence of data.  To do so, we'll need to structure the input data as a  Vector  of observations at each time step. This  Vector  will therefore be of  length = seq_length  and each of its elements will represent the input features for a given step. In our example, this translates into a  Vector  of length 3, where each element is a  Matrix  of size  (features, batch_size) , or just a  Vector  of length  features  if dealing with a single observation.   julia> x = [rand(Float32, 2) for i = 1:3];\n\njulia> [m(xi) for xi in x]\n3-element Vector{Vector{Float32}}:\n [0.36080405]\n [-0.13914406]\n [0.9310162] Use of map and broadcast Mapping and broadcasting operations with stateful layers such are discouraged, since the julia language doesn't guarantee a specific execution order. Therefore, avoid   y = m.(x)\n# or \ny = map(m, x) and use explicit loops  y = [m(x) for x in x] If for some reason one wants to exclude the first step of the RNN chain for the computation of the loss, that can be handled with: using Flux.Losses: mse\n\nfunction loss(x, y)\n  m(x[1]) # ignores the output but updates the hidden states\n  sum(mse(m(xi), yi) for (xi, yi) in zip(x[2:end], y))\nend\n\ny = [rand(Float32, 1) for i=1:2]\nloss(x, y) In such a model, only the last two outputs are used to compute the loss, hence the target  y  being of length 2. This is a strategy that can be used to easily handle a  seq-to-one  kind of structure, compared to the  seq-to-seq  assumed so far.    Alternatively, if one wants to perform some warmup of the sequence, it could be performed once, followed with a regular training where all the steps of the sequence would be considered for the gradient update: function loss(x, y)\n  sum(mse(m(xi), yi) for (xi, yi) in zip(x, y))\nend\n\nseq_init = [rand(Float32, 2)]\nseq_1 = [rand(Float32, 2) for i = 1:3]\nseq_2 = [rand(Float32, 2) for i = 1:3]\n\ny1 = [rand(Float32, 1) for i = 1:3]\ny2 = [rand(Float32, 1) for i = 1:3]\n\nX = [seq_1, seq_2]\nY = [y1, y2]\ndata = zip(X,Y)\n\nFlux.reset!(m)\n[m(x) for x in seq_init]\n\nps = Flux.params(m)\nopt= Adam(1e-3)\nFlux.train!(loss, ps, data, opt) In this previous example, model's state is first reset with  Flux.reset! . Then, there's a warmup that is performed over a sequence of length 1 by feeding it with  seq_init , resulting in a warmup state. The model can then be trained for 1 epoch, where 2 batches are provided ( seq_1  and  seq_2 ) and all the timesteps outputs are considered for the loss. In this scenario, it is important to note that a single continuous sequence is considered. Since the model state is not reset between the 2 batches, the state of the model flows through the batches, which only makes sense in the context where  seq_1  is the continuation of  seq_init  and so on. Batch size would be 1 here as there's only a single sequence within each batch. If the model was to be trained on multiple independent sequences, then these sequences could be added to the input data as a second dimension. For example, in a language model, each batch would contain multiple independent sentences. In such scenario, if we set the batch size to 4, a single batch would be of the shape: x = [rand(Float32, 2, 4) for i = 1:3]\ny = [rand(Float32, 1, 4) for i = 1:3] That would mean that we have 4 sentences (or samples), each with 2 features (let's say a very small embedding!) and each with a length of 3 (3 words per sentence). Computing  m(batch[1]) , would still represent  x1 -> y1  in our diagram and returns the first word output, but now for each of the 4 independent sentences (second dimension of the input matrix). We do not need to use  Flux.reset!(m)  here; each sentence in the batch will output in its own \"column\", and the outputs of the different sentences won't mix.  To illustrate, we go through an example of batching with our implementation of  rnn_cell . The implementation doesn't need to change; the batching comes for \"free\" from the way Julia does broadcasting and the rules of matrix multiplication. output_size = 5\ninput_size = 2\nWxh = randn(Float32, output_size, input_size)\nWhh = randn(Float32, output_size, output_size)\nb   = randn(Float32, output_size)\n\nfunction rnn_cell(h, x)\n    h = tanh.(Wxh * x .+ Whh * h .+ b)\n    return h, h\nend Here, we use the last dimension of the input and the hidden state as the batch dimension. I.e.,  h[:, n]  would be the hidden state of the nth sentence in the batch. batch_size = 4\nx = rand(Float32, input_size, batch_size) # dummy input data\nh = rand(Float32, output_size, batch_size) # random initial hidden state\n\nh, y = rnn_cell(h, x) julia> size(h) == size(y) == (output_size, batch_size)\ntrue In many situations, such as when dealing with a language model, the sentences in each batch are independent (i.e. the last item of the first sentence of the first batch is independent from the first item of the first sentence of the second batch), so we cannot handle the model as if each batch was the direct continuation of the previous one. To handle such situations, we need to reset the state of the model between each batch, which can be conveniently performed within the loss function: function loss(x, y)\n  Flux.reset!(m)\n  sum(mse(m(xi), yi) for (xi, yi) in zip(x, y))\nend A potential source of ambiguity with RNN in Flux can come from the different data layout compared to some common frameworks where data is typically a 3 dimensional array:  (features, seq length, samples) . In Flux, those 3 dimensions are provided through a vector of seq length containing a matrix  (features, samples) ."},{"id":329,"pagetitle":"Shape Inference","title":"Shape Inference","ref":"/flux/stable/outputsize/#Shape-Inference","content":" Shape Inference Flux has some tools to help generate models in an automated fashion, by inferring the size of arrays that layers will recieve, without doing any computation.  This is especially useful for convolutional models, where the same  Conv  layer accepts any size of image, but the next layer may not.  The higher-level tool is a macro  @autosize  which acts on the code defining the layers, and replaces each appearance of  _  with the relevant size. This simple example returns a model with  Dense(845 => 10)  as the last layer: @autosize (28, 28, 1, 32) Chain(Conv((3, 3), _ => 5, relu, stride=2), Flux.flatten, Dense(_ => 10)) The input size may be provided at runtime, like  @autosize (sz..., 1, 32) Chain(Conv( ..., but all the layer constructors containing  _  must be explicitly written out – the macro sees the code as written. This macro relies on a lower-level function  outputsize , which you can also use directly: c = Conv((3, 3), 1 => 5, relu, stride=2)\nFlux.outputsize(c, (28, 28, 1, 32))  # returns (13, 13, 5, 32) The function  outputsize  works by passing a \"dummy\" array into the model, which propagates through very cheaply. It should work for all layers, including custom layers, out of the box. An example of how to automate model building is this: \"\"\"\n    make_model(width, height, [inchannels, nclasses; layer_config])\n\nCreate a CNN for a given set of configuration parameters. Arguments:\n- `width`, `height`: the input image size in pixels\n- `inchannels`: the number of channels in the input image, default `1`\n- `nclasses`: the number of output classes, default `10`\n- Keyword `layer_config`: a vector of the number of channels per layer, default `[16, 16, 32, 64]`\n\"\"\"\nfunction make_model(width, height, inchannels = 1, nclasses = 10;\n                    layer_config = [16, 16, 32, 64])\n  # construct a vector of layers:\n  conv_layers = []\n  push!(conv_layers, Conv((5, 5), inchannels => layer_config[1], relu, pad=SamePad()))\n  for (inch, outch) in zip(layer_config, layer_config[2:end])\n    push!(conv_layers, Conv((3, 3), inch => outch, sigmoid, stride=2))\n  end\n\n  # compute the output dimensions after these conv layers:\n  conv_outsize = Flux.outputsize(conv_layers, (width, height, inchannels); padbatch=true)\n\n  # use this to define appropriate Dense layer:\n  last_layer = Dense(prod(conv_outsize) => nclasses)\n  return Chain(conv_layers..., Flux.flatten, last_layer)\nend\n\nm = make_model(28, 28, 3, layer_config = [9, 17, 33, 65])\n\nFlux.outputsize(m, (28, 28, 3, 42)) == (10, 42) == size(m(randn(Float32, 28, 28, 3, 42))) Alternatively, using the macro, the definition of  make_model  could end with:   # compute the output dimensions & construct appropriate Dense layer:\n  return @autosize (width, height, inchannels, 1) Chain(conv_layers..., Flux.flatten, Dense(_ => nclasses))\nend"},{"id":330,"pagetitle":"Shape Inference","title":"Listing","ref":"/flux/stable/outputsize/#Listing","content":" Listing"},{"id":331,"pagetitle":"Shape Inference","title":"Flux.@autosize","ref":"/flux/stable/outputsize/#Flux.@autosize","content":" Flux.@autosize  —  Macro @autosize (size...,) Chain(Layer(_ => 2), Layer(_), ...) Returns the specified model, with each  _  replaced by an inferred number, for input of the given  size . The unknown sizes are usually the second-last dimension of that layer's input, which Flux regards as the channel dimension. (A few layers,  Dense  &  LayerNorm , instead always use the first dimension.) The underscore may appear as an argument of a layer, or inside a  => . It may be used in further calculations, such as  Dense(_ => _÷4) . Examples julia> @autosize (3, 1) Chain(Dense(_ => 2, sigmoid), BatchNorm(_, affine=false))\nChain(\n  Dense(3 => 2, σ),                     # 8 parameters\n  BatchNorm(2, affine=false),\n) \n\njulia> img = [28, 28];\n\njulia> @autosize (img..., 1, 32) Chain(              # size is only needed at runtime\n          Chain(c = Conv((3,3), _ => 5; stride=2, pad=SamePad()),\n                p = MeanPool((3,3)),\n                b = BatchNorm(_),\n                f = Flux.flatten),\n          Dense(_ => _÷4, relu, init=Flux.rand32),   # can calculate output size _÷4\n          SkipConnection(Dense(_ => _, relu), +),\n          Dense(_ => 10),\n       )\nChain(\n  Chain(\n    c = Conv((3, 3), 1 => 5, pad=1, stride=2),  # 50 parameters\n    p = MeanPool((3, 3)),\n    b = BatchNorm(5),                   # 10 parameters, plus 10\n    f = Flux.flatten,\n  ),\n  Dense(80 => 20, relu),                # 1_620 parameters\n  SkipConnection(\n    Dense(20 => 20, relu),              # 420 parameters\n    +,\n  ),\n  Dense(20 => 10),                      # 210 parameters\n)         # Total: 10 trainable arrays, 2_310 parameters,\n          # plus 2 non-trainable, 10 parameters, summarysize 10.469 KiB.\n\njulia> outputsize(ans, (28, 28, 1, 32))\n(10, 32) Limitations: While  @autosize (5, 32) Flux.Bilinear(_ => 7)  is OK, something like  Bilinear((_, _) => 7)  will fail. While  Scale(_)  and  LayerNorm(_)  are fine (and use the first dimension),  Scale(_,_)  and  LayerNorm(_,_)  will fail if  size(x,1) != size(x,2) . source"},{"id":332,"pagetitle":"Shape Inference","title":"Flux.outputsize","ref":"/flux/stable/outputsize/#Flux.outputsize","content":" Flux.outputsize  —  Function outputsize(m, x_size, y_size, ...; padbatch=false) For model or layer  m  accepting multiple arrays as input, this returns  size(m((x, y, ...)))  given  size_x = size(x) , etc. Examples julia> x, y = rand(Float32, 5, 64), rand(Float32, 7, 64);\n\njulia> par = Parallel(vcat, Dense(5 => 9), Dense(7 => 11));\n\njulia> Flux.outputsize(par, (5, 64), (7, 64))\n(20, 64)\n\njulia> m = Chain(par, Dense(20 => 13), softmax);\n\njulia> Flux.outputsize(m, (5,), (7,); padbatch=true)\n(13, 1)\n\njulia> par(x, y) == par((x, y)) == Chain(par, identity)((x, y))\ntrue Notice that  Chain  only accepts multiple arrays as a tuple, while  Parallel  also accepts them as multiple arguments;  outputsize  always supplies the tuple. source"},{"id":335,"pagetitle":"Performance Tips","title":"Performance Tips","ref":"/flux/stable/performance/#[Performance-Tips]((@id-man-performance-tips))","content":" Performance Tips All the usual  Julia performance tips apply . As always  profiling your code  is generally a useful way of finding bottlenecks. Below follow some Flux specific tips/reminders."},{"id":336,"pagetitle":"Performance Tips","title":"Don't use more precision than you need","ref":"/flux/stable/performance/#Don't-use-more-precision-than-you-need","content":" Don't use more precision than you need Flux works great with all kinds of number types. But often you do not need to be working with say  Float64  (let alone  BigFloat ). Switching to  Float32  can give you a significant speed up, not because the operations are faster, but because the memory usage is halved. Which means allocations occur much faster. And you use less memory."},{"id":337,"pagetitle":"Performance Tips","title":"Preserve inputs' types","ref":"/flux/stable/performance/#Preserve-inputs'-types","content":" Preserve inputs' types Not only should your activation and loss functions be  type-stable , they should also preserve the type of their inputs. A very artificial example using an activation function like my_tanh(x) = Float64(tanh(x)) will result in performance on  Float32  input orders of magnitude slower than the normal  tanh  would, because it results in having to use slow mixed type multiplication in the dense layers. Similar situations can occur in the loss function during backpropagation. Which means if you change your data say from  Float64  to  Float32  (which should give a speedup: see above), you will see a large slow-down. This can occur sneakily, because you can cause type-promotion by interacting with a numeric literals. E.g. the following will have run into the same problem as above: leaky_tanh(x) = 0.01*x + tanh(x) While one could change the activation function (e.g. to use  0.01f0*x ), the idiomatic (and safe way)  to avoid type casts whenever inputs changes is to use  oftype : leaky_tanh(x) = oftype(x/1, 0.01)*x + tanh(x)"},{"id":338,"pagetitle":"Performance Tips","title":"Evaluate batches as Matrices of features","ref":"/flux/stable/performance/#Evaluate-batches-as-Matrices-of-features","content":" Evaluate batches as Matrices of features While it can sometimes be tempting to process your observations (feature vectors) one at a time e.g. function loss_total(xs::AbstractVector{<:Vector}, ys::AbstractVector{<:Vector})\n    sum(zip(xs, ys)) do (x, y_target)\n        y_pred = model(x)  # evaluate the model\n        return loss(y_pred, y_target)\n    end\nend It is much faster to concatenate them into a matrix, as this will hit BLAS matrix-matrix multiplication, which is much faster than the equivalent sequence of matrix-vector multiplications. The improvement is enough that it is worthwhile allocating new memory to store them contiguously. x_batch = reduce(hcat, xs)\ny_batch = reduce(hcat, ys)\n...\nfunction loss_total(x_batch::Matrix, y_batch::Matrix)\n    y_preds = model(x_batch)\n    sum(loss.(y_preds, y_batch))\nend When doing this kind of concatenation use  reduce(hcat, xs)  rather than  hcat(xs...) . This will avoid the splatting penalty, and will hit the optimised  reduce  method."},{"id":341,"pagetitle":"Saving & Loading","title":"Saving and Loading Models","ref":"/flux/stable/saving/#Saving-and-Loading-Models","content":" Saving and Loading Models You may wish to save models so that they can be loaded and run in a later session. Flux provides a number of ways to do this.  The recommended way, which is the most robust one for long term storage,  is to use  Flux.state  in combination with a serialization format like  JLD2.jl  or  BSON.jl . Save a model: julia> using Flux\n\njulia> struct MyModel\n           net\n       end\n\njulia> Flux.@functor MyModel\n\njulia> MyModel() = MyModel(Chain(Dense(10, 5, relu), Dense(5, 2)));\n\njulia> model = MyModel()\nMyModel(Chain(Dense(10 => 5, relu), Dense(5 => 2)))\n\njulia> model_state = Flux.state(model);\n\njulia> using JLD2\n\njulia> jldsave(\"mymodel.jld2\"; model_state) Load it again in a new session using  Flux.loadmodel! : julia> using Flux, JLD2\n\njulia> model_state = JLD2.load(\"mymodel.jld2\", \"model_state\");\n\njulia> model = MyModel(); # MyModel definition must be available\n\njulia> Flux.loadmodel!(model, model_state); Note If a saved model's parameters are stored on the GPU, the model will not load later on if there is no GPU support available. It's best to  move your model to the CPU  with  cpu(model)  before saving it."},{"id":342,"pagetitle":"Saving & Loading","title":"Checkpointing","ref":"/flux/stable/saving/#Checkpointing","content":" Checkpointing In longer training runs it's a good idea to periodically save your model, so that you can resume if training is interrupted (for example, if there's a power cut).  julia> using Flux: throttle\n\njulia> using JLD2\n\njulia> m = Chain(Dense(10 => 5, relu), Dense(5 => 2))\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n)                   # Total: 4 arrays, 67 parameters, 524 bytes.\n\njulia> for epoch in 1:10\n          # ... train model ...\n          jldsave(\"model-checkpoint.jld2\", model_state = Flux.state(m))\n       end; This will update the  \"model-checkpoint.jld2\"  every epoch. You can get more advanced by saving a series of models throughout training, for example jldsave(\"model-$(now()).jld2\", model_state = Flux.state(m)) will produce a series of models like  \"model-2018-03-06T02:57:10.41.jld2\" . You could also store the current test set loss, so that it's easy to (for example) revert to an older copy of the model if it starts to overfit. jldsave(\"model-$(now()).jld2\", model_state = Flux.state(m), loss = testloss()) Note that to resume a model's training, you might need to restore other stateful parts of your training loop. Possible examples are the optimiser state and the randomness used to partition the original data into the training and validation sets. You can store the optimiser state alongside the model, to resume training exactly where you left off:  model = MyModel()\nopt_state = Flux.setup(AdamW(), model)\n\n# ... train model ...\n\nmodel_state = Flux.state(model)\njldsave(\"checkpoint_epoch=42.jld2\"; model_state, opt_state)"},{"id":343,"pagetitle":"Saving & Loading","title":"Saving Models as Julia Structs","ref":"/flux/stable/saving/#Saving-Models-as-Julia-Structs","content":" Saving Models as Julia Structs Models are just normal Julia structs, so it's fine to use any Julia storage format to save the struct as it is instead of saving the state returned by  Flux.state .   BSON.jl  is particularly convenient for this, since it can also save anynomous functions, which are sometimes part of a model definition. Save a model: julia> using Flux\n\njulia> model = Chain(Dense(10, 5, NNlib.relu), Dense(5, 2));\n\njulia> using BSON: @save\n\njulia> @save \"mymodel.bson\" model Load it again in a new session: julia> using Flux, BSON\n\njulia> BSON.@load \"mymodel.bson\" model\n\njulia> model\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n)                   # Total: 4 arrays, 67 parameters, 524 bytes. Warning Saving models this way could lead to compatibility issues across julia versions and across Flux versions if some of the Flux layers' internals are changed. It is therefore not recommended for long term storage, use  Flux.state  instead. Warning Previous versions of Flux suggested saving only the model weights using  @save \"mymodel.bson\" params(model) . This is no longer recommended and even strongly discouraged. Saving models this way will only store the trainable parameters which will result in incorrect behavior for layers like  BatchNorm ."},{"id":348,"pagetitle":"Callback Helpers","title":"Callback Helpers","ref":"/flux/stable/training/callbacks/#man-callback-helpers","content":" Callback Helpers"},{"id":349,"pagetitle":"Callback Helpers","title":"Flux.throttle","ref":"/flux/stable/training/callbacks/#Flux.throttle","content":" Flux.throttle  —  Function throttle(f, timeout; leading=true, trailing=false) Return a function that when invoked, will only be triggered at most once during  timeout  seconds. Normally, the throttled function will run as much as it can, without ever going more than once per  wait  duration; but if you'd like to disable the execution on the leading edge, pass  leading=false . To enable execution on the trailing edge, pass  trailing=true . Examples julia> a = Flux.throttle(() -> println(\"Flux\"), 2);\n\njulia> for i = 1:4  # a called in alternate iterations\n           a()\n           sleep(1)\n       end\nFlux\nFlux source"},{"id":350,"pagetitle":"Callback Helpers","title":"Patience Helpers","ref":"/flux/stable/training/callbacks/#Patience-Helpers","content":" Patience Helpers Flux provides utilities for controlling your training procedure according to some monitored condition and a maximum  patience . For example, you can use  early_stopping  to stop training when the model is converging or deteriorating, or you can use  plateau  to check if the model is stagnating. For example, below we create a pseudo-loss function that decreases, bottoms out, and then increases. The early stopping trigger will break the loop before the loss increases too much. # create a pseudo-loss that decreases for 4 calls, then starts increasing\n# we call this like loss()\nloss = let t = 0\n  () -> begin\n    t += 1\n    (t - 4) ^ 2\n  end\nend\n\n# create an early stopping trigger\n# returns true when the loss increases for two consecutive steps\nes = early_stopping(loss, 2; init_score = 9)\n\n# this will stop at the 6th (4 decreasing + 2 increasing calls) epoch\nfor epoch in 1:10\n  es() && break\nend The keyword argument  distance  of  early_stopping  is a function of the form  distance(best_score, score) . By default  distance  is  - , which implies that the monitored metric  f  is expected to be decreasing and minimized. If you use some increasing metric (e.g. accuracy), you can customize the  distance  function:  (best_score, score) -> score - best_score . # create a pseudo-accuracy that increases by 0.01 each time from 0 to 1\n# we call this like acc()\nacc = let v = 0\n  () -> v = max(1, v + 0.01)\nend\n\n# create an early stopping trigger for accuracy\nes = early_stopping(acc, 3; delta = (best_score, score) -> score - best_score)\n\n# this will iterate until the 10th epoch\nfor epoch in 1:10\n  es() && break\nend early_stopping  and  plateau  are both built on top of  patience . You can use  patience  to build your own triggers that use a patient counter. For example, if you want to trigger when the loss is below a threshold for several consecutive iterations: threshold(f, thresh, delay) = patience(delay) do\n  f() < thresh\nend Both  predicate  in  patience  and  f  in  early_stopping  /  plateau  can accept extra arguments. You can pass such extra arguments to  predicate  or  f  through the returned function: trigger = patience((a; b) -> a > b, 3)\n\n# this will iterate until the 10th epoch\nfor epoch in 1:10\n  trigger(1; b = 2) && break\nend\n\n# this will stop at the 3rd epoch\nfor epoch in 1:10\n  trigger(3; b = 2) && break\nend"},{"id":351,"pagetitle":"Callback Helpers","title":"Flux.patience","ref":"/flux/stable/training/callbacks/#Flux.patience","content":" Flux.patience  —  Function patience(predicate, wait) Return a function that internally counts by one when  predicate(...) == true , otherwise the count is reset to zero. If the count is greater than or equal to  wait , the function returns  true , otherwise it returns  false . Examples julia> loss() = rand();\n\njulia> trigger = Flux.patience(() -> loss() < 1, 3);\n\n\njulia> for i in 1:10\n         @info \"Epoch $i\"\n         trigger() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3 source"},{"id":352,"pagetitle":"Callback Helpers","title":"Flux.early_stopping","ref":"/flux/stable/training/callbacks/#Flux.early_stopping","content":" Flux.early_stopping  —  Function early_stopping(f, delay; distance = -, init_score = 0, min_dist = 0) Return a function that internally counts by one when  distance(best_score, f(...)) <= min_dist , where  best_score  is the last seen best value of  f(...) . If the count is greater than or equal to  delay , the function returns  true , otherwise it returns  false . The count is reset when  distance(best_score, f(...)) > min_dist . Examples julia> loss = let l = 0\n         () -> l += 1\n       end; # pseudo loss function that returns increasing values\n\njulia> es = Flux.early_stopping(loss, 3);\n\n\njulia> for i in 1:10\n         @info \"Epoch $i\"\n         es() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3 source"},{"id":353,"pagetitle":"Callback Helpers","title":"Flux.plateau","ref":"/flux/stable/training/callbacks/#Flux.plateau","content":" Flux.plateau  —  Function plateau(f, width; distance = -, init_score = 0, min_dist = 1f-6) Return a function that internally counts by one when  abs(distance(last_score, f(...))) <= min_dist , where  last_score  holds the last value of  f(...) . If the count is greater than or equal to  width , the function returns  true , otherwise it returns  false . The count is reset when  abs(distance(last_score, f(...))) > min_dist . Examples julia> f = let v = 10\n         () -> v = v / abs(v) - v\n       end; # -9, 8, -7, 6, ...\n\njulia> trigger = Flux.plateau(f, 3; init_score=10, min_dist=18);\n\n\njulia> for i in 1:10\n         @info \"Epoch $i\"\n         trigger() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3\n[ Info: Epoch 4 source"},{"id":356,"pagetitle":"Optimisation Rules","title":"Optimisation Rules","ref":"/flux/stable/training/optimisers/#man-optimisers","content":" Optimisation Rules Flux builds in many optimisation rules for use with  train!  and other training functions. The mechanism by which these work is gradually being replaced as part of the change from \"implicit\" dictionary-based to \"explicit\" tree-like structures. At present, the same struct (such as  Adam ) can be used with either form, and will be automatically translated. For full details of how the new interface works, see the  Optimisers.jl documentation . For full details on how the old \"implicit\" interface worked, see the  Flux 0.13.6 manual ."},{"id":357,"pagetitle":"Optimisation Rules","title":"Optimiser Reference","ref":"/flux/stable/training/optimisers/#Optimiser-Reference","content":" Optimiser Reference All optimisers return an object that, when passed to  train! , will update the parameters passed to it."},{"id":358,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.Descent","ref":"/flux/stable/training/optimisers/#Flux.Optimise.Descent","content":" Flux.Optimise.Descent  —  Type Descent(η = 0.1) Classic gradient descent optimiser with learning rate  η . For each parameter  p  and its gradient  δp , this runs  p -= η*δp Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Examples opt = Descent()\n\nopt = Descent(0.3)\n\nps = Flux.params(model)\n\ngs = gradient(ps) do\n    loss(x, y)\nend\n\nFlux.Optimise.update!(opt, ps, gs) source"},{"id":359,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.Momentum","ref":"/flux/stable/training/optimisers/#Flux.Optimise.Momentum","content":" Flux.Optimise.Momentum  —  Type Momentum(η = 0.01, ρ = 0.9) Gradient descent optimiser with learning rate  η  and momentum  ρ . Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Momentum ( ρ ): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations. Examples opt = Momentum()\n\nopt = Momentum(0.01, 0.99) source"},{"id":360,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.Nesterov","ref":"/flux/stable/training/optimisers/#Flux.Optimise.Nesterov","content":" Flux.Optimise.Nesterov  —  Type Nesterov(η = 0.001, ρ = 0.9) Gradient descent optimiser with learning rate  η  and Nesterov momentum  ρ . Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Nesterov momentum ( ρ ): Controls the acceleration of gradient descent in the                          prominent direction, in effect damping oscillations. Examples opt = Nesterov()\n\nopt = Nesterov(0.003, 0.95) source"},{"id":361,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.RMSProp","ref":"/flux/stable/training/optimisers/#Flux.Optimise.RMSProp","content":" Flux.Optimise.RMSProp  —  Type RMSProp(η = 0.001, ρ = 0.9, ϵ = 1.0e-8) Optimizer using the  RMSProp  algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don't need tuning. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Momentum ( ρ ): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations. Examples opt = RMSProp()\n\nopt = RMSProp(0.002, 0.95) source"},{"id":362,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.Adam","ref":"/flux/stable/training/optimisers/#Flux.Optimise.Adam","content":" Flux.Optimise.Adam  —  Type Adam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8) Adam  optimiser. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. Examples opt = Adam()\n\nopt = Adam(0.001, (0.9, 0.8)) source"},{"id":363,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.RAdam","ref":"/flux/stable/training/optimisers/#Flux.Optimise.RAdam","content":" Flux.Optimise.RAdam  —  Type RAdam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8) Rectified Adam  optimiser. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. Examples opt = RAdam()\n\nopt = RAdam(0.001, (0.9, 0.8)) source"},{"id":364,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.AdaMax","ref":"/flux/stable/training/optimisers/#Flux.Optimise.AdaMax","content":" Flux.Optimise.AdaMax  —  Type AdaMax(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8) AdaMax  is a variant of Adam based on the ∞-norm. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. Examples opt = AdaMax()\n\nopt = AdaMax(0.001, (0.9, 0.995)) source"},{"id":365,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.AdaGrad","ref":"/flux/stable/training/optimisers/#Flux.Optimise.AdaGrad","content":" Flux.Optimise.AdaGrad  —  Type AdaGrad(η = 0.1, ϵ = 1.0e-8) AdaGrad  optimiser. It has parameter specific learning rates based on how frequently it is updated. Parameters don't need tuning. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Examples opt = AdaGrad()\n\nopt = AdaGrad(0.001) source"},{"id":366,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.AdaDelta","ref":"/flux/stable/training/optimisers/#Flux.Optimise.AdaDelta","content":" Flux.Optimise.AdaDelta  —  Type AdaDelta(ρ = 0.9, ϵ = 1.0e-8) AdaDelta  is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don't need tuning. Parameters Rho ( ρ ): Factor by which the gradient is decayed at each time step. Examples opt = AdaDelta()\n\nopt = AdaDelta(0.89) source"},{"id":367,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.AMSGrad","ref":"/flux/stable/training/optimisers/#Flux.Optimise.AMSGrad","content":" Flux.Optimise.AMSGrad  —  Type AMSGrad(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8) The  AMSGrad  version of the Adam optimiser. Parameters don't need tuning. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. Examples opt = AMSGrad()\n\nopt = AMSGrad(0.001, (0.89, 0.995)) source"},{"id":368,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.NAdam","ref":"/flux/stable/training/optimisers/#Flux.Optimise.NAdam","content":" Flux.Optimise.NAdam  —  Type NAdam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8) NAdam  is a Nesterov variant of Adam. Parameters don't need tuning. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. Examples opt = NAdam()\n\nopt = NAdam(0.002, (0.89, 0.995)) source"},{"id":369,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.AdamW","ref":"/flux/stable/training/optimisers/#Flux.Optimise.AdamW","content":" Flux.Optimise.AdamW  —  Function AdamW(η = 0.001, β::Tuple = (0.9, 0.999), decay = 0) AdamW  is a variant of Adam fixing (as in repairing) its weight decay regularization. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. decay : Decay applied to weights during optimisation. Examples opt = AdamW()\n\nopt = AdamW(0.001, (0.89, 0.995), 0.1) source"},{"id":370,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.OAdam","ref":"/flux/stable/training/optimisers/#Flux.Optimise.OAdam","content":" Flux.Optimise.OAdam  —  Type OAdam(η = 0.0001, β::Tuple = (0.5, 0.9), ϵ = 1.0e-8) OAdam  (Optimistic Adam) is a variant of Adam adding an \"optimistic\" term suitable for adversarial training. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. Examples opt = OAdam()\n\nopt = OAdam(0.001, (0.9, 0.995)) source"},{"id":371,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.AdaBelief","ref":"/flux/stable/training/optimisers/#Flux.Optimise.AdaBelief","content":" Flux.Optimise.AdaBelief  —  Type AdaBelief(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8) The  AdaBelief  optimiser is a variant of the well-known Adam optimiser. Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. Decay of momentums ( β::Tuple ): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate. Examples opt = AdaBelief()\n\nopt = AdaBelief(0.001, (0.9, 0.8)) source"},{"id":372,"pagetitle":"Optimisation Rules","title":"Composing Optimisers","ref":"/flux/stable/training/optimisers/#Composing-Optimisers","content":" Composing Optimisers Flux defines a special kind of optimiser simply called  Optimiser  which takes in arbitrary optimisers as input. Its behaviour is similar to the usual optimisers, but differs in that it acts by calling the optimisers listed in it sequentially. Each optimiser produces a modified gradient that will be fed into the next, and the resultant update will be applied to the parameter as usual. A classic use case is where adding decays is desirable. Flux defines some basic decays including  ExpDecay ,  InvDecay  etc. opt = Optimiser(ExpDecay(1, 0.1, 1000, 1e-4), Descent()) Here we apply exponential decay to the  Descent  optimiser. The defaults of  ExpDecay  say that its learning rate will be decayed every 1000 steps. It is then applied like any optimiser. w = randn(10, 10)\nw1 = randn(10,10)\nps = Params([w, w1])\n\nloss(x) = Flux.Losses.mse(w * x, w1 * x)\n\nloss(rand(10)) # around 9\n\nfor t = 1:10^5\n  θ = Params([w, w1])\n  θ̄ = gradient(() -> loss(rand(10)), θ)\n  Flux.Optimise.update!(opt, θ, θ̄)\nend\n\nloss(rand(10)) # around 0.9 It is possible to compose optimisers for some added flexibility."},{"id":373,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.Optimiser","ref":"/flux/stable/training/optimisers/#Flux.Optimise.Optimiser","content":" Flux.Optimise.Optimiser  —  Type Optimiser(a, b, c...) Combine several optimisers into one; each optimiser produces a modified gradient that will be fed into the next, and this is finally applied to the parameter as usual. Note This will be replaced by  Optimisers.OptimiserChain  in Flux 0.15. source"},{"id":374,"pagetitle":"Optimisation Rules","title":"Scheduling Optimisers","ref":"/flux/stable/training/optimisers/#Scheduling-Optimisers","content":" Scheduling Optimisers In practice, it is fairly common to schedule the learning rate of an optimiser to obtain faster convergence. There are a variety of popular scheduling policies, and you can find implementations of them in  ParameterSchedulers.jl . The documentation for ParameterSchedulers.jl provides a more detailed overview of the different scheduling policies, and how to use them with Flux optimisers. Below, we provide a brief snippet illustrating a  cosine annealing  schedule with a momentum optimiser. First, we import ParameterSchedulers.jl and initialize a cosine annealing schedule to vary the learning rate between  1e-4  and  1e-2  every 10 steps. We also create a new  Momentum  optimiser. using ParameterSchedulers\n\nopt = Momentum()\nschedule = Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10)\nfor (eta, epoch) in zip(schedule, 1:100)\n  opt.eta = eta\n  # your training code here\nend schedule  can also be indexed (e.g.  schedule(100) ) or iterated like any iterator in Julia. ParameterSchedulers.jl schedules are stateless (they don't store their iteration state). If you want a  stateful  schedule, you can use  ParameterSchedulers.Stateful : using ParameterSchedulers: Stateful, next!\n\nschedule = Stateful(Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10))\nfor epoch in 1:100\n  opt.eta = next!(schedule)\n  # your training code here\nend ParameterSchedulers.jl allows for many more scheduling policies including arbitrary functions, looping any function with a given period, or sequences of many schedules. See the ParameterSchedulers.jl documentation for more info."},{"id":375,"pagetitle":"Optimisation Rules","title":"Decays","ref":"/flux/stable/training/optimisers/#Decays","content":" Decays Similar to optimisers, Flux also defines some simple decays that can be used in conjunction with other optimisers, or standalone."},{"id":376,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.ExpDecay","ref":"/flux/stable/training/optimisers/#Flux.Optimise.ExpDecay","content":" Flux.Optimise.ExpDecay  —  Type ExpDecay(η = 0.001, decay = 0.1, decay_step = 1000, clip = 1e-4, start = 1) Discount the learning rate  η  by the factor  decay  every  decay_step  steps till a minimum of  clip . Parameters Learning rate ( η ): Amount by which gradients are discounted before updating                      the weights. decay : Factor by which the learning rate is discounted. decay_step : Schedule decay operations by setting the number of steps between               two decay operations. clip : Minimum value of learning rate. 'start': Step at which the decay starts. See also the  Scheduling Optimisers  section of the docs for more general scheduling techniques. Examples ExpDecay  is typically composed  with other optimisers  as the last transformation of the gradient: opt = Optimiser(Adam(), ExpDecay(1.0)) Note: you may want to start with  η=1  in  ExpDecay  when combined with other optimisers ( Adam  in this case) that have their own learning rate. source"},{"id":377,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.InvDecay","ref":"/flux/stable/training/optimisers/#Flux.Optimise.InvDecay","content":" Flux.Optimise.InvDecay  —  Type InvDecay(γ = 0.001) Apply inverse time decay to an optimiser, so that the effective step size at iteration  n  is  eta / (1 + γ * n)  where  eta  is the initial step size. The wrapped optimiser's step size is not modified. See also the  Scheduling Optimisers  section of the docs for more general scheduling techniques. Examples InvDecay  is typically composed  with other optimisers  as the last transformation of the gradient: # Inverse decay of the learning rate\n# with starting value 0.001 and decay coefficient 0.01.\nopt = Optimiser(Adam(1f-3), InvDecay(1f-2)) source"},{"id":378,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.WeightDecay","ref":"/flux/stable/training/optimisers/#Flux.Optimise.WeightDecay","content":" Flux.Optimise.WeightDecay  —  Type WeightDecay(λ = 0) Decay weights by  $λ$ .  Typically composed  with other optimisers as the first transformation to the gradient, making it equivalent to adding  $L_2$  regularization  with coefficient   $λ$  to the loss. Examples opt = Optimiser(WeightDecay(1f-4), Adam()) source"},{"id":379,"pagetitle":"Optimisation Rules","title":"Gradient Clipping","ref":"/flux/stable/training/optimisers/#Gradient-Clipping","content":" Gradient Clipping Gradient clipping is useful for training recurrent neural networks, which have a tendency to suffer from the exploding gradient problem. An example usage is opt = Optimiser(ClipValue(1e-3), Adam(1e-3))"},{"id":380,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.ClipValue","ref":"/flux/stable/training/optimisers/#Flux.Optimise.ClipValue","content":" Flux.Optimise.ClipValue  —  Type ClipValue(thresh) Clip gradients when their absolute value exceeds  thresh . Note This will be replaced by  Optimisers.ClipGrad  in Flux 0.15. source"},{"id":381,"pagetitle":"Optimisation Rules","title":"Flux.Optimise.ClipNorm","ref":"/flux/stable/training/optimisers/#Flux.Optimise.ClipNorm","content":" Flux.Optimise.ClipNorm  —  Type ClipNorm(thresh) Clip gradients when their L2 norm exceeds  thresh . source"},{"id":384,"pagetitle":"Training API","title":"Training API Reference","ref":"/flux/stable/training/reference/#Training-API-Reference","content":" Training API Reference The new version of Flux's training code was written as an independent package,  Optimisers.jl . Only the function  train!  belongs to Flux itself. The Optimisers package is designed to allow for immutable objects. But at present all Flux models contain parameter arrays (such as  Array s and  CuArray s) which can be updated in-place. Because of this: The objects returned by  Optimisers.update!  can be ignored. Flux defines its own version of  setup  which checks this assumption. (Using instead  Optimisers.setup  will also work, they return the same thing.) The new implementation of rules such as Adam in the Optimisers is quite different from the old one in  Flux.Optimise . In Flux 0.14,  Flux.Adam()  returns the old one, with supertype  Flux.Optimise.AbstractOptimiser , but  setup  will silently translate it to its new counterpart. The available rules are listed the  optimisation rules  page here; see the  Optimisers documentation  for details on how the new rules work."},{"id":385,"pagetitle":"Training API","title":"Flux.Train.setup","ref":"/flux/stable/training/reference/#Flux.Train.setup","content":" Flux.Train.setup  —  Function opt_state = setup(rule, model) This is a version of  Optimisers.setup , and is the first step before using  train! . It differs from  Optimisers.setup  in that it: has one extra check for mutability (since Flux expects to mutate the model in-place, while Optimisers.jl is designed to return an updated model) has methods which accept Flux's old optimisers, and convert them. (The old  Flux.Optimise.Adam  and new  Optimisers.Adam  are distinct types.) New This function was added in Flux 0.13.9. It was not used by the old \"implicit\" interface, using  Flux.Optimise  module and  Flux.params . Example julia> model = Dense(2=>1, leakyrelu; init=ones);\n\njulia> opt_state = Flux.setup(Momentum(0.1), model)  # this encodes the optimiser and its state\n(weight = Leaf(Momentum{Float64}(0.1, 0.9), [0.0 0.0]), bias = Leaf(Momentum{Float64}(0.1, 0.9), [0.0]), σ = ())\n\njulia> x1, y1 = [0.2, -0.3], [0.4];  # use the same data for two steps:\n\njulia> Flux.train!(model, [(x1, y1), (x1, y1)], opt_state) do m, x, y\n         sum(abs.(m(x) .- y)) * 100\n       end\n\njulia> model.bias  # was zero, mutated by Flux.train!\n1-element Vector{Float64}:\n 10.19\n\njulia> opt_state  # mutated by Flux.train!\n(weight = Leaf(Momentum{Float64}(0.1, 0.9), [-2.018 3.027]), bias = Leaf(Momentum{Float64}(0.1, 0.9), [-10.09]), σ = ()) source"},{"id":386,"pagetitle":"Training API","title":"Flux.Optimise.train!","ref":"/flux/stable/training/reference/#Flux.Optimise.train!-NTuple{4, Any}","content":" Flux.Optimise.train!  —  Method train!(loss, model, data, opt_state) Uses a  loss  function and training  data  to improve the  model 's parameters according to a particular optimisation rule encoded in  opt_state .  Iterates through  data  once, evaluating for each  d in data  either  loss(model, d...)  if  d isa Tuple , or else  loss(model, d)  for other  d . For example, with these definitions... data = [(x1, y1), (x2, y2), (x3, y3)]\n\nloss3(m, x, y) = norm(m(x) .- y)        # the model is the first argument\n\nopt_state = Flux.setup(Adam(), model)   # explicit setup of optimiser momenta ...calling  Flux.train!(loss3, model, data, opt_state)  runs a loop much like this: for d in data\n    ∂L∂m = gradient(loss3, model, d...)[1]\n    update!(opt_state, model, ∂L∂m)\nend You can also write this loop yourself, if you need more flexibility. For this reason  train!  is not highly extensible. It adds only a few features to the loop above: Stop with a  DomainError  if the loss is infinite or  NaN  at any point. Show a progress bar using  @withprogress . New This method was added in Flux 0.13.9. It has significant changes from the one used by Flux ≤ 0.13: It now takes the  model  itself, not the result of  Flux.params . (This is to move away from Zygote's \"implicit\" parameter handling, with  Grads .) Instead of  loss  being a function which accepts only the data, now it must also accept the  model  itself, as the first argument. opt_state  should be the result of  Flux.setup . Using an optimiser such as  Adam()  without this step should give you a warning. Callback functions are not supported. (But any code can be included in the above  for  loop.) source"},{"id":387,"pagetitle":"Training API","title":"Optimisers.update!","ref":"/flux/stable/training/reference/#Optimisers.update!","content":" Optimisers.update!  —  Function Optimisers.update!(tree, model, gradient) -> (tree, model) Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from  setup . This is used in exactly the same manner as  update , but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary  Array s or  CuArray s. However, you should not rely on the old model being fully updated but rather use the returned model. (The original state tree is always mutated, as each  Leaf  is mutable.) Example julia> using StaticArrays, Zygote, Optimisers\n\njulia> m = (x = [1f0, 2f0], y = SA[4f0, 5f0]);  # partly mutable model\n\njulia> t = Optimisers.setup(Momentum(1/30, 0.9), m)  # tree of states\n(x = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]), y = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]))\n\njulia> g = gradient(m -> sum(abs2.(m.x .+ m.y)), m)[1]  # structural gradient\n(x = Float32[10.0, 14.0], y = Float32[10.0, 14.0])\n\njulia> t2, m2 = Optimisers.update!(t, m, g);\n\njulia> m2  # after update or update!, this is the new model\n(x = Float32[0.6666666, 1.5333333], y = Float32[3.6666667, 4.5333333])\n\njulia> m2.x === m.x  # update! has re-used this array, for efficiency\ntrue\n\njulia> m  # original should be discarded, may be mutated but no guarantee\n(x = Float32[0.6666666, 1.5333333], y = Float32[4.0, 5.0])\n\njulia> t == t2  # original state tree is guaranteed to be mutated\ntrue train!  uses  @progress  which should show a progress bar in VSCode automatically. To see one in a terminal, you will need to install  TerminalLoggers.jl  and follow its setup instructions."},{"id":388,"pagetitle":"Training API","title":"Optimisation Modifiers","ref":"/flux/stable/training/reference/#Optimisation-Modifiers","content":" Optimisation Modifiers The state returned by  setup  can be modified to temporarily prevent training of some parts of the model, or to change the learning rate or other hyperparameter. The functions for doing so may be accessed as  Flux.freeze! ,  Flux.thaw! , and  Flux.adjust! . All mutate the state (or part of it) and return  nothing ."},{"id":389,"pagetitle":"Training API","title":"Optimisers.adjust!","ref":"/flux/stable/training/reference/#Optimisers.adjust!","content":" Optimisers.adjust!  —  Function Optimisers.adjust!(tree, η) Alters the state  tree = setup(rule, model)  to change the parameters of the optimisation rule, without destroying its stored state. Typically used mid-way through training. Can be applied to part of a model, by acting only on the corresponding part of the state  tree . To change just the learning rate, provide a number  η::Real . Example julia> m = (vec = rand(Float32, 2), fun = sin);\n\njulia> st = Optimisers.setup(Nesterov(), m)  # stored momentum is initialised to zero\n(vec = Leaf(Nesterov(0.001, 0.9), Float32[0.0, 0.0]), fun = ())\n\njulia> st, m = Optimisers.update(st, m, (vec = [16, 88], fun = nothing));  # with fake gradient\n\njulia> st\n(vec = Leaf(Nesterov(0.001, 0.9), Float32[-0.016, -0.088]), fun = ())\n\njulia> Optimisers.adjust!(st, 0.123)  # change learning rate, stored momentum untouched\n\njulia> st\n(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ()) To change other parameters,  adjust!  also accepts keyword arguments matching the field names of the optimisation rule's type. julia> fieldnames(Adam)\n(:eta, :beta, :epsilon)\n\njulia> st2 = Optimisers.setup(OptimiserChain(ClipGrad(), Adam()), m)\n(vec = Leaf(OptimiserChain(ClipGrad(10.0), Adam(0.001, (0.9, 0.999), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())\n\njulia> Optimisers.adjust(st2; beta = (0.777, 0.909), delta = 11.1)  # delta acts on ClipGrad\n(vec = Leaf(OptimiserChain(ClipGrad(11.1), Adam(0.001, (0.777, 0.909), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())\n\njulia> Optimisers.adjust(st; beta = \"no such field\")  # silently ignored!\n(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())"},{"id":390,"pagetitle":"Training API","title":"Optimisers.freeze!","ref":"/flux/stable/training/reference/#Optimisers.freeze!","content":" Optimisers.freeze!  —  Function Optimisers.freeze!(tree) Temporarily alters the state  tree = setup(rule, model)  so that parameters will not be updated. Un-done by  thaw! . Can be applied to the state corresponding to only part of a model, for instance with  model::Chain , to freeze  model.layers[1]  you should call  freeze!(tree.layers[1]) . Example julia> m = (x = ([1.0], 2.0), y = [3.0]);\n\njulia> s = Optimisers.setup(Momentum(), m);\n\njulia> Optimisers.freeze!(s.x)\n\njulia> Optimisers.update!(s, m, (x = ([pi], 10pi), y = [100pi]));  # with fake gradient\n\njulia> m\n(x = ([1.0], 2.0), y = [-0.14159265358979312])\n\njulia> s\n(x = (Leaf(Momentum(0.01, 0.9), [0.0], frozen = true), ()), y = Leaf(Momentum(0.01, 0.9), [3.14159]))\n\njulia> Optimisers.thaw!(s)\n\njulia> s.x\n(Leaf(Momentum(0.01, 0.9), [0.0]), ())"},{"id":391,"pagetitle":"Training API","title":"Optimisers.thaw!","ref":"/flux/stable/training/reference/#Optimisers.thaw!","content":" Optimisers.thaw!  —  Function Optimisers.thaw!(tree) The reverse of  freeze! . Applies to all parameters, mutating every  Leaf(rule, state, frozen = true)  to  Leaf(rule, state, frozen = false) ."},{"id":392,"pagetitle":"Training API","title":"Implicit style (Flux ≤ 0.14)","ref":"/flux/stable/training/reference/#Implicit-style-(Flux-0.14)","content":" Implicit style (Flux ≤ 0.14) Flux used to handle gradients, training, and optimisation rules quite differently. The new style described above is called \"explicit\" by Zygote, and the old style \"implicit\". Flux 0.13 and 0.14 are the transitional versions which support both; Flux 0.15 will remove the old. How to upgrade The blue-green boxes in the  training section  describe the changes needed to upgrade old code. For full details on the interface for implicit-style optimisers, see the  Flux 0.13.6 manual . Flux ≤ 0.12 Earlier versions of Flux exported  params , thus allowing unqualified  params(model)  after  using Flux . This conflicted with too many other packages, and was removed in Flux 0.13. If you get an error  UndefVarError: params not defined , this probably means that you are following code for Flux 0.12 or earlier on a more recent version."},{"id":393,"pagetitle":"Training API","title":"Flux.params","ref":"/flux/stable/training/reference/#Flux.params","content":" Flux.params  —  Function params(model)\nparams(layers...) Given a model or specific layers from a model, create a  Params  object pointing to its trainable parameters. This can be used with the  gradient  function, see the  training section of the manual , or as input to the  Flux.train!  function. The behaviour of  params  on custom types can be customized using  Functors.@functor  or  Flux.trainable . Examples julia> using Flux: params\n\njulia> params(Chain(Dense(ones(2,3)), softmax))  # unpacks Flux models\nParams([[1.0 1.0 1.0; 1.0 1.0 1.0], [0.0, 0.0]])\n\njulia> bn = BatchNorm(2, relu)\nBatchNorm(2, relu)  # 4 parameters, plus 4 non-trainable\n\njulia> params(bn)  # only the trainable parameters\nParams([Float32[0.0, 0.0], Float32[1.0, 1.0]])\n\njulia> params([1, 2, 3], [4])  # one or more arrays of numbers\nParams([[1, 2, 3], [4]])\n\njulia> params([[1, 2, 3], [4]])  # unpacks array of arrays\nParams([[1, 2, 3], [4]])\n\njulia> params(1, [2 2], (alpha=[3,3,3], beta=Ref(4), gamma=sin))  # ignores scalars, unpacks NamedTuples\nParams([[2 2], [3, 3, 3]]) source"},{"id":394,"pagetitle":"Training API","title":"Optimisers.update!","ref":"/flux/stable/training/reference/#Optimisers.update!-Tuple{Flux.Optimise.AbstractOptimiser, AbstractArray, Any}","content":" Optimisers.update!  —  Method update!(opt, p, g)\nupdate!(opt, ps::Params, gs) Perform an update step of the parameters  ps  (or the single parameter  p ) according to optimiser  opt::AbstractOptimiser   and the gradients  gs  (the gradient  g ). As a result, the parameters are mutated and the optimiser's internal state may change. The gradient could be mutated as well. Deprecated This method for implicit  Params  (and  AbstractOptimiser ) will be removed from Flux 0.15. The explicit method  update!(opt, model, grad)  from Optimisers.jl will remain. source"},{"id":395,"pagetitle":"Training API","title":"Flux.Optimise.train!","ref":"/flux/stable/training/reference/#Flux.Optimise.train!-Tuple{Any, Params, Any, Flux.Optimise.AbstractOptimiser}","content":" Flux.Optimise.train!  —  Method train!(loss, pars::Params, data, opt::AbstractOptimiser; [cb]) Uses a  loss  function and training  data  to improve the  model's parameters according to a particular optimisation rule  opt . Deprecated This method with implicit  Params  will be removed from Flux 0.15. It should be replaced with the explicit method  train!(loss, model, data, opt) . For each  d in data , first the gradient of the  loss  is computed like this:     gradient(() -> loss(d...), pars)  # if d isa Tuple\n    gradient(() -> loss(d), pars)     # otherwise Here  pars  is produced by calling  Flux.params  on your model. (Or just on the layers you want to train, like  train!(loss, params(model[1:end-2]), data, opt) .) This is the \"implicit\" style of parameter handling. This gradient is then used by optimiser  opt  to update the parameters:     update!(opt, pars, grads) The optimiser should be from the  Flux.Optimise  module (see  Optimisers ). Different optimisers can be combined using  Flux.Optimise.Optimiser . This training loop iterates through  data  once. It will stop with a  DomainError  if the loss is  NaN  or infinite. You can use use  train!  inside a for loop to do this several times, or  use for instance  Itertools.ncycle  to make a longer  data  iterator. Callbacks Callbacks  are given with the keyword argument  cb . For example, this will print \"training\" every 10 seconds (using  Flux.throttle ):     train!(loss, params, data, opt, cb = throttle(() -> println(\"training\"), 10)) Multiple callbacks can be passed to  cb  as array. source"},{"id":396,"pagetitle":"Training API","title":"Callbacks","ref":"/flux/stable/training/reference/#Callbacks","content":" Callbacks Implicit  train!  takes an additional argument,  cb , that's used for callbacks so that you can observe the training process. For example: train!(objective, ps, data, opt, cb = () -> println(\"training\")) Callbacks are called for every batch of training data. You can slow this down using  Flux.throttle(f, timeout)  which prevents  f  from being called more than once every  timeout  seconds. A more typical callback might look like this: test_x, test_y = # ... create single batch of test data ...\nevalcb() = @show(loss(test_x, test_y))\nthrottled_cb = throttle(evalcb, 5)\nfor epoch in 1:20\n  @info \"Epoch $epoch\"\n  Flux.train!(objective, ps, data, opt, cb = throttled_cb)\nend See the page about  callback helpers  for more."},{"id":399,"pagetitle":"Training","title":"Training a Flux Model","ref":"/flux/stable/training/training/#man-training","content":" Training a Flux Model Training refers to the process of slowly adjusting the parameters of a model to make it work better. Besides the model itself, we will need three things: An  objective function  that evaluates how well a model is doing on some input. An  optimisation rule  which describes how the model's parameters should be adjusted. Some  training data  to use as the input during this process. Usually the training data is some collection of examples (or batches of examples) which are handled one-by-one. One  epoch  of training means that each example is used once, something like this: # Initialise the optimiser for this model:\nopt_state = Flux.setup(rule, model)\n\nfor data in train_set\n  # Unpack this element (for supervised training):\n  input, label = data\n\n  # Calculate the gradient of the objective\n  # with respect to the parameters within the model:\n  grads = Flux.gradient(model) do m\n      result = m(input)\n      loss(result, label)\n  end\n\n  # Update the parameters so as to reduce the objective,\n  # according the chosen optimisation rule:\n  Flux.update!(opt_state, model, grads[1])\nend This loop can also be written using the function  train! , but it's helpful to undersand the pieces first: train!(model, train_set, opt_state) do m, x, y\n  loss(m(x), y)\nend"},{"id":400,"pagetitle":"Training","title":"Model Gradients","ref":"/flux/stable/training/training/#Model-Gradients","content":" Model Gradients Fist recall from the section on  taking gradients  that   Flux.gradient(f, a, b)  always calls  f(a, b) , and returns a tuple  (∂f_∂a, ∂f_∂b) . In the code above, the function  f  passed to  gradient  is an anonymous function with one argument, created by the  do  block, hence   grads  is a tuple with one element. Instead of a  do  block, we could have written: grads = Flux.gradient(m -> loss(m(input), label), model) Since the model is some nested set of layers,  grads[1]  is a similarly nested set of  NamedTuple s, ultimately containing gradient components. If (for example)   θ = model.layers[1].weight[2,3]  is one scalar parameter, an entry in a matrix of weights, then the derivative of the loss with respect to it is  ∂f_∂θ = grads[1].layers[1].weight[2,3] . It is important that the execution of the model takes place inside the call to  gradient , in order for the influence of the model's parameters to be observed by Zygote. It is also important that every  update!  step receives a newly computed gradient, as it will change whenever the model's parameters are changed, and for each new data point. Implicit gradients Flux ≤ 0.14 used Zygote's \"implicit\" mode, in which  gradient  takes a zero-argument function. It looks like this: pars = Flux.params(model)\ngrad = gradient(() -> loss(model(input), label), pars) Here  pars::Params  and  grad::Grads  are two dictionary-like structures. Support for this will be removed from Flux 0.15, and these blue (teal?) boxes explain what needs to change."},{"id":401,"pagetitle":"Training","title":"Loss Functions","ref":"/flux/stable/training/training/#Loss-Functions","content":" Loss Functions The objective function must return a number representing how far the model is from the desired result. This is termed the  loss  of the model. This number can be produced by any ordinary Julia code, but this must be executed within the call to  gradient . For instance, we could define a function loss(y_hat, y) = sum((y_hat .- y).^2) or write this directly inside the  do  block above. Many commonly used functions, like  mse  for mean-squared error or  crossentropy  for cross-entropy loss, are available from the  Flux.Losses  module. Implicit-style loss functions Flux ≤ 0.14 needed a loss function which closed over a reference to the model, instead of being a pure function. Thus in old code you may see something like loss(x, y) = sum((model(x) .- y).^2) which defines a function making reference to a particular global variable  model ."},{"id":402,"pagetitle":"Training","title":"Optimisation Rules","ref":"/flux/stable/training/training/#Optimisation-Rules","content":" Optimisation Rules The simplest kind of optimisation using the gradient is termed  gradient descent  (or sometimes  stochastic gradient descent  when, as here, it is not applied to the entire dataset at once). Gradient descent needs a  learning rate  which is a small number describing how fast to walk downhill, usually written as the Greek letter \"eta\",  η . This is often described as a  hyperparameter , to distinguish it from the parameters which are being updated  θ = θ - η * ∂loss_∂θ . We want to update all the parameters in the model, like this: η = 0.01   # learning rate\n\n# For each parameter array, update\n# according to the corresponding gradient:\nfmap(model, grads[1]) do p, g\n  p .= p .- η .* g\nend A slightly more refined version of this loop to update all the parameters is wrapped up as a function  update! (opt_state, model, grads[1]) . And the learning rate is the only thing stored in the  Descent  struct. However, there are many other optimisation rules, which adjust the step size and direction in various clever ways. Most require some memory of the gradients from earlier steps, rather than always walking straight downhill –  Momentum  is the simplest. The function  setup  creates the necessary storage for this, for a particular model. It should be called once, before training, and returns a tree-like object which is the first argument of  update! . Like this: # Initialise momentum \nopt_state = Flux.setup(Momentum(0.01, 0.9), model)\n\nfor data in train_set\n  grads = [...]\n\n  # Update both model parameters and optimiser state:\n  Flux.update!(opt_state, model, grads[1])\nend Many commonly-used optimisation rules, such as  Adam , are built-in. These are listed on the  optimisers  page. Implicit-style optimiser state This  setup  makes another tree-like structure. Old versions of Flux did not do this, and instead stored a dictionary-like structure within the optimiser  Adam(0.001) . This was initialised on first use of the version of  update!  for \"implicit\" parameters."},{"id":403,"pagetitle":"Training","title":"Datasets & Batches","ref":"/flux/stable/training/training/#Datasets-and-Batches","content":" Datasets & Batches The loop above iterates through  train_set , expecting at each step a tuple  (input, label) . The very simplest such object is a vector of tuples, such as this: x = randn(28, 28)\ny = rand(10)\ndata = [(x, y)] or  data = [(x, y), (x, y), (x, y)]  for the same values three times. Very often, the initial data is large arrays which you need to slice into examples. To produce one iterator of pairs  (x, y) , you might want  zip : X = rand(28, 28, 60_000);  # many images, each 28 × 28\nY = rand(10, 60_000)\ndata = zip(eachslice(X; dims=3), eachcol(Y))\n\nfirst(data) isa Tuple{AbstractMatrix, AbstractVector}  # true Here each iteration will use one matrix  x  (an image, perhaps) and one vector  y . It is very common to instead train on  batches  of such inputs (or  mini-batches , the two words mean the same thing) both for efficiency and for better results. This can be easily done using the  DataLoader : data = Flux.DataLoader((X, Y), batchsize=32)\n\nx1, y1 = first(data)\nsize(x1) == (28, 28, 32)\nlength(data) == 1875 === 60_000 ÷ 32 Flux's layers are set up to accept such a batch of input data, and the convolutional layers such as  Conv  require it. The batch index is always the last dimension."},{"id":404,"pagetitle":"Training","title":"Training Loops","ref":"/flux/stable/training/training/#Training-Loops","content":" Training Loops Simple training loops like the one above can be written compactly using the  train!  function. Including  setup , this reads: opt_state = Flux.setup(Adam(), model)\n\nfor epoch in 1:100\n  Flux.train!(model, train_set, opt_state) do m, x, y\n    loss(m(x), y)\n  end\nend Or explicitly writing the anonymous function which this  do  block creates,  train!((m,x,y) -> loss(m(x),y), model, train_set, opt_state)  is exactly equivalent. Implicit-style `train!` This is a new method of  train! , which takes the result of  setup  as its 4th argument. The 1st argument is a function which accepts the model itself. Flux versions ≤ 0.14 provided a method of  train!  for \"implicit\" parameters, which works like this: train!((x,y) -> loss(model(x), y), Flux.params(model), train_set, Adam()) Real training loops often need more flexibility, and the best way to do this is just to write the loop. This is ordinary Julia code, without any need to work through some callback API. Here is an example, in which it may be helpful to note: The function  withgradient  is like  gradient  but also returns the value of the function, for logging or diagnostic use. Logging or printing is best done outside of the  gradient  call, as there is no need to differentiate these commands. Julia's  break  and  continue  keywords let you exit from parts of the loop. opt_state = Flux.setup(Adam(), model)\n\nmy_log = []\nfor epoch in 1:100\n  losses = Float32[]\n  for (i, data) in enumerate(train_set)\n    input, label = data\n\n    val, grads = Flux.withgradient(model) do m\n      # Any code inside here is differentiated.\n      # Evaluation of the model and loss must be inside!\n      result = m(input)\n      my_loss(result, label)\n    end\n\n    # Save the loss from the forward pass. (Done outside of gradient.)\n    push!(losses, val)\n\n    # Detect loss of Inf or NaN. Print a warning, and then skip update!\n    if !isfinite(val)\n      @warn \"loss is $val on item $i\" epoch\n      continue\n    end\n\n    Flux.update!(opt_state, model, grads[1])\n  end\n\n  # Compute some accuracy, and save details as a NamedTuple\n  acc = my_accuracy(model, train_set)\n  push!(my_log, (; acc, losses))\n\n  # Stop training when some criterion is reached\n  if  acc > 0.95\n    println(\"stopping after $epoch epochs\")\n    break\n  end\nend"},{"id":405,"pagetitle":"Training","title":"Regularisation","ref":"/flux/stable/training/training/#Regularisation","content":" Regularisation The term  regularisation  covers a wide variety of techniques aiming to improve the result of training. This is often done to avoid overfitting. Some of these are can be implemented by simply modifying the loss function.   L₂ regularisation  (sometimes called ridge regression) adds to the loss a penalty proportional to  θ^2  for every scalar parameter. For a very simple model could be implemented as follows: grads = Flux.gradient(densemodel) do m\n  result = m(input)\n  penalty = sum(abs2, m.weight)/2 + sum(abs2, m.bias)/2\n  my_loss(result, label) + 0.42 * penalty\nend Accessing each individual parameter array by hand won't work well for large models. Instead, we can use  Flux.params  to collect all of them, and then apply a function to each one, and sum the result: pen_l2(x::AbstractArray) = sum(abs2, x)/2\n\ngrads = Flux.gradient(model) do m\n  result = m(input)\n  penalty = sum(pen_l2, Flux.params(m))\n  my_loss(result, label) + 0.42 * penalty\nend However, the gradient of this penalty term is very simple: It is proportional to the original weights. So there is a simpler way to implement exactly the same thing, by modifying the optimiser instead of the loss function. This is done by replacing this: opt_state = Flux.setup(Adam(0.1), model) with this: decay_opt_state = Flux.setup(OptimiserChain(WeightDecay(0.42), Adam(0.1)), model) Flux's optimisers are really modifications applied to the gradient before using it to update the parameters, and  OptimiserChain  applies two such modifications. The first,  WeightDecay  adds  0.42  times original parameter to the gradient, matching the gradient of the penalty above (with the same, unrealistically large, constant). After that, in either case,  Adam  computes the final update. The same  OptimiserChain  mechanism can be used for other purposes, such as gradient clipping with  ClipGrad  or  ClipNorm . Besides L2 / weight decay, another common and quite different kind of regularisation is provided by the  Dropout  layer. This turns off some outputs of the previous layer during training. It should switch automatically, but see  trainmode!  /  testmode!  to manually enable or disable this layer."},{"id":406,"pagetitle":"Training","title":"Freezing & Schedules","ref":"/flux/stable/training/training/#Freezing-and-Schedules","content":" Freezing & Schedules Finer control of training, you may wish to alter the learning rate mid-way through training. This can be done with  adjust! , like this: opt_state = Flux.setup(Adam(0.1), model)  # initialise once\n\nfor epoch in 1:1000\n  train!([...], state)  # Train with η = 0.1 for first 100,\n  if epoch == 100       # then change to use η = 0.01 for the rest.\n    Flux.adjust!(opt_state, 0.01)\n  end\nend Flux ≤ 0.14 With the old \"implicit\" optimiser,  opt = Adam(0.1) , the equivalent was to directly mutate the  Adam  struct,  opt.eta = 0.001 .  Other hyper-parameters can also be adjusted, such as  Flux.adjust!(opt_state, beta = (0.8, 0.99)) . And such modifications can be applied to just one part of the model. For instance, this sets a different learning rate for the encoder and the decoder: # Consider some model with two parts:\nbimodel = Chain(enc = [...], dec = [...])\n\n# This returns a tree whose structure matches the model:\nopt_state = Flux.setup(Adam(0.02), bimodel)\n\n# Adjust the learning rate to be used for bimodel.layers.enc\nFlux.adjust!(opt_state.layers.enc, 0.03) To completely disable training of some part of the model, use  freeze! . This is a temporary modification, reversed by  thaw! : Flux.freeze!(opt_state.layers.enc)\n\n# Now training won't update parameters in bimodel.layers.enc\ntrain!(loss, bimodel, data, opt_state)\n\n# Un-freeze the entire model:\nFlux.thaw!(opt_state) Flux ≤ 0.14 The earlier \"implicit\" equivalent was to pass to  gradient  an object referencing only part of the model, such as  Flux.params(bimodel.layers.enc) ."},{"id":407,"pagetitle":"Training","title":"Implicit or Explicit?","ref":"/flux/stable/training/training/#Implicit-or-Explicit?","content":" Implicit or Explicit? Flux used to handle gradients, training, and optimisation rules quite differently. The new style described above is called \"explicit\" by Zygote, and the old style \"implicit\". Flux 0.13 and 0.14 are the transitional versions which support both. The blue-green boxes above describe the changes. For more details on training in the implicit style, see  Flux 0.13.6 documentation . For details about the two gradient modes, see  Zygote's documentation ."},{"id":410,"pagetitle":"Gradients – Zygote.jl","title":"Automatic Differentiation using Zygote.jl","ref":"/flux/stable/training/zygote/#autodiff-zygote","content":" Automatic Differentiation using Zygote.jl Flux re-exports the  gradient  from  Zygote , and uses this function within  train!  to differentiate the model. Zygote has its own  documentation , in particular listing some  important limitations ."},{"id":411,"pagetitle":"Gradients – Zygote.jl","title":"Explicit style","ref":"/flux/stable/training/zygote/#Explicit-style","content":" Explicit style The preferred way of using Zygote, and the only way of using most other AD packages, is to explicitly provide a function and its arguments."},{"id":412,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.gradient","ref":"/flux/stable/training/zygote/#Zygote.gradient-Tuple{Any, Vararg{Any}}","content":" Zygote.gradient  —  Method gradient(f, args...) Returns a tuple containing  ∂f/∂x  for each argument  x , the derivative (for scalar  x ) or the gradient. f(args...)  must be a real number, see  jacobian  for array output. See also  withgradient  to keep the value  f(args...) , and  pullback  for value and back-propagator. julia> gradient(*, 2.0, 3.0, 5.0)\n(15.0, 10.0, 6.0)\n\njulia> gradient(x -> sum(abs2,x), [7.0, 11.0, 13.0])\n([14.0, 22.0, 26.0],)\n\njulia> gradient([7, 11], 0, 1) do x, y, d\n         p = size(x, d)\n         sum(x.^p .+ y)\n       end\n([14.0, 22.0], 2.0, nothing)"},{"id":413,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.withgradient","ref":"/flux/stable/training/zygote/#Zygote.withgradient-Tuple{Any, Vararg{Any}}","content":" Zygote.withgradient  —  Method withgradient(f, args...)\nwithgradient(f, ::Params) Returns both the value of the function and the  gradient , as a named tuple.  julia> y, ∇ = withgradient(/, 1, 2)\n(val = 0.5, grad = (0.5, -0.25))\n\njulia> ∇ == gradient(/, 1, 2)\ntrue Allows you to capture auxillary outputs, in addition to the scalar used by  gradient . To do this,  f  must return a Tuple or NamedTuple. Then it calculates  grad = gradient(first∘f, args...) but returns the whole val = f(args...)`: julia> withgradient([1,2,4]) do x\n          z = 1 ./ x\n          sum(z), z  # here z is an auxillary output\n       end\n(val = (1.75, [1.0, 0.5, 0.25]), grad = ([-1.0, -0.25, -0.0625],))\n\njulia> withgradient(3.0, 4.0) do x, y\n          (div = x/y, mul = x*y)\n       end\n(val = (div = 0.75, mul = 12.0), grad = (0.25, -0.1875)) Also supports implicit mode: julia> w = [3.0];\n\njulia> res = withgradient(() -> sum(abs2, w), Params([w]))\n(val = 9.0, grad = Grads(...))\n\njulia> res.grad[w]\n1-element Vector{Float64}:\n 6.0"},{"id":414,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.jacobian","ref":"/flux/stable/training/zygote/#Zygote.jacobian-Tuple{Any, Vararg{Any}}","content":" Zygote.jacobian  —  Method jacobian(f, args...) -> Tuple For each array  a ∈ args  this returns a matrix with  Ja[k,i] = ∂y[k]/∂a[i]  where  y = f(args...)  is usually a vector. Arrays of higher dimension are treated like  vec(a) , or  vec(y)  for output. For scalar  x::Number ∈ args , the result is a vector  Jx[k] = ∂y[k]/∂x , while for scalar  y  all results have just one row. With any other argument type, no result is produced, even if  gradient  would work. This reverse-mode Jacobian needs to evaluate the pullback once for each element of  y . Doing so is usually only efficient when  length(y)  is small compared to  length(a) , otherwise forward mode is likely to be better. See also  withjacobian ,  hessian ,  hessian_reverse . Examples julia> jacobian(a -> 100*a[1:3].^2, 1:7)[1]  # first index (rows) is output\n3×7 Matrix{Int64}:\n 200    0    0  0  0  0  0\n   0  400    0  0  0  0  0\n   0    0  600  0  0  0  0\n\njulia> jacobian((a,x) -> a.^2 .* x, [1,2,3], 1)  # scalar argument has vector jacobian\n([2 0 0; 0 4 0; 0 0 6], [1, 4, 9])\n\njulia> jacobian((a,d) -> prod(a, dims=d), [1 2; 3 4; 5 6], 2)\n([2 0 … 0 0; 0 4 … 3 0; 0 0 … 0 5], [0, 0, 0]) Warning For arguments of any type except  Number  &  AbstractArray , the result is  nothing . julia> jacobian((a,s) -> a.^length(s), [1,2,3], \"str\")\n([3 0 0; 0 12 0; 0 0 27], nothing)\n\njulia> jacobian((a,t) -> sum(a .* t[1]) + t[2], [1,2,3], (4,5))\n([4 4 4], nothing)\n\njulia> gradient((a,t) -> sum(a .* t[1]) + t[2], [1,2,3], (4,5))  # gradient undersands the tuple\n([4 4 4], (6, 1))"},{"id":415,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.withjacobian","ref":"/flux/stable/training/zygote/#Zygote.withjacobian-Tuple{Any, Vararg{Any}}","content":" Zygote.withjacobian  —  Method withjacobian(f, args...) Returns both the value  f(args...)  and the  jacobian  as a named tuple. julia> withjacobian(cumsum, [1,2,3])\n(val = [1, 3, 6], grad = ([1 0 0; 1 1 0; 1 1 1],))"},{"id":416,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.hessian","ref":"/flux/stable/training/zygote/#Zygote.hessian","content":" Zygote.hessian  —  Function hessian(f, x) Construct the Hessian  ∂²f/∂x² , where  x  is a real number or an array, and  f(x)  is a real number. When  x  is an array, the result is a matrix  H[i,j] = ∂²f/∂x[i]∂x[j] , using linear indexing  x[i]  even if the argument is higher-dimensional. This uses forward over reverse, ForwardDiff over Zygote, calling  hessian_dual(f, x) . See  hessian_reverse  for an all-Zygote alternative. See also  diaghessian  to compute only the diagonal part. Examples julia> hessian(x -> x[1]*x[2], randn(2))\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> hessian(x -> sum(x.^3), [1 2; 3 4])  # uses linear indexing of x\n4×4 Matrix{Int64}:\n 6   0   0   0\n 0  18   0   0\n 0   0  12   0\n 0   0   0  24\n\njulia> hessian(sin, pi/2)\n-1.0"},{"id":417,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.hessian_reverse","ref":"/flux/stable/training/zygote/#Zygote.hessian_reverse","content":" Zygote.hessian_reverse  —  Function hessian_reverse(f, x) This should be equivalent to  hessian(f, x) , but implemented using reverse over reverse mode, all Zygote. (This is usually much slower, and more likely to find errors.)"},{"id":418,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.diaghessian","ref":"/flux/stable/training/zygote/#Zygote.diaghessian","content":" Zygote.diaghessian  —  Function diaghessian(f, args...) -> Tuple Diagonal part of the Hessian. Returns a tuple containing, for each argument  x ,  h  of the same shape with  h[i] = Hᵢᵢ = ∂²y/∂x[i]∂x[i] .  The original evaluation  y = f(args...)  must give a real number  y . For one vector argument  x , this is equivalent to  (diag(hessian(f,x)),) . Like  hessian  it uses ForwardDiff over Zygote.  Warning For arguments of any type except  Number  &  AbstractArray , the result is  nothing . Examples julia> diaghessian(x -> sum(x.^3), [1 2; 3 4])[1]\n2×2 Matrix{Int64}:\n  6  12\n 18  24\n\njulia> Diagonal(vec(ans)) == hessian(x -> sum(x.^3), [1 2; 3 4])  # full Hessian is diagonal\ntrue\n\njulia> diaghessian((x,y) -> sum(x .* y .* y'), [1 22; 333 4], [0.5, 0.666])  # two array arguments\n([0.0 0.0; 0.0 0.0], [2.0, 8.0])\n\njulia> diaghessian(atan, 1, 2)  # two scalar arguments\n(-0.16, 0.16)\n\njulia> hessian(xy -> atan(xy[1], xy[2]), [1, 2])  # full Hessian is not diagonal\n2×2 Matrix{Float64}:\n -0.16  -0.12\n -0.12   0.16"},{"id":419,"pagetitle":"Gradients – Zygote.jl","title":"Implicit style (Flux ≤ 0.14)","ref":"/flux/stable/training/zygote/#Implicit-style-(Flux-0.14)","content":" Implicit style (Flux ≤ 0.14) Flux used to use what Zygote calls \"implicit\" gradients,  described here  in its documentation. However, support for this will be removed from Flux 0.15. Training The blue-green boxes in the  training section  describe the changes needed to upgrade old code from implicit to explicit style."},{"id":420,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.gradient","ref":"/flux/stable/training/zygote/#Zygote.gradient-Tuple{Any, Params}","content":" Zygote.gradient  —  Method gradient(f, args...) Returns a tuple containing  ∂f/∂x  for each argument  x , the derivative (for scalar  x ) or the gradient. f(args...)  must be a real number, see  jacobian  for array output. See also  withgradient  to keep the value  f(args...) , and  pullback  for value and back-propagator. julia> gradient(*, 2.0, 3.0, 5.0)\n(15.0, 10.0, 6.0)\n\njulia> gradient(x -> sum(abs2,x), [7.0, 11.0, 13.0])\n([14.0, 22.0, 26.0],)\n\njulia> gradient([7, 11], 0, 1) do x, y, d\n         p = size(x, d)\n         sum(x.^p .+ y)\n       end\n([14.0, 22.0], 2.0, nothing)"},{"id":421,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.Params","ref":"/flux/stable/training/zygote/#Zygote.Params","content":" Zygote.Params  —  Type Params([A, B]) Container for implicit parameters, used when differentiating a zero-argument function  () -> loss(A, B)  with respect to  A, B ."},{"id":422,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.Grads","ref":"/flux/stable/training/zygote/#Zygote.Grads","content":" Zygote.Grads  —  Type Grads(...) Dictionary-like container returned when taking gradients with respect to implicit parameters. For an array  W , appearing  within  Params([W, A, B...]) , the gradient is  g[W] ."},{"id":423,"pagetitle":"Gradients – Zygote.jl","title":"Zygote.jacobian","ref":"/flux/stable/training/zygote/#Zygote.jacobian-Tuple{Any, Params}","content":" Zygote.jacobian  —  Method jacobian(loss, ::Params) Like  gradient  with implicit parameters, this method takes a zero-argument function and returns an  IdDict -like object, now containing the Jacobian for each parameter. Examples julia> xs = [1 2; 3 4]; ys = [5,7,9];\n\njulia> Jxy = jacobian(() -> ys[1:2] .+ sum(xs.^2), Params([xs, ys]))\nGrads(...)\n\njulia> Jxy[ys]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  1  0\n\njulia> Jxy[xs]\n2×4 Matrix{Int64}:\n 2  6  4  8\n 2  6  4  8"},{"id":424,"pagetitle":"Gradients – Zygote.jl","title":"ChainRules","ref":"/flux/stable/training/zygote/#ChainRules","content":" ChainRules Sometimes it is necessary to exclude some code, or a whole function, from automatic differentiation. This can be done using  ChainRules :"},{"id":425,"pagetitle":"Gradients – Zygote.jl","title":"ChainRulesCore.ignore_derivatives","ref":"/flux/stable/training/zygote/#ChainRulesCore.ignore_derivatives","content":" ChainRulesCore.ignore_derivatives  —  Function ignore_derivatives(f::Function) Tells the AD system to ignore the gradients of the wrapped closure. The primal computation (forward pass) is executed normally. ignore_derivatives() do\n    value = rand()\n    push!(collection, value)\nend Using this incorrectly could lead to incorrect gradients. For example, the following function will have zero gradients with respect to its argument: function wrong_grads(x)\n    y = ones(3)\n    ignore_derivatives() do\n        push!(y, x)\n    end\n    return sum(y)\nend ignore_derivatives(x) Tells the AD system to ignore the gradients of the argument. Can be used to avoid unnecessary computation of gradients. ignore_derivatives(x) * w"},{"id":426,"pagetitle":"Gradients – Zygote.jl","title":"ChainRulesCore.@non_differentiable","ref":"/flux/stable/training/zygote/#ChainRulesCore.@non_differentiable","content":" ChainRulesCore.@non_differentiable  —  Macro @non_differentiable(signature_expression) A helper to make it easier to declare that a method is not differentiable. This is a short-hand for defining an  frule  and  rrule  that return  NoTangent()  for all partials (even for the function  s̄elf -partial itself) Keyword arguments should not be included. julia> @non_differentiable Base.:(==)(a, b)\n\njulia> _, pullback = rrule(==, 2.0, 3.0);\n\njulia> pullback(1.0)\n(NoTangent(), NoTangent(), NoTangent()) You can place type-constraints in the signature: julia> @non_differentiable Base.length(xs::Union{Number, Array})\n\njulia> frule((ZeroTangent(), 1), length, [2.0, 3.0])\n(2, NoTangent()) Warning This helper macro covers only the simple common cases. It does not support  where -clauses. For these you can declare the  rrule  and  frule  directly To manually supply the gradient for one function, you should define a method of  rrule . ChainRules has  detailed documentation  on how this works."},{"id":427,"pagetitle":"Gradients – Zygote.jl","title":"ChainRulesCore.rrule","ref":"/flux/stable/training/zygote/#ChainRulesCore.rrule","content":" ChainRulesCore.rrule  —  Function rrule([::RuleConfig,] f, x...) Expressing  x  as the tuple  (x₁, x₂, ...)  and the output tuple of  f(x...)  as  Ω , return the tuple: (Ω, (Ω̄₁, Ω̄₂, ...) -> (s̄elf, x̄₁, x̄₂, ...)) Where the second return value is the the propagation rule or pullback. It takes in cotangents corresponding to the outputs ( x̄₁, x̄₂, ... ), and  s̄elf , the internal values of the function itself (for closures) If no method matching  rrule(f, xs...)  has been defined, then return  nothing . Examples: unary input, unary output scalar function: julia> x = rand();\n\njulia> sinx, sin_pullback = rrule(sin, x);\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pullback(1) == (NoTangent(), cos(x))\ntrue binary input, unary output scalar function: julia> x, y = rand(2);\n\njulia> hypotxy, hypot_pullback = rrule(hypot, x, y);\n\njulia> hypotxy == hypot(x, y)\ntrue\n\njulia> hypot_pullback(1) == (NoTangent(), (x / hypot(x, y)), (y / hypot(x, y)))\ntrue The optional  RuleConfig  option allows specifying rrules only for AD systems that support given features. If not needed, then it can be omitted and the  rrule  without it will be hit as a fallback. This is the case for most rules. See also:  frule ,  @scalar_rule ,  RuleConfig"},{"id":428,"pagetitle":"Gradients – Zygote.jl","title":"ChainRulesCore.frule","ref":"/flux/stable/training/zygote/#ChainRulesCore.frule","content":" ChainRulesCore.frule  —  Function frule([::RuleConfig,] (Δf, Δx...), f, x...) Expressing the output of  f(x...)  as  Ω , return the tuple: (Ω, ΔΩ) The second return value is the tangent w.r.t. the output. If no method matching  frule((Δf, Δx...), f, x...)  has been defined, then return  nothing . Examples: unary input, unary output scalar function: julia> dself = NoTangent();\n\njulia> x = rand()\n0.8236475079774124\n\njulia> sinx, Δsinx = frule((dself, 1), sin, x)\n(0.7336293678134624, 0.6795498147167869)\n\njulia> sinx == sin(x)\ntrue\n\njulia> Δsinx == cos(x)\ntrue Unary input, binary output scalar function: julia> sincosx, Δsincosx = frule((dself, 1), sincos, x);\n\njulia> sincosx == sincos(x)\ntrue\n\njulia> Δsincosx[1] == cos(x)\ntrue\n\njulia> Δsincosx[2] == -sin(x)\ntrue Note that techically speaking julia does not have multiple output functions, just functions that return a single output that is iterable, like a  Tuple . So this is actually a  Tangent : julia> Δsincosx\nTangent{Tuple{Float64, Float64}}(0.6795498147167869, -0.7336293678134624) The optional  RuleConfig  option allows specifying frules only for AD systems that support given features. If not needed, then it can be omitted and the  frule  without it will be hit as a fallback. This is the case for most rules. See also:  rrule ,  @scalar_rule ,  RuleConfig"},{"id":429,"pagetitle":"Gradients – Zygote.jl","title":"ChainRulesCore.@scalar_rule","ref":"/flux/stable/training/zygote/#ChainRulesCore.@scalar_rule","content":" ChainRulesCore.@scalar_rule  —  Macro @scalar_rule(f(x₁, x₂, ...),\n             @setup(statement₁, statement₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...) A convenience macro that generates simple scalar forward or reverse rules using the provided partial derivatives. Specifically, generates the corresponding methods for  frule  and  rrule : function ChainRulesCore.frule((NoTangent(), Δx₁, Δx₂, ...), ::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, (\n            (∂f₁_∂x₁ * Δx₁ + ∂f₁_∂x₂ * Δx₂ + ...),\n            (∂f₂_∂x₁ * Δx₁ + ∂f₂_∂x₂ * Δx₂ + ...),\n            ...\n        )\nend\n\nfunction ChainRulesCore.rrule(::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, ((ΔΩ₁, ΔΩ₂, ...)) -> (\n            NoTangent(),\n            ∂f₁_∂x₁ * ΔΩ₁ + ∂f₂_∂x₁ * ΔΩ₂ + ...),\n            ∂f₁_∂x₂ * ΔΩ₁ + ∂f₂_∂x₂ * ΔΩ₂ + ...),\n            ...\n        )\nend If no type constraints in  f(x₁, x₂, ...)  within the call to  @scalar_rule  are provided, each parameter in the resulting  frule / rrule  definition is given a type constraint of  Number . Constraints may also be explicitly be provided to override the  Number  constraint, e.g.  f(x₁::Complex, x₂) , which will constrain  x₁  to  Complex  and  x₂  to  Number . At present this does not support defining for closures/functors. Thus in reverse-mode, the first returned partial, representing the derivative with respect to the function itself, is always  NoTangent() . And in forward-mode, the first input to the returned propagator is always ignored. The result of  f(x₁, x₂, ...)  is automatically bound to  Ω . This allows the primal result to be conveniently referenced (as  Ω ) within the derivative/setup expressions. This macro assumes complex functions are holomorphic. In general, for non-holomorphic functions, the  frule  and  rrule  must be defined manually. If the derivative is one, (e.g. for identity functions)  true  can be used as the most general multiplicative identity. The  @setup  argument can be elided if no setup code is need. In other words: @scalar_rule(f(x₁, x₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...) is equivalent to: @scalar_rule(f(x₁, x₂, ...),\n             @setup(nothing),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...) For examples, see ChainRules'  rulesets  directory. See also:  frule ,  rrule ."},{"id":430,"pagetitle":"Gradients – Zygote.jl","title":"ChainRulesCore.NoTangent","ref":"/flux/stable/training/zygote/#ChainRulesCore.NoTangent","content":" ChainRulesCore.NoTangent  —  Type NoTangent() <: AbstractZero This tangent indicates that the derivative does not exist. It is the tangent type for primal types that are not differentiable, such as integers or booleans (when they are not being used to represent floating-point values). The only valid way to perturb such values is to not change them at all. As a consequence,  NoTangent  is functionally identical to  ZeroTangent() , but it provides additional semantic information. Adding  NoTangent()  to a primal is generally wrong: gradient-based methods cannot be used to optimize over discrete variables. An optimization package making use of this might want to check for such a case. Note This does not indicate that the derivative is not implemented, but rather that mathematically it is not defined. This mostly shows up as the derivative with respect to dimension, index, or size arguments.     function rrule(fill, x, len::Int)\n        y = fill(x, len)\n        fill_pullback(ȳ) = (NoTangent(), @thunk(sum(Ȳ)), NoTangent())\n        return y, fill_pullback\n    end"},{"id":431,"pagetitle":"Gradients – Zygote.jl","title":"ChainRulesCore.ZeroTangent","ref":"/flux/stable/training/zygote/#ChainRulesCore.ZeroTangent","content":" ChainRulesCore.ZeroTangent  —  Type ZeroTangent() <: AbstractZero The additive identity for tangents. This is basically the same as  0 . A derivative of  ZeroTangent()  does not propagate through the primal function."},{"id":434,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Deep Learning with Julia & Flux: A 60 Minute Blitz","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#man-blitz","content":" Deep Learning with Julia & Flux: A 60 Minute Blitz This is a quick intro to  Flux  loosely based on  PyTorch's tutorial . It introduces basic Julia programming, as well Zygote, a source-to-source automatic differentiation (AD) framework in Julia. We'll use these tools to build a very simple neural network."},{"id":435,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Arrays","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Arrays","content":" Arrays The starting point for all of our models is the  Array  (sometimes referred to as a  Tensor  in other frameworks). This is really just a list of numbers, which might be arranged into a shape like a square. Let's write down an array with three elements. x = [1, 2, 3] Here's a matrix – a square array with four elements. x = [1 2; 3 4] We often work with arrays of thousands of elements, and don't usually write them down by hand. Here's how we can create an array of 5×3 = 15 elements, each a random number from zero to one. x = rand(5, 3) There's a few functions like this; try replacing  rand  with  ones ,  zeros , or  randn  to see what they do. By default, Julia works stores numbers is a high-precision format called  Float64 . In ML we often don't need all those digits, and can ask Julia to work with  Float32  instead. We can even ask for more digits using  BigFloat . x = rand(BigFloat, 5, 3)\n\nx = rand(Float32, 5, 3) We can ask the array how many elements it has. length(x) Or, more specifically, what size it has. size(x) We sometimes want to see some elements of the array on their own. x\n\nx[2, 3] This means get the second row and the third column. We can also get every row of the third column. x[:, 3] We can add arrays, and subtract them, which adds or subtracts each element of the array. x + x\n\nx - x Julia supports a feature called  broadcasting , using the  .  syntax. This tiles small arrays (or single numbers) to fill bigger ones. x .+ 1 We can see Julia tile the column vector  1:5  across all rows of the larger array. zeros(5,5) .+ (1:5) The x' syntax is used to transpose a column  1:5  into an equivalent row, and Julia will tile that across columns. zeros(5,5) .+ (1:5)' We can use this to make a times table. (1:5) .* (1:5)' Finally, and importantly for machine learning, we can conveniently do things like matrix multiply. W = randn(5, 10)\nx = rand(10)\nW * x Julia's arrays are very powerful, and you can learn more about what they can do  here ."},{"id":436,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"CUDA Arrays","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#CUDA-Arrays","content":" CUDA Arrays CUDA functionality is provided separately by the  CUDA package . If you have a GPU and CUDA available, you can run  ] add CUDA  in a REPL or IJulia to get it. Once CUDA is loaded you can move any array to the GPU with the  cu  function, and it supports all of the above operations with the same syntax. using CUDA\nx = cu(rand(5, 3))"},{"id":437,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Automatic Differentiation","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Automatic-Differentiation","content":" Automatic Differentiation You probably learned to take derivatives in school. We start with a simple mathematical function like f(x) = 3x^2 + 2x + 1\n\nf(5) In simple cases it's pretty easy to work out the gradient by hand – here it's  6x+2 . But it's much easier to make Flux do the work for us! using Flux: gradient\n\ndf(x) = gradient(f, x)[1]\n\ndf(5) You can try this with a few different inputs to make sure it's really the same as  6x+2 . We can even do this multiple times (but the second derivative is a fairly boring  6 ). ddf(x) = gradient(df, x)[1]\n\nddf(5) Flux's AD can handle any Julia code you throw at it, including loops, recursion and custom layers, so long as the mathematical functions you call are differentiable. For example, we can differentiate a Taylor approximation to the  sin  function. mysin(x) = sum((-1)^k*x^(1+2k)/factorial(1+2k) for k in 0:5)\n\nx = 0.5\n\nmysin(x), gradient(mysin, x)\n\nsin(x), cos(x) You can see that the derivative we calculated is very close to  cos(x) , as we expect. This gets more interesting when we consider functions that take  arrays  as inputs, rather than just a single number. For example, here's a function that takes a matrix and two vectors (the definition itself is arbitrary) myloss(W, b, x) = sum(W * x .+ b)\n\nW = randn(3, 5)\nb = zeros(3)\nx = rand(5)\n\ngradient(myloss, W, b, x) Now we get gradients for each of the inputs  W ,  b  and  x , which will come in handy when we want to train models. Because ML models can contain hundreds of parameters, Flux provides a slightly different way of writing  gradient . We instead mark arrays with  param  to indicate that we want their derivatives.  W  and  b  represent the weight and bias respectively. using Flux: params\n\nW = randn(3, 5)\nb = zeros(3)\nx = rand(5)\n\ny(x) = sum(W * x .+ b)\n\ngrads = gradient(()->y(x), params([W, b]))\n\ngrads[W], grads[b] We can now grab the gradients of  W  and  b  directly from those parameters. This comes in handy when working with  layers . A layer is just a handy container for some parameters. For example,  Dense  does a linear transform for you. using Flux\n\nm = Dense(10, 5)\n\nx = rand(Float32, 10) We can easily get the parameters of any layer or model with params with  params . params(m) This makes it very easy to calculate the gradient for all parameters in a network, even if it has many parameters. x = rand(Float32, 10)\nm = Chain(Dense(10, 5, relu), Dense(5, 2), softmax)\nl(x) = sum(Flux.crossentropy(m(x), [0.5, 0.5]))\ngrads = gradient(params(m)) do\n    l(x)\nend\nfor p in params(m)\n    println(grads[p])\nend You don't have to use layers, but they can be convient for many simple kinds of models and fast iteration. The next step is to update our weights and perform optimisation. As you might be familiar,  Gradient Descent  is a simple algorithm that takes the weights and steps using a learning rate and the gradients.  weights = weights - learning_rate * gradient . using Flux.Optimise: update!, Descent\nη = 0.1\nfor p in params(m)\n  update!(p, -η * grads[p])\nend While this is a valid way of updating our weights, it can get more complicated as the algorithms we use get more involved. Flux comes with a bunch of pre-defined optimisers and makes writing our own really simple. We just give it the learning rate η: opt = Descent(0.01) Training  a network reduces down to iterating on a dataset mulitple times, performing these steps in order. Just for a quick implementation, let’s train a network that learns to predict  0.5  for every input of 10 floats.  Flux  defines the  train!  function to do it for us. data, labels = rand(10, 100), fill(0.5, 2, 100)\nloss(x, y) = sum(Flux.crossentropy(m(x), y))\nFlux.train!(loss, params(m), [(data,labels)], opt) You don't have to use  train! . In cases where arbitrary logic might be better suited, you could open up this training loop like so:   for d in training_set # assuming d looks like (data, labels)\n    # our super logic\n    gs = gradient(params(m)) do #m is our model\n      l = loss(d...)\n    end\n    update!(opt, params(m), gs)\n  end"},{"id":438,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Training a Classifier","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Training-a-Classifier","content":" Training a Classifier Getting a real classifier to work might help cement the workflow a bit more.  CIFAR10  is a dataset of 50k tiny training images split into 10 classes. We will do the following steps in order: Load CIFAR10 training and test datasets Define a Convolution Neural Network Define a loss function Train the network on the training data Test the network on the test data"},{"id":439,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Loading the Dataset","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Loading-the-Dataset","content":" Loading the Dataset using Statistics\nusing Flux, Flux.Optimise\nusing MLDatasets: CIFAR10\nusing Images.ImageCore\nusing Flux: onehotbatch, onecold\nusing Base.Iterators: partition\nusing CUDA This image will give us an idea of what we are dealing with.  train_x, train_y = CIFAR10.traindata(Float32)\nlabels = onehotbatch(train_y, 0:9) The  train_x  contains 50000 images converted to 32 X 32 X 3 arrays with the third dimension being the 3 channels (R,G,B). Let's take a look at a random image from the train_x. For this, we need to permute the dimensions to 3 X 32 X 32 and use  colorview  to convert it back to an image.  using Plots\nimage(x) = colorview(RGB, permutedims(x, (3, 2, 1)))\nplot(image(train_x[:,:,:,rand(1:end)])) We can now arrange the training data in batches of say, 1000 and keep a validation set to track our progress. This process is called minibatch learning, which is a popular method of training large neural networks. Rather that sending the entire dataset at once, we break it down into smaller chunks (called minibatches) that are typically chosen at random, and train only on them. It is shown to help with escaping  saddle points . The first 49k images (in batches of 1000) will be our training set, and the rest is for validation.  partition  handily breaks down the set we give it in consecutive parts (1000 in this case). train = ([(train_x[:,:,:,i], labels[:,i]) for i in partition(1:49000, 1000)]) |> gpu\nvalset = 49001:50000\nvalX = train_x[:,:,:,valset] |> gpu\nvalY = labels[:, valset] |> gpu"},{"id":440,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Defining the Classifier","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Defining-the-Classifier","content":" Defining the Classifier Now we can define our Convolutional Neural Network (CNN). A convolutional neural network is one which defines a kernel and slides it across a matrix to create an intermediate representation to extract features from. It creates higher order features as it goes into deeper layers, making it suitable for images, where the strucure of the subject is what will help us determine which class it belongs to. m = Chain(\n  Conv((5,5), 3=>16, relu),\n  MaxPool((2,2)),\n  Conv((5,5), 16=>8, relu),\n  MaxPool((2,2)),\n  x -> reshape(x, :, size(x, 4)),\n  Dense(200, 120),\n  Dense(120, 84),\n  Dense(84, 10),\n  softmax) |> gpu We will use a crossentropy loss and an Momentum optimiser here. Crossentropy will be a good option when it comes to working with mulitple independent classes. Momentum gradually lowers the learning rate as we proceed with the training. It helps maintain a bit of adaptivity in our optimisation, preventing us from over shooting from our desired destination. using Flux: crossentropy, Momentum\n\nloss(x, y) = sum(crossentropy(m(x), y))\nopt = Momentum(0.01) We can start writing our train loop where we will keep track of some basic accuracy numbers about our model. We can define an  accuracy  function for it like so. accuracy(x, y) = mean(onecold(m(x), 0:9) .== onecold(y, 0:9))"},{"id":441,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Training the Classifier","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Training-the-Classifier","content":" Training the Classifier Training is where we do a bunch of the interesting operations we defined earlier, and see what our net is capable of. We will loop over the dataset 10 times and feed the inputs to the neural network and optimise. epochs = 10\n\nfor epoch = 1:epochs\n  for d in train\n    gs = gradient(params(m)) do\n      l = loss(d...)\n    end\n    update!(opt, params(m), gs)\n  end\n  @show accuracy(valX, valY)\nend Seeing our training routine unfold gives us an idea of how the network learnt the function. This is not bad for a small hand-written network, trained for a limited time."},{"id":442,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Training on a GPU","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Training-on-a-GPU","content":" Training on a GPU The  gpu  functions you see sprinkled through this bit of the code tell Flux to move these entities to an available GPU, and subsequently train on it. No extra faffing about required! The same bit of code would work on any hardware with some small annotations like you saw here."},{"id":443,"pagetitle":"Deep Learning with Julia & Flux: A 60 Minute Blitz","title":"Testing the Network","ref":"/flux/stable/tutorials/2020-09-15-deep-learning-flux/#Testing-the-Network","content":" Testing the Network We have trained the network for 100 passes over the training dataset. But we need to check if the network has learnt anything at all. We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions. This will be done on a yet unseen section of data. Okay, first step. Let us perform the exact same preprocessing on this set, as we did on our training set. test_x, test_y = CIFAR10.testdata(Float32)\ntest_labels = onehotbatch(test_y, 0:9)\n\ntest = gpu.([(test_x[:,:,:,i], test_labels[:,i]) for i in partition(1:10000, 1000)]) Next, display an image from the test set. plot(image(test_x[:,:,:,rand(1:end)])) The outputs are energies for the 10 classes. Higher the energy for a class, the more the network thinks that the image is of the particular class. Every column corresponds to the output of one image, with the 10 floats in the column being the energies. Let's see how the model fared. ids = rand(1:10000, 5)\nrand_test = test_x[:,:,:,ids] |> gpu\nrand_truth = test_y[ids]\nm(rand_test) This looks similar to how we would expect the results to be. At this point, it's a good idea to see how our net actually performs on new data, that we have prepared. accuracy(test[1]...) This is much better than random chance set at 10% (since we only have 10 classes), and not bad at all for a small hand written network like ours. Let's take a look at how the net performed on all the classes performed individually. class_correct = zeros(10)\nclass_total = zeros(10)\nfor i in 1:10\n  preds = m(test[i][1])\n  lab = test[i][2]\n  for j = 1:1000\n    pred_class = findmax(preds[:, j])[2]\n    actual_class = findmax(lab[:, j])[2]\n    if pred_class == actual_class\n      class_correct[pred_class] += 1\n    end\n    class_total[actual_class] += 1\n  end\nend\n\nclass_correct ./ class_total The spread seems pretty good, with certain classes performing significantly better than the others. Why should that be? Info Originally published at  fluxml.ai  on 15 November 2020. Written by Saswat Das, Mike Innes, Andrew Dinhobl, Ygor Canalli, Sudhanshu Agrawal, João Felipe Santos."},{"id":446,"pagetitle":"Tutorial: Simple Multi-layer Perceptron","title":"Tutorial: Simple Multi-layer Perceptron","ref":"/flux/stable/tutorials/2021-01-26-mlp/#man-mlp-tutorial","content":" Tutorial: Simple Multi-layer Perceptron In this example, we create a simple  multi-layer perceptron  (MLP) that classifies handwritten digits using the MNIST dataset. A MLP consists of at least  three layers  of stacked perceptrons: Input, hidden, and output. Each neuron of an MLP has parameters (weights and bias) and uses an  activation function  to compute its output.  To run this example, we need the following packages: using Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, logitcrossentropy, throttle, params\nusing Base.Iterators: repeated\nusing CUDA\nusing MLDatasets\nif has_cuda()\t\t# Check if CUDA is available\n    @info \"CUDA is on\"\n    CUDA.allowscalar(false)\nend We set default values for learning rate, batch size, epochs, and the usage of a GPU (if available) for our model: Base.@kwdef mutable struct Args\n    rate::Float64 = 3e-4    # learning rate\n    batchsize::Int = 1024   # batch size\n    epochs::Int = 10        # number of epochs\n    device::Function = gpu  # set as gpu, if gpu available\nend If a GPU is available on our local system, then Flux uses it for computing the loss and updating the weights and biases when training our model."},{"id":447,"pagetitle":"Tutorial: Simple Multi-layer Perceptron","title":"Data","ref":"/flux/stable/tutorials/2021-01-26-mlp/#Data","content":" Data We create the function  getdata  to load the MNIST train and test data sets from  MLDatasets  and prepare them for the training process. In addition, we set mini-batches of the data sets by loading them onto a  DataLoader  object.  function getdata(args)\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n    # Loading Dataset\t\n    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)\n    xtest, ytest = MLDatasets.MNIST.testdata(Float32)\n\t\n    # Reshape Data in order to flatten each image into a linear array\n    xtrain = Flux.flatten(xtrain)\n    xtest = Flux.flatten(xtest)\n\n    # One-hot-encode the labels\n    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n\n    # Batching\n    train_data = DataLoader((xtrain, ytrain), batchsize=args.batchsize, shuffle=true)\n    test_data = DataLoader((xtest, ytest), batchsize=args.batchsize)\n\n    return train_data, test_data\nend getdata  performs the following steps: Loads MNIST data set:  Loads the train and test set tensors. The shape of train data is  28x28x60000  and test data is  28X28X10000 .  Reshapes the train and test data:   Uses the  flatten  function to reshape the train data set into a  784x60000  array and test data set into a  784x10000 . Notice that we reshape the data so that we can pass these as arguments for the input layer of our model (a simple MLP expects a vector as an input). One-hot encodes the train and test labels:  Creates a batch of one-hot vectors so we can pass the labels of the data as arguments for the loss function. For this example, we use the  logitcrossentropy  function and it expects data to be one-hot encoded.  Creates batches of data:  Creates two DataLoader objects (train and test) that handle data mini-batches of size  1024  (as defined above). We create these two objects so that we can pass the entire data set through the loss function at once when training our model. Also, it shuffles the data points during each iteration ( shuffle=true )."},{"id":448,"pagetitle":"Tutorial: Simple Multi-layer Perceptron","title":"Model","ref":"/flux/stable/tutorials/2021-01-26-mlp/#Model","content":" Model As we mentioned above, a MLP consist of  three  layers that are fully connected. For this example, we define out model with the following layers and dimensions:  Input:  It has  784  perceptrons (the MNIST image size is  28x28 ). We flatten the train and test data so that we can pass them as arguments to this layer. Hidden:  It has  32  perceptrons that use the  relu  activation function. Output:  It has  10  perceptrons that output the model's prediction or probability that a digit is 0 to 9.  We define our model with the  build_model  function:  function build_model(; imgsize=(28,28,1), nclasses=10)\n    return Chain(\n \t    Dense(prod(imgsize), 32, relu),\n            Dense(32, nclasses))\nend Note that we use the functions  Dense  so that our model is  densely  (or fully) connected and  Chain  to chain the computation of the three layers."},{"id":449,"pagetitle":"Tutorial: Simple Multi-layer Perceptron","title":"Loss functions","ref":"/flux/stable/tutorials/2021-01-26-mlp/#Loss-functions","content":" Loss functions Now, we define the loss function  loss_all . It expects a DataLoader object and the  model  function we defined aboved as arguments. Notice that this function iterates through the  dataloader  object in mini-batches and uses the function  logitcrossentropy  to compute the difference between the predicted and actual values.  function loss_all(dataloader, model)\n    l = 0f0\n    for (x,y) in dataloader\n        l += logitcrossentropy(model(x), y)\n    end\n    l/length(dataloader)\nend In addition, we define the function ( accuracy ) to report the accuracy of our model during the training process. To compute the accuray, we need to decode the output of our model using the  onecold  function.  function accuracy(data_loader, model)\n    acc = 0\n    for (x,y) in data_loader\n        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))*1 / size(x,2)\n    end\n    acc/length(data_loader)\nend"},{"id":450,"pagetitle":"Tutorial: Simple Multi-layer Perceptron","title":"Train our model","ref":"/flux/stable/tutorials/2021-01-26-mlp/#Train-our-model","content":" Train our model Finally, we create the  train  function that calls the functions we defined and trains the model. function train(; kws...)\n    # Initializing Model parameters \n    args = Args(; kws...)\n\n    # Load Data\n    train_data,test_data = getdata(args)\n\n    # Construct model\n    m = build_model()\n    train_data = args.device.(train_data)\n    test_data = args.device.(test_data)\n    m = args.device(m)\n    loss(x,y) = logitcrossentropy(m(x), y)\n    \n    ## Training\n    evalcb = () -> @show(loss_all(train_data, m))\n    opt = Adam(args.rate)\n\t\n    for epoch in 1:args.epochs\n        @info \"Epoch $epoch\"\n        Flux.train!(loss, params(m), train_data, opt, cb = evalcb)\n    end\n\n    @show accuracy(train_data, m)\n\n    @show accuracy(test_data, m)\nend train  performs the following steps: Initializes the model parameters:  Creates the  args  object that contains the defult values for training our model. Loads the train and test data:  Calls the function  getdata  we defined above. Constructs the model:  Builds the model and loads the train and test data sets, and our model  onto the GPU (if available). Trains the model:  Defines the  callback  function  evalcb  to show the value of the  loss_all  function during the training process. Then, it sets  Adam  as the optimiser for training out model. Finally, it runs the training process for  10  epochs (as defined in the  args  object) and shows the  accuracy  value for the train and test data. To see the full version of this example, see  Simple multi-layer perceptron - model-zoo ."},{"id":451,"pagetitle":"Tutorial: Simple Multi-layer Perceptron","title":"Resources","ref":"/flux/stable/tutorials/2021-01-26-mlp/#Resources","content":" Resources 3Blue1Brown Neural networks videos . Neural Networks and Deep Learning . Info Originally published at  fluxml.ai  on 26 January 2021. Written by Adarsh Kumar, Mike J Innes, Andrew Dinhobl, Jerry Ling, natema, Zhang Shitian, Liliana Badillo, Dhairya Gandhi"},{"id":454,"pagetitle":"Tutorial: A Simple ConvNet","title":"Tutorial: A Simple ConvNet","ref":"/flux/stable/tutorials/2021-02-07-convnet/#man-convnet-tutorial","content":" Tutorial: A Simple ConvNet In this tutorial, we build a simple Convolutional Neural Network (ConvNet) to classify the MNIST dataset. This model has a simple architecture with three feature detection layers (Conv -> ReLU -> MaxPool) followed by a final dense layer that classifies MNIST handwritten digits. Note that this model, while simple, should hit around 99% test accuracy after training for approximately 20 epochs. This example writes out the saved model to the file  mnist_conv.bson . Also, it demonstrates basic model construction, training, saving, conditional early-exit, and learning rate scheduling. To run this example, we need the following packages: using Flux, MLDatasets, Statistics\nusing Flux: onehotbatch, onecold, logitcrossentropy, params\nusing MLDatasets: MNIST\nusing Base.Iterators: partition\nusing Printf, BSON\nusing CUDA\nCUDA.allowscalar(false) We set default values for learning rate, batch size, number of epochs, and path for saving the file  mnist_conv.bson : Base.@kwdef mutable struct TrainArgs\n   lr::Float64 = 3e-3\n   epochs::Int = 20\n   batch_size = 128\n   savepath::String = \"./\"\nend"},{"id":455,"pagetitle":"Tutorial: A Simple ConvNet","title":"Data","ref":"/flux/stable/tutorials/2021-02-07-convnet/#Data","content":" Data To train our model, we need to bundle images together with their labels and group them into mini-batches (makes the training process faster). We define the function  make_minibatch  that takes as inputs the images ( X ) and their labels ( Y ) as well as the indices for the mini-batches ( idx ): function make_minibatch(X, Y, idxs)\n   X_batch = Array{Float32}(undef, size(X)[1:end-1]..., 1, length(idxs))\n   for i in 1:length(idxs)\n       X_batch[:, :, :, i] = Float32.(X[:,:,idxs[i]])\n   end\n   Y_batch = onehotbatch(Y[idxs], 0:9)\n   return (X_batch, Y_batch)\nend make_minibatch  takes the following steps: Creates the  X_batch  array of size  28x28x1x128  to store the mini-batches.  Stores the mini-batches in  X_batch . One hot encodes the labels of the images. Stores the labels in  Y_batch . get_processed_data  loads the train and test data from  Flux.Data.MNIST . First, it loads the images and labels of the train data set, and creates an array that contains the indices of the train images that correspond to each mini-batch (of size  args.batch_size ). Then, it calls the  make_minibatch  function to create all of the train mini-batches. Finally, it loads the test images and creates one mini-batch that contains them all. function get_processed_data(args)\n   # Load labels and images\n   train_imgs, train_labels = MNIST.traindata()\n   mb_idxs = partition(1:length(train_labels), args.batch_size)\n   train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]\n  \n   # Prepare test set as one giant minibatch:\n   test_imgs, test_labels = MNIST.testdata()\n   test_set = make_minibatch(test_imgs, test_labels, 1:length(test_labels))\n \n   return train_set, test_set\n \nend"},{"id":456,"pagetitle":"Tutorial: A Simple ConvNet","title":"Model","ref":"/flux/stable/tutorials/2021-02-07-convnet/#Model","content":" Model Now, we define the  build_model  function that creates a ConvNet model which is composed of  three  convolution layers (feature detection) and  one  classification layer. The input layer size is  28x28 . The images are grayscale, which means there is only  one  channel (compared to 3 for RGB) in every data point. Combined together, the convolutional layer structure would look like  Conv(kernel, input_channels => output_channels, ...) . Each convolution layer reduces the size of the image by applying the Rectified Linear unit (ReLU) and MaxPool operations. On the other hand, the classification layer outputs a vector of 10 dimensions (a dense layer), that is, the number of classes that the model will be able to predict. function build_model(args; imgsize = (28,28,1), nclasses = 10)\n   cnn_output_size = Int.(floor.([imgsize[1]/8,imgsize[2]/8,32])) \n \n   return Chain(\n   # First convolution, operating upon a 28x28 image\n   Conv((3, 3), imgsize[3]=>16, pad=(1,1), relu),\n   MaxPool((2,2)),\n \n   # Second convolution, operating upon a 14x14 image\n   Conv((3, 3), 16=>32, pad=(1,1), relu),\n   MaxPool((2,2)),\n \n   # Third convolution, operating upon a 7x7 image\n   Conv((3, 3), 32=>32, pad=(1,1), relu),\n   MaxPool((2,2)),\n \n   # Reshape 3d array into a 2d one using `Flux.flatten`, at this point it should be (3, 3, 32, N)\n   flatten,\n   Dense(prod(cnn_output_size), 10))\nend To chain the layers of a model we use the Flux function  Chain . It enables us to call the layers in sequence on a given input. Also, we use the function  flatten  to reshape the output image from the last convolution layer. Finally, we call the  Dense  function to create the classification layer."},{"id":457,"pagetitle":"Tutorial: A Simple ConvNet","title":"Training","ref":"/flux/stable/tutorials/2021-02-07-convnet/#Training","content":" Training Before training our model, we need to define a few functions that will be helpful for the process: augment  adds gaussian random noise to our image, to make it more robust: anynan  checks whether any element of the params is NaN or not: accuracy  computes the proportion of inputs  x  correctly classified by our ConvNet: augment(x) = x .+ gpu(0.1f0*randn(eltype(x), size(x)))\nanynan(x) = any(y -> any(isnan, y), x)\naccuracy(x, y, model) = mean(onecold(cpu(model(x))) .== onecold(cpu(y))) Finally, we define the  train  function: function train(; kws...)   \n   args = TrainArgs(; kws...)\n \n   @info(\"Loading data set\")\n   train_set, test_set = get_processed_data(args)\n \n   # Define our model.  We will use a simple convolutional architecture with\n   # three iterations of Conv -> ReLU -> MaxPool, followed by a final Dense layer.\n   @info(\"Building model...\")\n   model = build_model(args)\n \n   # Load model and datasets onto GPU, if enabled\n   train_set = gpu.(train_set)\n   test_set = gpu.(test_set)\n   model = gpu(model)\n  \n   # Make sure our model is nicely precompiled before starting our training loop\n   model(train_set[1][1])\n \n   # `loss()` calculates the crossentropy loss between our prediction `y_hat`\n   # (calculated from `model(x)`) and the ground truth `y`.  We augment the data\n   # a bit, adding gaussian random noise to our image to make it more robust.\n   function loss(x, y)   \n       x̂ = augment(x)\n       ŷ = model(x̂)\n       return logitcrossentropy(ŷ, y)\n   end\n  \n   # Train our model with the given training set using the Adam optimiser and\n   # printing out performance against the test set as we go.\n   opt = Adam(args.lr)\n  \n   @info(\"Beginning training loop...\")\n   best_acc = 0.0\n   last_improvement = 0\n   for epoch_idx in 1:args.epochs\n       # Train for a single epoch\n       Flux.train!(loss, params(model), train_set, opt)\n      \n       # Terminate on NaN\n       if anynan(Flux.params(model))\n           @error \"NaN params\"\n           break\n       end\n  \n       # Calculate accuracy:\n       acc = accuracy(test_set..., model)\n      \n       @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n       # If our accuracy is good enough, quit out.\n       if acc >= 0.999\n           @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n           break\n       end\n  \n       # If this is the best accuracy we've seen so far, save the model out\n       if acc >= best_acc\n           @info(\" -> New best accuracy! Saving model out to mnist_conv.bson\")\n           BSON.@save joinpath(args.savepath, \"mnist_conv.bson\") params=cpu.(params(model)) epoch_idx acc\n           best_acc = acc\n           last_improvement = epoch_idx\n       end\n  \n       # If we haven't seen improvement in 5 epochs, drop our learning rate:\n       if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n           opt.eta /= 10.0\n           @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n \n           # After dropping learning rate, give it a few epochs to improve\n           last_improvement = epoch_idx\n       end\n  \n       if epoch_idx - last_improvement >= 10\n           @warn(\" -> We're calling this converged.\")\n           break\n       end\n   end\nend train  calls the functions we defined above and trains our model. It stops when the model achieves 99% accuracy (early-exiting) or after performing 20 steps. More specifically, it performs the following steps: Loads the MNIST dataset. Builds our ConvNet model (as described above). Loads the train and test data sets as well as our model onto a GPU (if available). Defines a  loss  function that calculates the crossentropy between our prediction and the ground truth. Sets the  Adam optimiser  to train the model with learning rate  args.lr . Runs the training loop. For each step (or epoch), it executes the following: Calls  Flux.train!  function to execute one training step. If any of the parameters of our model is  NaN , then the training process is terminated. Calculates the model accuracy. If the model accuracy is >= 0.999, then early-exiting is executed. If the actual accuracy is the best so far, then the model is saved to  mnist_conv.bson . Also, the new best accuracy and the current epoch is saved. If there has not been any improvement for the last 5 epochs, then the learning rate is dropped and the process waits a little longer for the accuracy to improve. If the last improvement was more than 10 epochs ago, then the process is terminated."},{"id":458,"pagetitle":"Tutorial: A Simple ConvNet","title":"Testing","ref":"/flux/stable/tutorials/2021-02-07-convnet/#Testing","content":" Testing Finally, to test our model we define the  test  function:  function test(; kws...)\n   args = TrainArgs(; kws...)\n  \n   # Loading the test data\n   _,test_set = get_processed_data(args)\n  \n   # Re-constructing the model with random initial weights\n   model = build_model(args)\n  \n   # Loading the saved parameters\n   BSON.@load joinpath(args.savepath, \"mnist_conv.bson\") params\n  \n   # Loading parameters onto the model\n   Flux.loadparams!(model, params)\n  \n   test_set = gpu.(test_set)\n   model = gpu(model)\n   @show accuracy(test_set...,model)\nend test  loads the MNIST test data set, reconstructs the model, and loads the saved parameters (in  mnist_conv.bson ) onto it. Finally, it computes our model's predictions for the test set and shows the test accuracy (around 99%). To see the full version of this example, see  Simple ConvNets - model-zoo ."},{"id":459,"pagetitle":"Tutorial: A Simple ConvNet","title":"Resources","ref":"/flux/stable/tutorials/2021-02-07-convnet/#Resources","content":" Resources Neural Networks in Flux.jl with Huda Nassar (working with the MNIST dataset) Convolutional Neural Networks (CNNs / ConvNets) . Convolutional Neural Networks Tutorial in PyTorch . Info Originally published at  fluxml.ai  on 7 February 2021. Written by Elliot Saba, Adarsh Kumar, Mike J Innes, Dhairya Gandhi, Sudhanshu Agrawal, Sambit Kumar Dash, fps.io, Carlo Lucibello, Andrew Dinhobl, Liliana Badillo"},{"id":462,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Deep Convolutional Generative Adversarial Network (DCGAN)","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Deep-Convolutional-Generative-Adversarial-Network-(DCGAN)","content":" Deep Convolutional Generative Adversarial Network (DCGAN) This is a beginner level tutorial for generating images of handwritten digits using a  Deep Convolutional Generative Adversarial Network  inspired by the  TensorFlow tutorial on DCGAN ."},{"id":463,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"What are GANs?","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#What-are-GANs?","content":" What are GANs? Generative Adversarial Neural Networks or simply GANs  introduced by Goodfellow et al. is one of the most innovative ideas in modern-day machine learning. GANs are used extensively in the field of image and audio processing to generate high-quality synthetic data that can easily be passed off as real data. A GAN is composed of two sub-models - the  generator  and the  discriminator  acting against one another. The generator can be considered as an artist who draws (generates) new images that look real, whereas the discriminator is a critic who learns to tell real images apart from fakes. The GAN starts with a generator and discriminator which have very little or no idea about the underlying data. During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes. [source] This tutorial demonstrates the process of training a DC-GAN on the  MNIST dataset for handwritten digits . The following animation shows a series of images produced by the generator as it was trained for 25 epochs. The images begin as random noise, but over time, the images become increasingly similar to handwritten numbers."},{"id":464,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Setup","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Setup","content":" Setup We need to install some Julia packages before we start with our implementation of DCGAN. using Pkg\n\n# Activate a new project environment in the current directory\nPkg.activate(\".\")\n# Add the required packages to the environment\nPkg.add([\"Images\", \"Flux\", \"MLDatasets\", \"CUDA\", \"Parameters\"]) Note: Depending on your internet speed, it may take a few minutes for the packages install. After installing the libraries, load the required packages and functions: using Base.Iterators: partition\nusing Printf\nusing Statistics\nusing Random\nusing Images\nusing Flux: params, DataLoader\nusing Flux.Optimise: update!\nusing Flux.Losses: logitbinarycrossentropy\nusing MLDatasets: MNIST\nusing CUDA Now we set default values for the learning rates, batch size, epochs, the usage of a GPU (if available) and other hyperparameters for our model. Base.@kwdef struct HyperParams\n    batch_size::Int = 128\n    latent_dim::Int = 100\n    epochs::Int = 25\n    verbose_freq::Int = 1000\n    output_dim::Int = 5\n    disc_lr::Float64 = 0.0002\n    gen_lr::Float64 = 0.0002\n    device::Function = gpu\nend"},{"id":465,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Loading the data","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Loading-the-data","content":" Loading the data As mentioned before, we will be using the MNIST dataset for handwritten digits. So we begin with a simple function for loading and pre-processing the MNIST images: function load_MNIST_images(hparams)\n    images = MNIST.traintensor(Float32)\n\n    # Normalize the images to (-1, 1)\n    normalized_images = @. 2f0 * images - 1f0\n    image_tensor = reshape(normalized_images, 28, 28, 1, :)\n\n    # Create a dataloader that iterates over mini-batches of the image tensor\n    dataloader = DataLoader(image_tensor, batchsize=hparams.batch_size, shuffle=true)\n\n    return dataloader\nend To learn more about loading images in Flux, you can check out  this tutorial . Note: The data returned from the dataloader is loaded is on the CPU. To train on the GPU, we need to transfer the data to the GPU beforehand."},{"id":466,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Create the models","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Create-the-models","content":" Create the models"},{"id":467,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Generator","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Generator","content":" Generator Our generator, a.k.a. the artist, is a neural network that maps low dimensional data to a high dimensional form. This low dimensional data (seed) is generally a vector of random values sampled from a normal distribution. The high dimensional data is the generated image. The  Dense  layer is used for taking the seed as an input which is upsampled several times using the  ConvTranspose  layer until we reach the desired output size (in our case, 28x28x1). Furthermore, after each  ConvTranspose  layer, we apply the Batch Normalization to stabilize the learning process. We will be using the  relu  activation function for each layer except the output layer, where we use  tanh  activation. We will also apply the weight initialization method mentioned in the original DCGAN paper. # Function for initializing the model weights with values \n# sampled from a Gaussian distribution with μ=0 and σ=0.02\ndcgan_init(shape...) = randn(Float32, shape) * 0.02f0 function Generator(latent_dim)\n    Chain(\n        Dense(latent_dim, 7*7*256, bias=false),\n        BatchNorm(7*7*256, relu),\n\n        x -> reshape(x, 7, 7, 256, :),\n\n        ConvTranspose((5, 5), 256 => 128; stride = 1, pad = 2, init = dcgan_init, bias=false),\n        BatchNorm(128, relu),\n\n        ConvTranspose((4, 4), 128 => 64; stride = 2, pad = 1, init = dcgan_init, bias=false),\n        BatchNorm(64, relu),\n\n        # The tanh activation ensures that output is in range of (-1, 1)\n        ConvTranspose((4, 4), 64 => 1, tanh; stride = 2, pad = 1, init = dcgan_init, bias=false),\n    )\nend Time for a small test!! We create a dummy generator and feed a random vector as a seed to the generator. If our generator is initialized correctly it will return an array of size (28, 28, 1,  batch_size ). The  @assert  macro in Julia will raise an exception for the wrong output size. # Create a dummy generator of latent dim 100\ngenerator = Generator(100)\nnoise = randn(Float32, 100, 3) # The last axis is the batch size\n\n# Feed the random noise to the generator\ngen_image = generator(noise)\n@assert size(gen_image) == (28, 28, 1, 3) Our generator model is yet to learn the correct weights, so it does not produce a recognizable image for now. To train our poor generator we need its equal rival, the  discriminator ."},{"id":468,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Discriminator","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Discriminator","content":" Discriminator The Discriminator is a simple CNN based image classifier. The  Conv  layer a is used with a  leakyrelu  activation function.  function Discriminator()\n    Chain(\n        Conv((4, 4), 1 => 64; stride = 2, pad = 1, init = dcgan_init),\n        x->leakyrelu.(x, 0.2f0),\n        Dropout(0.3),\n\n        Conv((4, 4), 64 => 128; stride = 2, pad = 1, init = dcgan_init),\n        x->leakyrelu.(x, 0.2f0),\n        Dropout(0.3),\n\n        # The output is now of the shape (7, 7, 128, batch_size)\n        flatten,\n        Dense(7 * 7 * 128, 1) \n    )\nend For a more detailed implementation of a CNN-based image classifier, you can refer to  this tutorial . Now let us check if our discriminator is working: # Dummy Discriminator\ndiscriminator = Discriminator()\n# We pass the generated image to the discriminator\nlogits = discriminator(gen_image)\n@assert size(logits) == (1, 3) Just like our dummy generator, the untrained discriminator has no idea about what is a real or fake image. It needs to be trained alongside the generator to output positive values for real images, and negative values for fake images."},{"id":469,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Loss functions for GAN","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Loss-functions-for-GAN","content":" Loss functions for GAN In a GAN problem, there are only two labels involved: fake and real. So Binary CrossEntropy is an easy choice for a preliminary loss function.  But even if Flux's  binarycrossentropy  does the job for us, due to numerical stability it is always preferred to compute cross-entropy using logits. Flux provides  logitbinarycrossentropy  specifically for this purpose. Mathematically it is equivalent to  binarycrossentropy(σ(ŷ), y, kwargs...)."},{"id":470,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Discriminator Loss","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Discriminator-Loss","content":" Discriminator Loss The discriminator loss quantifies how well the discriminator can distinguish real images from fakes. It compares  discriminator's predictions on real images to an array of 1s, and discriminator's predictions on fake (generated) images to an array of 0s. These two losses are summed together to give a scalar loss. So we can write the loss function of the discriminator as: function discriminator_loss(real_output, fake_output)\n    real_loss = logitbinarycrossentropy(real_output, 1)\n    fake_loss = logitbinarycrossentropy(fake_output, 0)\n    return real_loss + fake_loss\nend"},{"id":471,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Generator Loss","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Generator-Loss","content":" Generator Loss The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). generator_loss(fake_output) = logitbinarycrossentropy(fake_output, 1) We also need optimisers for our network. Why you may ask? Read more  here . For both the generator and discriminator, we will use the  ADAM optimiser ."},{"id":472,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Utility functions","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Utility-functions","content":" Utility functions The output of the generator ranges from (-1, 1), so it needs to be de-normalized before we can display it as an image. To make things a bit easier, we define a function to visualize the output of the generator as a grid of images.  function create_output_image(gen, fixed_noise, hparams)\n    fake_images = cpu(gen.(fixed_noise))\n    image_array = reduce(vcat, reduce.(hcat, partition(fake_images, hparams.output_dim)))\n    image_array = permutedims(dropdims(image_array; dims=(3, 4)), (2, 1))\n    image_array = @. Gray(image_array + 1f0) / 2f0\n    return image_array\nend"},{"id":473,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Training","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Training","content":" Training For the sake of simplifying our training problem, we will divide the generator and discriminator training into two separate functions.  function train_discriminator!(gen, disc, real_img, fake_img, opt, ps, hparams)\n\n    disc_loss, grads = Flux.withgradient(ps) do\n        discriminator_loss(disc(real_img), disc(fake_img))\n    end\n\n    # Update the discriminator parameters\n    update!(opt, ps, grads)\n    return disc_loss\nend We define a similar function for the generator. function train_generator!(gen, disc, fake_img, opt, ps, hparams)\n\n    gen_loss, grads = Flux.withgradient(ps) do\n        generator_loss(disc(fake_img))\n    end\n\n    update!(opt, ps, grads)\n    return gen_loss\nend Now that we have defined every function we need, we integrate everything into a single  train  function where we first set up all the models and optimisers and then train the GAN for a specified number of epochs. function train(hparams)\n\n    dev = hparams.device\n    # Check if CUDA is actually present\n    if hparams.device == gpu\n        if !CUDA.has_cuda()\n        dev = cpu\n        @warn \"No gpu found, falling back to CPU\"\n        end\n    end\n\n    # Load the normalized MNIST images\n    dataloader = load_MNIST_images(hparams)\n\n    # Initialize the models and pass them to correct device\n    disc = Discriminator() |> dev\n    gen =  Generator(hparams.latent_dim) |> dev\n\n    # Collect the generator and discriminator parameters\n    disc_ps = params(disc)\n    gen_ps = params(gen)\n\n    # Initialize the ADAM optimisers for both the sub-models\n    # with respective learning rates\n    disc_opt = ADAM(hparams.disc_lr)\n    gen_opt = ADAM(hparams.gen_lr)\n\n    # Create a batch of fixed noise for visualizing the training of generator over time\n    fixed_noise = [randn(Float32, hparams.latent_dim, 1) |> dev for _=1:hparams.output_dim^2]\n\n    # Training loop\n    train_steps = 0\n    for ep in 1:hparams.epochs\n        @info \"Epoch $ep\"\n        for real_img in dataloader\n\n            # Transfer the data to the GPU\n            real_img = real_img |> dev\n\n            # Create a random noise\n            noise = randn!(similar(real_img, (hparams.latent_dim, hparams.batch_size)))\n            # Pass the noise to the generator to create a fake imagae\n            fake_img = gen(noise)\n\n            # Update discriminator and generator\n            loss_disc = train_discriminator!(gen, disc, real_img, fake_img, disc_opt, disc_ps, hparams)\n            loss_gen = train_generator!(gen, disc, fake_img, gen_opt, gen_ps, hparams)\n\n            if train_steps % hparams.verbose_freq == 0\n                @info(\"Train step $(train_steps), Discriminator loss = $(loss_disc), Generator loss = $(loss_gen)\")\n                # Save generated fake image\n                output_image = create_output_image(gen, fixed_noise, hparams)\n                save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n            end\n            train_steps += 1\n        end\n    end\n\n    output_image = create_output_image(gen, fixed_noise, hparams)\n    save(@sprintf(\"output/dcgan_steps_%06d.png\", train_steps), output_image)\n\n    return nothing\nend Now we finally get to train the GAN: # Define the hyper-parameters (here, we go with the default ones)\nhparams = HyperParams()\ntrain(hparams)"},{"id":474,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Output","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Output","content":" Output The generated images are stored inside the  output  folder. To visualize the output of the generator over time, we create a gif of the generated images. folder = \"output\"\n# Get the image filenames from the folder\nimg_paths = readdir(folder, join=true)\n# Load all the images as an array\nimages = load.(img_paths)\n# Join all the images in the array to create a matrix of images\ngif_mat = cat(images..., dims=3)\nsave(\"./output.gif\", gif_mat)"},{"id":475,"pagetitle":"Deep Convolutional Generative Adversarial Network (DCGAN)","title":"Resources & References","ref":"/flux/stable/tutorials/2021-10-08-dcgan-mnist/#Resources-and-References","content":" Resources & References The DCGAN implementation in the Model Zoo. Info Originally published at  fluxml.ai  on 8 October 2021, by Deeptendu Santra"},{"id":478,"pagetitle":"Tutorial: Generative Adversarial Networks","title":"Tutorial: Generative Adversarial Networks","ref":"/flux/stable/tutorials/2021-10-14-vanilla-gan/#[Tutorial:-Generative-Adversarial-Networks](](@id-man-gan-tutorial))","content":" Tutorial: Generative Adversarial Networks This tutorial describes how to implement a vanilla Generative Adversarial Network using Flux and how train it on the MNIST dataset. It is based on this  Pytorch tutorial . The original GAN  paper  by Goodfellow et al. is a great resource that describes the motivation and theory behind GANs: In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles. Let's implement a GAN in Flux. To get started we first import a few useful packages: using MLDatasets: MNIST\nusing Flux.Data: DataLoader\nusing Flux\nusing CUDA\nusing Zygote\nusing UnicodePlots To download a package in the Julia REPL, type  ]  to enter package mode and then type  add MLDatasets  or perform this operation with the Pkg module like this > import Pkg\n> Pkg.add(\"MLDatasets\") While  UnicodePlots  is not necessary, it can be used to plot generated samples into the terminal during training. Having direct feedback, instead of looking at plots in a separate window, use fantastic for debugging. Next, let us define values for learning rate, batch size, epochs, and other hyper-parameters. While we are at it, we also define optimisers for the generator and discriminator network. More on what these are later.     lr_g = 2e-4          # Learning rate of the generator network\n    lr_d = 2e-4          # Learning rate of the discriminator network\n    batch_size = 128    # batch size\n    num_epochs = 1000   # Number of epochs to train for\n    output_period = 100 # Period length for plots of generator samples\n    n_features = 28 * 28# Number of pixels in each sample of the MNIST dataset\n    latent_dim = 100    # Dimension of latent space\n    opt_dscr = ADAM(lr_d)# Optimiser for the discriminator\n    opt_gen = ADAM(lr_g) # Optimiser for the generator In this tutorial I'm assuming that a CUDA-enabled GPU is available on the system where the script is running. If this is not the case, simply remove the  |>gpu  decorators:  piping ."},{"id":479,"pagetitle":"Tutorial: Generative Adversarial Networks","title":"Data loading","ref":"/flux/stable/tutorials/2021-10-14-vanilla-gan/#Data-loading","content":" Data loading The MNIST data set is available from  MLDatasets . The first time you instantiate it you will be prompted if you want to download it. You should agree to this.  GANs can be trained unsupervised. Therefore only keep the images from the training set and discard the labels. After we load the training data we re-scale the data from values in [0:1] to values in [-1:1]. GANs are notoriously tricky to train and this re-scaling is a recommended  GAN hack . The  re-scaled data is used to define a data loader which handles batching and shuffling the data.     # Load the dataset\n    train_x, _ = MNIST.traindata(Float32);\n    # This dataset has pixel values ∈ [0:1]. Map these to [-1:1]\n    train_x = 2f0 * reshape(train_x, 28, 28, 1, :) .- 1f0 |>gpu;\n    # DataLoader allows to access data batch-wise and handles shuffling.\n    train_loader = DataLoader(train_x, batchsize=batch_size, shuffle=true);"},{"id":480,"pagetitle":"Tutorial: Generative Adversarial Networks","title":"Defining the Networks","ref":"/flux/stable/tutorials/2021-10-14-vanilla-gan/#Defining-the-Networks","content":" Defining the Networks A vanilla GAN, the discriminator and the generator are both plain,  feed-forward  multilayer perceptrons . We use leaky rectified linear units  leakyrelu  to ensure out model is non-linear.  Here, the coefficient  α  (in the  leakyrelu  below), is set to 0.2. Empirically,   this value allows for good training of the network (based on prior experiments).  It has also been found that Dropout ensures a good generalization of the learned  network, so we will use that below. Dropout is usually active when training a  model and inactive in inference. Flux automatically sets the training mode when calling the model in a gradient context. As a final non-linearity, we use the  sigmoid  activation function. discriminator = Chain(Dense(n_features, 1024, x -> leakyrelu(x, 0.2f0)),\n                        Dropout(0.3),\n                        Dense(1024, 512, x -> leakyrelu(x, 0.2f0)),\n                        Dropout(0.3),\n                        Dense(512, 256, x -> leakyrelu(x, 0.2f0)),\n                        Dropout(0.3),\n                        Dense(256, 1, sigmoid)) |> gpu Let's define the generator in a similar fashion. This network maps a latent variable (a variable that is not directly observed but instead inferred) to the  image space and we set the input and output dimension accordingly. A  tanh  squashes  the output of the final layer to values in [-1:1], the same range that we squashed  the training data onto. generator = Chain(Dense(latent_dim, 256, x -> leakyrelu(x, 0.2f0)),\n                    Dense(256, 512, x -> leakyrelu(x, 0.2f0)),\n                    Dense(512, 1024, x -> leakyrelu(x, 0.2f0)),\n                    Dense(1024, n_features, tanh)) |> gpu"},{"id":481,"pagetitle":"Tutorial: Generative Adversarial Networks","title":"Training functions for the networks","ref":"/flux/stable/tutorials/2021-10-14-vanilla-gan/#Training-functions-for-the-networks","content":" Training functions for the networks To train the discriminator, we present it with real data from the MNIST data set and with fake data and reward it by predicting the correct labels for each sample. The correct labels are of course 1 for in-distribution data and 0 for out-of-distribution data coming from the generator.   Binary cross entropy  is the loss function of choice. While the Flux documentation suggests to use  Logit binary cross entropy , the GAN seems to be difficult to train with this loss function. This function returns the discriminator loss for logging purposes. We can calculate the loss in the same call as evaluating the pullback and resort to getting the pullback directly from Zygote instead of calling  Flux.train!  on the model. To calculate the gradients of the loss function with respect to the parameters of the discriminator we then only have to evaluate the pullback with a seed gradient of 1.0. These gradients are used to update the model parameters function train_dscr!(discriminator, real_data, fake_data)\n    this_batch = size(real_data)[end] # Number of samples in the batch\n    # Concatenate real and fake data into one big vector\n    all_data = hcat(real_data, fake_data)\n\n    # Target vector for predictions: 1 for real data, 0 for fake data.\n    all_target = [ones(eltype(real_data), 1, this_batch) zeros(eltype(fake_data), 1, this_batch)] |> gpu;\n\n    ps = Flux.params(discriminator)\n    loss, pullback = Zygote.pullback(ps) do\n        preds = discriminator(all_data)\n        loss = Flux.Losses.binarycrossentropy(preds, all_target)\n    end\n    # To get the gradients we evaluate the pullback with 1.0 as a seed gradient.\n    grads = pullback(1f0)\n\n    # Update the parameters of the discriminator with the gradients we calculated above\n    Flux.update!(opt_dscr, Flux.params(discriminator), grads)\n    \n    return loss \nend Now we need to define a function to train the generator network. The job of the generator is to fool the discriminator so we reward the generator when the discriminator predicts a high probability for its samples to be real data. In the training function we first need to sample some noise, i.e. normally distributed data. This has to be done outside the pullback since we don't want to get the gradients with respect to the noise, but to the generator parameters. Inside the pullback we need to first apply the generator to the noise since we will take the gradient with respect to the parameters of the generator. We also need to call the discriminator in order to evaluate the loss function inside the pullback. Here we need to remember to deactivate the dropout layers of the discriminator. We do this by setting the discriminator into test mode before the pullback. Immediately after the pullback we set it back into training mode. Then we evaluate the pullback, call it with a seed gradient of 1.0 as above, update the parameters of the generator network and return the loss. function train_gen!(discriminator, generator)\n    # Sample noise\n    noise = randn(latent_dim, batch_size) |> gpu;\n\n    # Define parameters and get the pullback\n    ps = Flux.params(generator)\n    # Set discriminator into test mode to disable dropout layers\n    testmode!(discriminator)\n    # Evaluate the loss function while calculating the pullback. We get the loss for free\n    loss, back = Zygote.pullback(ps) do\n        preds = discriminator(generator(noise));\n        loss = Flux.Losses.binarycrossentropy(preds, 1.) \n    end\n    # Evaluate the pullback with a seed-gradient of 1.0 to get the gradients for\n    # the parameters of the generator\n    grads = back(1.0f0)\n    Flux.update!(opt_gen, Flux.params(generator), grads)\n    # Set discriminator back into automatic mode\n    trainmode!(discriminator, mode=:auto)\n    return loss\nend"},{"id":482,"pagetitle":"Tutorial: Generative Adversarial Networks","title":"Training","ref":"/flux/stable/tutorials/2021-10-14-vanilla-gan/#Training","content":" Training Now we are ready to train the GAN. In the training loop we keep track of the per-sample loss of the generator and the discriminator, where we use the batch loss returned by the two training functions defined above. In each epoch we iterate over the mini-batches given by the data loader. Only minimal data processing needs to be done before the training functions can be called.  lossvec_gen = zeros(num_epochs)\nlossvec_dscr = zeros(num_epochs)\n\nfor n in 1:num_epochs\n    loss_sum_gen = 0.0f0\n    loss_sum_dscr = 0.0f0\n\n    for x in train_loader\n        # - Flatten the images from 28x28xbatchsize to 784xbatchsize\n        real_data = flatten(x);\n\n        # Train the discriminator\n        noise = randn(latent_dim, size(x)[end]) |> gpu\n        fake_data = generator(noise)\n        loss_dscr = train_dscr!(discriminator, real_data, fake_data)\n        loss_sum_dscr += loss_dscr\n\n        # Train the generator\n        loss_gen = train_gen!(discriminator, generator)\n        loss_sum_gen += loss_gen\n    end\n\n    # Add the per-sample loss of the generator and discriminator\n    lossvec_gen[n] = loss_sum_gen / size(train_x)[end]\n    lossvec_dscr[n] = loss_sum_dscr / size(train_x)[end]\n\n    if n % output_period == 0\n        @show n\n        noise = randn(latent_dim, 4) |> gpu;\n        fake_data = reshape(generator(noise), 28, 4*28);\n        p = heatmap(fake_data, colormap=:inferno)\n        print(p)\n    end\nend  For the hyper-parameters shown in this example, the generator produces useful images after about 1000 epochs. And after about 5000 epochs the result look indistinguishable from real MNIST data. Using a Nvidia V100 GPU on a 2.7 GHz Power9 CPU with 32 hardware threads, training 100 epochs takes about 80 seconds when using the GPU. The GPU utilization is between 30 and 40%. To observe the network more frequently during training you can for example set   output_period=20 . Training the GAN using the CPU takes about 10 minutes  per epoch and is not recommended."},{"id":483,"pagetitle":"Tutorial: Generative Adversarial Networks","title":"Results","ref":"/flux/stable/tutorials/2021-10-14-vanilla-gan/#Results","content":" Results Below you can see what some of the images output may look like after different numbers of epochs."},{"id":484,"pagetitle":"Tutorial: Generative Adversarial Networks","title":"Resources","ref":"/flux/stable/tutorials/2021-10-14-vanilla-gan/#Resources","content":" Resources A collection of GANs in Flux Wikipedia GAN hacks Info Originally published at  fluxml.ai  on 14 October 2021, by Ralph Kube."},{"id":487,"pagetitle":"Linear Regression","title":"Tutorial: Linear Regression","ref":"/flux/stable/tutorials/linear_regression/#man-linear-regression","content":" Tutorial: Linear Regression Flux is a pure Julia ML stack that allows you to build predictive models. Here are the steps for a typical Flux program: Provide training and test data Build a model with configurable parameters to make predictions Iteratively train the model by tweaking the parameters to improve predictions Verify your model Under the hood, Flux uses a technique called automatic differentiation to take gradients that help improve predictions. Flux is also fully written in Julia so you can easily replace any layer of Flux with your own code to improve your understanding or satisfy special requirements. The following page contains a step-by-step walkthrough of the linear regression algorithm in  Julia  using  Flux ! We will start by creating a simple linear regression model for dummy data and then move on to a real dataset. The first part would involve writing some parts of the model on our own, which will later be replaced by  Flux . Let us start by building a simple linear regression model. This model would be trained on the data points of the form  (x₁, y₁), (x₂, y₂), ... , (xₙ, yₙ) . In the real world, these  x s can have multiple features, and the  y s denote a label. In our example, each  x  has a single feature; hence, our data would have  n  data points, each point mapping a single feature to a single label. Importing the required  Julia  packages - julia> using Flux, Plots"},{"id":488,"pagetitle":"Linear Regression","title":"Generating a dataset","ref":"/flux/stable/tutorials/linear_regression/#Generating-a-dataset","content":" Generating a dataset The data usually comes from the real world, which we will be exploring in the last part of this tutorial, but we don't want to jump straight to the relatively harder part. Here we will generate the  x s of our data points and map them to the respective  y s using a simple function. Remember, here each  x  is equivalent to a feature, and each  y  is the corresponding label. Combining all the  x s and  y s would create the complete dataset. julia> x = hcat(collect(Float32, -3:0.1:3)...)\n1×61 Matrix{Float32}:\n -3.0  -2.9  -2.8  -2.7  -2.6  -2.5  …  2.4  2.5  2.6  2.7  2.8  2.9  3.0 The  hcat  call generates a  Matrix  with numbers ranging from  -3.0  to  3.0  with a gap of  0.1  between them. Each column of this matrix holds a single  x , a total of 61  x s. The next step would be to generate the corresponding labels or the  y s. julia> f(x) = @. 3x + 2;\n\njulia> y = f(x)\n1×61 Matrix{Float32}:\n -7.0  -6.7  -6.4  -6.1  -5.8  -5.5  …  9.5  9.8  10.1  10.4  10.7  11.0 The function  f  maps each  x  to a  y , and as  x  is a  Matrix , the expression broadcasts the scalar values using  @.  macro. Our data points are ready, but they are too perfect. In a real-world scenario, we will not have an  f  function to generate  y  values, but instead, the labels would be manually added. julia> x = x .* reshape(rand(Float32, 61), (1, 61)); Visualizing the final data - julia> plot(vec(x), vec(y), lw = 3, seriestype = :scatter, label = \"\", title = \"Generated data\", xlabel = \"x\", ylabel= \"y\"); The data looks random enough now! The  x  and  y  values are still somewhat correlated; hence, the linear regression algorithm should work fine on our dataset. We can now proceed ahead and build a model for our dataset!"},{"id":489,"pagetitle":"Linear Regression","title":"Building a model","ref":"/flux/stable/tutorials/linear_regression/#Building-a-model","content":" Building a model A linear regression model is defined mathematically as - \\[model(W, b, x) = Wx + b\\] where  W  is the weight matrix and  b  is the bias. For our case, the weight matrix ( W ) would constitute only a single element, as we have only a single feature. We can define our model in  Julia  using the exact same notation! julia> custom_model(W, b, x) = @. W*x + b\ncustom_model (generic function with 1 method) The  @.  macro allows you to perform the calculations by broadcasting the scalar quantities (for example - the bias). The next step would be to initialize the model parameters, which are the weight and the bias. There are a lot of initialization techniques available for different machine learning models, but for the sake of this example, let's pull out the weight from a uniform distribution and initialize the bias as  0 . julia> W = rand(Float32, 1, 1)\n1×1 Matrix{Float32}:\n 0.99285793\n\njulia> b = [0.0f0]\n1-element Vector{Float32}:\n 0.0 Time to test if our model works! julia> custom_model(W, b, x) |> size\n(1, 61)\n\njulia> custom_model(W, b, x)[1], y[1]\n(-1.6116865f0, -7.0f0) It does! But the predictions are way off. We need to train the model to improve the predictions, but before training the model we need to define the loss function. The loss function would ideally output a quantity that we will try to minimize during the entire training process. Here we will use the mean sum squared error loss function. julia> function custom_loss(W, b, x, y)\n           ŷ = custom_model(W, b, x)\n           sum((y .- ŷ).^2) / length(x)\n       end;\n\njulia> custom_loss(W, b, x, y)\n23.772217f0 Calling the loss function on our  x s and  y s shows how far our predictions ( ŷ ) are from the real labels. More precisely, it calculates the sum of the squares of residuals and divides it by the total number of data points. We have successfully defined our model and the loss function, but surprisingly, we haven't used  Flux  anywhere till now. Let's see how we can write the same code using  Flux .  julia> flux_model = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters A  Dense(1 => 1)  layer denotes a layer of one neuron with one input (one feature) and one output. This layer is exactly same as the mathematical model defined by us above! Under the hood,  Flux  too calculates the output using the same expression! But, we don't have to initialize the parameters ourselves this time, instead  Flux  does it for us. julia> flux_model.weight, flux_model.bias\n(Float32[-1.2678515;;], Float32[0.0]) Now we can check if our model is acting right. We can pass the complete data in one go, with each  x  having exactly one feature (one input) - julia> flux_model(x) |> size\n(1, 61)\n\njulia> flux_model(x)[1], y[1]\n(-1.8525281f0, -7.0f0) It is! The next step would be defining the loss function using  Flux 's functions - julia> function flux_loss(flux_model, x, y)\n           ŷ = flux_model(x)\n           Flux.mse(ŷ, y)\n       end;\n\njulia> flux_loss(flux_model, x, y)\n22.74856f0 Everything works as before! It almost feels like  Flux  provides us with smart wrappers for the functions we could have written on our own. Now, as the last step of this section, let's see how different the  flux_model  is from our  custom_model . A good way to go about this would be to fix the parameters of both models to be the same. Let's change the parameters of our  custom_model  to match that of the  flux_model  - julia> W = Float32[1.1412252]\n1-element Vector{Float32}:\n 1.1412252 To check how both the models are performing on the data, let's find out the losses using the  loss  and  flux_loss  functions - julia> custom_loss(W, b, x, y), flux_loss(flux_model, x, y)\n(22.74856f0, 22.74856f0) The losses are identical! This means that our  model  and the  flux_model  are identical on some level, and the loss functions are completely identical! The difference in models would be that  Flux 's  Dense  layer supports many other arguments that can be used to customize the layer further. But, for this tutorial, let us stick to our simple  custom_model ."},{"id":490,"pagetitle":"Linear Regression","title":"Training the model","ref":"/flux/stable/tutorials/linear_regression/#Training-the-model","content":" Training the model Let's train our model using the classic Gradient Descent algorithm. According to the gradient descent algorithm, the weights and biases should be iteratively updated using the following mathematical equations - \\[\\begin{aligned}\nW &= W - \\eta * \\frac{dL}{dW} \\\\\nb &= b - \\eta * \\frac{dL}{db}\n\\end{aligned}\\] Here,  W  is the weight matrix,  b  is the bias vector,  $\\eta$  is the learning rate,  $\\frac{dL}{dW}$  is the derivative of the loss function with respect to the weight, and  $\\frac{dL}{db}$  is the derivative of the loss function with respect to the bias. The derivatives are calculated using an Automatic Differentiation tool, and  Flux  uses  Zygote.jl  for the same. Since  Zygote.jl  is an independent Julia package, it can be used outside of Flux as well! Refer to the documentation of  Zygote.jl  for more information on the same. Our first step would be to obtain the gradient of the loss function with respect to the weights and the biases.  Flux  re-exports  Zygote 's  gradient  function; hence, we don't need to import  Zygote  explicitly to use the functionality. julia> dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, y); We can now update the parameters, following the gradient descent algorithm - julia> W .= W .- 0.1 .* dLdW\n1-element Vector{Float32}:\n 1.8144473\n\njulia> b .= b .- 0.1 .* dLdb\n1-element Vector{Float32}:\n 0.41325632 The parameters have been updated! We can now check the value of the loss function - julia> custom_loss(W, b, x, y)\n17.157953f0 The loss went down! This means that we successfully trained our model for one epoch. We can plug the training code written above into a loop and train the model for a higher number of epochs. It can be customized either to have a fixed number of epochs or to stop when certain conditions are met, for example,  change in loss < 0.1 . The loop can be tailored to suit the user's needs, and the conditions can be specified in plain  Julia ! Let's plug our super training logic inside a function and test it again - julia> function train_custom_model()\n           dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, y)\n           @. W = W - 0.1 * dLdW\n           @. b = b - 0.1 * dLdb\n       end;\n\njulia> train_custom_model();\n\njulia> W, b, custom_loss(W, b, x, y)\n(Float32[2.340657], Float32[0.7516814], 13.64972f0) It works, and the loss went down again! This was the second epoch of our training procedure. Let's plug this in a for loop and train the model for 30 epochs. julia> for i = 1:40\n          train_custom_model()\n       end\n\njulia> W, b, custom_loss(W, b, x, y)\n(Float32[4.2422233], Float32[2.2460847], 7.6680417f0) There was a significant reduction in loss, and the parameters were updated! We can train the model even more or tweak the hyperparameters to achieve the desired result faster, but let's stop here. We trained our model for 42 epochs, and loss went down from  22.74856  to  7.6680417f . Time for some visualization!"},{"id":491,"pagetitle":"Linear Regression","title":"Results","ref":"/flux/stable/tutorials/linear_regression/#Results","content":" Results The main objective of this tutorial was to fit a line to our dataset using the linear regression algorithm. The training procedure went well, and the loss went down significantly! Let's see what the fitted line looks like. Remember,  Wx + b  is nothing more than a line's equation, with  slope = W[1]  and  y-intercept = b[1]  (indexing at  1  as  W  and  b  are iterable). Plotting the line and the data points using  Plot.jl  - julia> plot(reshape(x, (61, 1)), reshape(y, (61, 1)), lw = 3, seriestype = :scatter, label = \"\", title = \"Simple Linear Regression\", xlabel = \"x\", ylabel= \"y\");\n\njulia> plot!((x) -> b[1] + W[1] * x, -3, 3, label=\"Custom model\", lw=2); The line fits well! There is room for improvement, but we leave that up to you! You can play with the optimisers, the number of epochs, learning rate, etc. to improve the fitting and reduce the loss!"},{"id":492,"pagetitle":"Linear Regression","title":"Linear regression model on a real dataset","ref":"/flux/stable/tutorials/linear_regression/#Linear-regression-model-on-a-real-dataset","content":" Linear regression model on a real dataset We now move on to a relatively complex linear regression model. Here we will use a real dataset from  MLDatasets.jl , which will not confine our data points to have only one feature. Let's start by importing the required packages - julia> using Flux, Statistics, MLDatasets, DataFrames"},{"id":493,"pagetitle":"Linear Regression","title":"Gathering real data","ref":"/flux/stable/tutorials/linear_regression/#Gathering-real-data","content":" Gathering real data Let's start by initializing our dataset. We will be using the  BostonHousing  dataset consisting of  506  data points. Each of these data points has  13  features and a corresponding label, the house's price. The  x s are still mapped to a single  y , but now, a single  x  data point has 13 features.  julia> dataset = BostonHousing();\n\njulia> x, y = BostonHousing(as_df=false)[:];\n\njulia> x, y = Float32.(x), Float32.(y); We can now split the obtained data into training and testing data - julia> x_train, x_test, y_train, y_test = x[:, 1:400], x[:, 401:end], y[:, 1:400], y[:, 401:end];\n\njulia> x_train |> size, x_test |> size, y_train |> size, y_test |> size\n((13, 400), (13, 106), (1, 400), (1, 106)) This data contains a diverse number of features, which means that the features have different scales. A wise option here would be to  normalise  the data, making the training process more efficient and fast. Let's check the standard deviation of the training data before normalising it. julia> std(x_train)\n134.06786f0 The data is indeed not normalised. We can use the  Flux.normalise  function to normalise the training data. julia> x_train_n = Flux.normalise(x_train);\n\njulia> std(x_train_n)\n1.0000844f0 The standard deviation is now close to one! Our data is ready!"},{"id":494,"pagetitle":"Linear Regression","title":"Building a Flux model","ref":"/flux/stable/tutorials/linear_regression/#Building-a-Flux-model","content":" Building a Flux model We can now directly use  Flux  and let it do all the work internally! Let's define a model that takes in 13 inputs (13 features) and gives us a single output (the label). We will then pass our entire data through this model in one go, and  Flux  will handle everything for us! Remember, we could have declared a model in plain  Julia  as well. The model will have 14 parameters: 13 weights and 1 bias. julia> model = Dense(13 => 1)\nDense(13 => 1)      # 14 parameters Same as before, our next step would be to define a loss function to quantify our accuracy somehow. The lower the loss, the better the model! julia> function loss(model, x, y)\n           ŷ = model(x)\n           Flux.mse(ŷ, y)\n       end;\n\njulia> loss(model, x_train_n, y_train)\n676.1656f0 We can now proceed to the training phase!"},{"id":495,"pagetitle":"Linear Regression","title":"Training the Flux model","ref":"/flux/stable/tutorials/linear_regression/#Training-the-Flux-model","content":" Training the Flux model The training procedure would make use of the same mathematics, but now we can pass in the model inside the  gradient  call and let  Flux  and  Zygote  handle the derivatives! julia> function train_model()\n           dLdm, _, _ = gradient(loss, model, x_train_n, y_train)\n           @. model.weight = model.weight - 0.000001 * dLdm.weight\n           @. model.bias = model.bias - 0.000001 * dLdm.bias\n       end; Contrary to our last training procedure, let's say that this time we don't want to hardcode the number of epochs. We want the training procedure to stop when the loss converges, that is, when  change in loss < δ . The quantity  δ  can be altered according to a user's need, but let's fix it to  10⁻³  for this tutorial. We can write such custom training loops effortlessly using  Flux  and plain  Julia ! julia> loss_init = Inf;\n\njulia> while true\n           train_model()\n           if loss_init == Inf\n               loss_init = loss(model, x_train_n, y_train)\n               continue\n           end\n           if abs(loss_init - loss(model, x_train_n, y_train)) < 1e-4\n               break\n           else\n               loss_init = loss(model, x_train_n, y_train)\n           end\n       end; The code starts by initializing an initial value for the loss,  infinity . Next, it runs an infinite loop that breaks if  change in loss < 10⁻³ , or the code changes the value of  loss_init  to the current loss and moves on to the next iteration. This custom loop works! This shows how easily a user can write down any custom training routine using Flux and Julia! Let's have a look at the loss - julia> loss(model, x_train_n, y_train)\n27.1272f0 The loss went down significantly! It can be minimized further by choosing an even smaller  δ ."},{"id":496,"pagetitle":"Linear Regression","title":"Testing the Flux model","ref":"/flux/stable/tutorials/linear_regression/#Testing-the-Flux-model","content":" Testing the Flux model The last step of this tutorial would be to test our model using the testing data. We will first normalise the testing data and then calculate the corresponding loss. julia> x_test_n = Flux.normalise(x_test);\n\njulia> loss(model, x_test_n, y_test)\n66.91015f0 The loss is not as small as the loss of the training data, but it looks good! This also shows that our model is not overfitting! Summarising this tutorial, we started by generating a random yet correlated dataset for our  custom model . We then saw how a simple linear regression model could be built with and without  Flux , and how they were almost identical.  Next, we trained the model by manually writing down the Gradient Descent algorithm and optimising the loss. We also saw how  Flux  provides various wrapper functionalities and keeps the API extremely intuitive and simple for the users.  After getting familiar with the basics of  Flux  and  Julia , we moved ahead to build a machine learning model for a real dataset. We repeated the exact same steps, but this time with a lot more features and data points, and by harnessing  Flux 's full capabilities. In the end, we developed a training loop that was smarter than the hardcoded one and ran the model on our normalised dataset to conclude the tutorial. Info Originally published on 21 November 2022, by Saransh Chopra."},{"id":499,"pagetitle":"Logistic Regression","title":"Logistic Regression","ref":"/flux/stable/tutorials/logistic_regression/#Logistic-Regression","content":" Logistic Regression The following page contains a step-by-step walkthrough of the logistic regression algorithm in Julia using Flux. We will then create a simple logistic regression model without any usage of Flux and compare the different working parts with Flux's implementation.  Let's start by importing the required Julia packages. julia> using Flux, Statistics, MLDatasets, DataFrames, OneHotArrays"},{"id":500,"pagetitle":"Logistic Regression","title":"Dataset","ref":"/flux/stable/tutorials/logistic_regression/#Dataset","content":" Dataset Let's start by importing a dataset from MLDatasets.jl. We will use the  Iris  dataset that contains the data of three different  Iris  species. The data consists of 150 data points ( x s), each having four features. Each of these  x  is mapped to  y , the name of a particular  Iris  specie. The following code will download the  Iris  dataset when run for the first time. julia> Iris()\ndataset Iris:\n  metadata   =>    Dict{String, Any} with 4 entries\n  features   =>    150×4 DataFrame\n  targets    =>    150×1 DataFrame\n  dataframe  =>    150×5 DataFrame\n\njulia> x, y = Iris(as_df=false)[:]; Let's have a look at our dataset - julia> y\n1×150 Matrix{InlineStrings.String15}:\n \"Iris-setosa\"  \"Iris-setosa\"  …  \"Iris-virginica\"  \"Iris-virginica\"\n\njulia> x |> summary\n\"4×150 Matrix{Float64}\" The  y  values here corresponds to a type of iris plant, with a total of 150 data points. The  x  values depict the sepal length, sepal width, petal length, and petal width (all in  cm ) of 150 iris plant (hence the matrix size  4×150 ). Different type of iris plants have different lengths and widths of sepals and petals associated with them, and there is a definitive pattern for this in nature. We can leverage this to train a simple classifier that outputs the type of iris plant using the length and width of sepals and petals as inputs. Our next step would be to convert this data into a form that can be fed to a machine learning model. The  x  values are arranged in a matrix and should ideally be converted to  Float32  type (see  Performance tips ), but the labels must be one hot encoded.  Here  is a great discourse thread on different techniques that can be used to one hot encode data with or without using any external Julia package. julia> x = Float32.(x);\n\njulia> y = vec(y);\n\njulia> custom_y_onehot = unique(y) .== permutedims(y)\n3×150 BitMatrix:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  1  1  1  1  1  1  1  1  1  1 This same operation can also be performed using  OneHotArrays '  onehotbatch  function. We will use both of these outputs parallelly to show how intuitive FluxML is! julia> const classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"];\n\njulia> flux_y_onehot = onehotbatch(y, classes)\n3×150 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  1  1  1  1  1  1  1  1  1  1  1 Our data is ready. The next step would be to build a classifier for the same."},{"id":501,"pagetitle":"Logistic Regression","title":"Building a model","ref":"/flux/stable/tutorials/logistic_regression/#Building-a-model","content":" Building a model A logistic regression model is defined mathematically as - \\[model(x) = σ(Wx + b)\\] where  W  is the weight matrix,  b  is the bias vector, and  σ  is any activation function. For our case, let's use the  softmax  activation function as we will be performing a multiclass classification task. julia> m(W, b, x) = W*x .+ b\nm (generic function with 1 method) Note that this model lacks an activation function, but we will come back to that. We can now move ahead to initialize the parameters of our model. Given that our model has four inputs (4 features in every data point), and three outputs (3 different classes), the parameters can be initialized in the following way - julia> W = rand(Float32, 3, 4);\n\njulia> b = [0.0f0, 0.0f0, 0.0f0]; Now our model can take in the complete dataset and predict the class of each  x  in one go. But, we need to ensure that our model outputs the probabilities of an input belonging to the respective classes. As our model has three outputs, each would denote the probability of the input belonging to a particular class. We will use an activation function to map our outputs to a probability value. It would make sense to use a  softmax  activation function here, which is defined mathematically as - \\[σ(\\vec{x}) = \\frac{\\\\e^{z_{i}}}{\\\\sum_{j=1}^{k} \\\\e^{z_{j}}}\\] The  softmax  function scales down the outputs to probability values such that the sum of all the final outputs equals  1 . Let's implement this in Julia. julia> custom_softmax(x) = exp.(x) ./ sum(exp.(x), dims=1)\ncustom_softmax (generic function with 1 method) The implementation looks straightforward enough! Note that we specify  dims=1  in the  sum  function to calculate the sum of probabilities in each column. Remember, we will have a 3×150 matrix (predicted  y s) as the output of our model, where each column would be an output of a corresponding input. Let's combine this  softmax  function with our model to construct the complete  custom_model . julia> custom_model(W, b, x) = m(W, b, x) |> custom_softmax\ncustom_model (generic function with 1 method) Let's check if our model works. julia> custom_model(W, b, x) |> size\n(3, 150) It works! Let's check if the  softmax  function is working. julia> all(0 .<= custom_model(W, b, x) .<= 1)\ntrue\n\njulia> sum(custom_model(W, b, x), dims=1)\n1×150 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0 Every output value is between  0  and  1 , and every column adds to  1 ! Let's convert our  custom_model  to a Flux model. Flux provides the users with a very elegant API that almost feels like writing your code! Note, all the  flux_*  variables in this tutorial would be general, that is, they can be used as it is with some other similar-looking dataset, but the  custom_*  variables will remain specific to this tutorial. julia> flux_model = Chain(Dense(4 => 3), softmax)\nChain(\n  Dense(4 => 3),                        # 15 parameters\n  NNlib.softmax,\n) A  Dense(4 => 3)  layer denotes a layer with four inputs (four features in every data point) and three outputs (three classes or labels). This layer is the same as the mathematical model defined by us above. Under the hood, Flux too calculates the output using the same expression, but we don't have to initialize the parameters ourselves this time, instead Flux does it for us. The  softmax  function provided by NNLib.jl is re-exported by Flux, which has been used here. Lastly, Flux provides users with a  Chain  struct which makes stacking layers seamless. A model's weights and biases can be accessed as follows - julia> flux_model[1].weight, flux_model[1].bias\n(Float32[0.78588694 -0.45968163 -0.77409476 0.2358028; -0.9049773 -0.58643705 0.466441 -0.79523873; 0.82426906 0.4143493 0.7630932 0.020588955], Float32[0.0, 0.0, 0.0]) We can now pass the complete data in one go, with each data point having four features (four inputs)!"},{"id":502,"pagetitle":"Logistic Regression","title":"Loss and accuracy","ref":"/flux/stable/tutorials/logistic_regression/#Loss-and-accuracy","content":" Loss and accuracy Our next step should be to define some quantitative values for our model, which we will maximize or minimize during the complete training procedure. These values will be the loss function and the accuracy metric. Let's start by defining a loss function, a  logitcrossentropy  function. julia> custom_logitcrossentropy(ŷ, y) = mean(.-sum(y .* logsoftmax(ŷ; dims = 1); dims = 1)); Now we can wrap the  custom_logitcrossentropy  inside a function that takes in the model parameters,  x s, and  y s, and returns the loss value. julia> function custom_loss(W, b, x, y)\n           ŷ = custom_model(W, b, x)\n           custom_logitcrossentropy(ŷ, y)\n       end;\n\njulia> custom_loss(W, b, x, custom_y_onehot)\n1.1714406827505623 The loss function works! Flux provides us with many minimal yet elegant loss functions. In fact, the  custom_logitcrossentropy  defined above has been taken directly from Flux. The functions present in Flux includes sanity checks, ensures efficient performance, and behaves well with the overall FluxML ecosystem. julia> function flux_loss(flux_model, x, y)\n           ŷ = flux_model(x)\n           Flux.logitcrossentropy(ŷ, y)\n       end;\n\njulia> flux_loss(flux_model, x, flux_y_onehot)\n1.2156688659673647 Next, let's define an accuracy function, which we will try to maximize during our training procedure. Before jumping to accuracy, let's define a  onecold  function. The  onecold  function would convert our output, which remember, are probability values, to the actual class names. We can divide this task into two parts - Identify the index of the maximum element of each column in the output matrix Convert this index to a class name The maximum index should be calculated along the columns (remember, each column is the output of a single  x  data point). We can use Julia's  argmax  function to achieve this. julia> argmax(custom_y_onehot, dims=1)  # calculate the cartesian index of max element column-wise\n1×150 Matrix{CartesianIndex{2}}:\n CartesianIndex(1, 1)  CartesianIndex(1, 2)  …  CartesianIndex(3, 150)\n\njulia> max_idx = [x[1] for x in argmax(custom_y_onehot; dims=1)]\n1×150 Matrix{Int64}:\n 1  1  1  1  1  1  1  1  1  1  1  1  1  …  3  3  3  3  3  3  3  3  3  3  3  3 Now we can write a function that calculates the indices of the maximum element in each column, and maps them to a class name. julia> function custom_onecold(custom_y_onehot)\n           max_idx = [x[1] for x in argmax(custom_y_onehot; dims=1)]\n           vec(classes[max_idx])\n       end;\n\njulia> custom_onecold(custom_y_onehot)\n150-element Vector{String}:\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n \"Iris-setosa\"\n ⋮\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\"\n \"Iris-virginica\" It works! Flux provides users with the  onecold  function so that we don't have to write it on our own. Let's see how our  custom_onecold  function compares to  Flux.onecold . julia> istrue = Flux.onecold(flux_y_onehot, classes) .== custom_onecold(custom_y_onehot);\n\njulia> all(istrue)\ntrue Both the functions act identically! We now move to the  accuracy  metric and run it with the untrained  custom_model . julia> custom_accuracy(W, b, x, y) = mean(custom_onecold(custom_model(W, b, x)) .== y);\n\njulia> custom_accuracy(W, b, x, y)\n0.3333333333333333 We could also have used Flux's built-in functionality to define this accuracy function. julia> flux_accuracy(x, y) = mean(Flux.onecold(flux_model(x), classes) .== y);\n\njulia> flux_accuracy(x, y)\n0.24"},{"id":503,"pagetitle":"Logistic Regression","title":"Training the model","ref":"/flux/stable/tutorials/logistic_regression/#Training-the-model","content":" Training the model Let's train our model using the classic Gradient Descent algorithm. According to the gradient descent algorithm, the weights and biases should be iteratively updated using the following mathematical equations - \\[\\begin{aligned}\nW &= W - \\eta * \\frac{dL}{dW} \\\\\nb &= b - \\eta * \\frac{dL}{db}\n\\end{aligned}\\] Here,  W  is the weight matrix,  b  is the bias vector,  $\\eta$  is the learning rate,  $\\frac{dL}{dW}$  is the derivative of the loss function with respect to the weight, and  $\\frac{dL}{db}$  is the derivative of the loss function with respect to the bias. The derivatives are calculated using an Automatic Differentiation tool, and Flux uses  Zygote.jl  for the same. Since Zygote.jl is an independent Julia package, it can be used outside of Flux as well! Refer to the documentation of Zygote.jl for more information on the same. Our first step would be to obtain the gradient of the loss function with respect to the weights and the biases. Flux re-exports Zygote's  gradient  function; hence, we don't need to import Zygote explicitly to use the functionality.  gradient  takes in a function and its arguments, and returns a tuple containing  ∂f/∂x  for each argument x. Let's pass in  custom_loss  and the arguments required by  custom_loss  to  gradient . We will require the derivatives of the loss function ( custom_loss ) with respect to the weights ( ∂f/∂w ) and the bias ( ∂f/∂b ) to carry out gradient descent, but we can ignore the partial derivatives of the loss function ( custom_loss ) with respect to  x  ( ∂f/∂x ) and one hot encoded  y  ( ∂f/∂y ). julia> dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, custom_y_onehot); We can now update the parameters, following the gradient descent algorithm - julia> W .= W .- 0.1 .* dLdW;\n\njulia> b .= b .- 0.1 .* dLdb; The parameters have been updated! We can now check the value of our custom loss function - julia> custom_loss(W, b, x, custom_y_onehot)\n1.164742997664842 The loss went down! Let's plug our super training logic inside a function. julia> function train_custom_model()\n           dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, custom_y_onehot)\n           W .= W .- 0.1 .* dLdW\n           b .= b .- 0.1 .* dLdb\n       end; We can plug the training function inside a loop and train the model for more epochs. The loop can be tailored to suit the user's needs, and the conditions can be specified in plain Julia. Here we will train the model for a maximum of  500  epochs, but to ensure that the model does not overfit, we will break as soon as our accuracy value crosses or becomes equal to  0.98 . julia> for i = 1:500\n            train_custom_model();\n            custom_accuracy(W, b, x, y) >= 0.98 && break\n       end\n    \njulia> @show custom_accuracy(W, b, x, y);\ncustom_accuracy(W, b, x, y) = 0.98 Everything works! Our model achieved an accuracy of  0.98 ! Let's have a look at the loss. julia> custom_loss(W, b, x, custom_y_onehot)\n0.6520349798243569 As expected, the loss went down too! Now, let's repeat the same steps with our  flux_model . We can write a similar-looking training loop for our  flux_model  and train it similarly. julia> flux_loss(flux_model, x, flux_y_onehot)\n1.215731131385928\n\njulia> function train_flux_model()\n           dLdm, _, _ = gradient(flux_loss, flux_model, x, flux_y_onehot)\n           @. flux_model[1].weight = flux_model[1].weight - 0.1 * dLdm[:layers][1][:weight]\n           @. flux_model[1].bias = flux_model[1].bias - 0.1 * dLdm[:layers][1][:bias]\n       end;\n\njulia> for i = 1:500\n            train_flux_model();\n            flux_accuracy(x, y) >= 0.98 && break\n       end Looking at the accuracy and loss value - julia> @show flux_accuracy(x, y);\nflux_accuracy(x, y) = 0.98\n\njulia> flux_loss(flux_model, x, flux_y_onehot)\n0.6952386604624324 We see a very similar final loss and accuracy. Summarising this tutorial, we saw how we can run a logistic regression algorithm in Julia with and without using Flux. We started by importing the classic  Iris  dataset, and one hot encoded the labels. Next, we defined our model, the loss function, and the accuracy, all by ourselves. Finally, we trained the model by manually writing down the Gradient Descent algorithm and optimising the loss. Interestingly, we implemented most of the functions on our own, and then parallelly compared them with the functionalities provided by Flux! Info Originally published on 1st April 2023, by Saransh Chopra."},{"id":506,"pagetitle":"Weight Initialisation","title":"Random Weight Initialisation","ref":"/flux/stable/utilities/#man-init-funcs","content":" Random Weight Initialisation Flux initialises convolutional layers and recurrent cells with  glorot_uniform  by default. Most layers accept a function as an  init  keyword, which replaces this default. For example: julia> conv = Conv((3, 3), 3 => 2, relu; init=Flux.glorot_normal)\nConv((3, 3), 3 => 2, relu)  # 56 parameters\n\njulia> conv.bias\n2-element Vector{Float32}:\n 0.0\n 0.0 Note that  init  creates the weight array, but not the bias vector. Many of the initialisation functions accept keywords such as  gain ,  and a random number generator. To make it easy to pass these to layers, there are methods which return a function: julia> Dense(4 => 5, tanh; init=Flux.glorot_uniform(gain=2))\nDense(4 => 5, tanh)  # 25 parameters\n\njulia> Dense(4 => 5, tanh; init=Flux.randn32(MersenneTwister(1)))\nDense(4 => 5, tanh)  # 25 parameters"},{"id":507,"pagetitle":"Weight Initialisation","title":"Initialisation functions","ref":"/flux/stable/utilities/#Initialisation-functions","content":" Initialisation functions"},{"id":508,"pagetitle":"Weight Initialisation","title":"Flux.glorot_uniform","ref":"/flux/stable/utilities/#Flux.glorot_uniform","content":" Flux.glorot_uniform  —  Function glorot_uniform([rng], size...; gain = 1) -> Array\nglorot_uniform([rng]; kw...) -> Function Return an  Array{Float32}  of the given  size  containing random numbers drawn from a uniform distribution on the interval  $[-x, x]$ , where  x = gain * sqrt(6 / (fan_in + fan_out)) . This method is described in [1] and also known as Xavier initialization. Examples julia> Flux.glorot_uniform(3, 4) |> summary\n\"3×4 Matrix{Float32}\"\n\njulia> round.(extrema(Flux.glorot_uniform(10, 100)), digits=3)\n(-0.232f0, 0.234f0)\n\njulia> round.(extrema(Flux.glorot_uniform(100, 10)), digits=3)\n(-0.233f0, 0.233f0)\n\njulia> round.(extrema(Flux.glorot_uniform(100, 100)), digits=3)\n(-0.173f0, 0.173f0)\n\njulia> Dense(3 => 2, tanh; init = Flux.glorot_uniform(MersenneTwister(1)))\nDense(3 => 2, tanh)  # 8 parameters\n\njulia> ans.bias\n2-element Vector{Float32}:\n 0.0\n 0.0 References [1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\"  Proceedings of the thirteenth international conference on artificial intelligence and statistics . 2010. source"},{"id":509,"pagetitle":"Weight Initialisation","title":"Flux.glorot_normal","ref":"/flux/stable/utilities/#Flux.glorot_normal","content":" Flux.glorot_normal  —  Function glorot_normal([rng], size...; gain = 1) -> Array\nglorot_normal([rng]; kw...) -> Function Return an  Array{Float32}  of the given  size  containing random numbers drawn from a normal distribution with standard deviation  gain * sqrt(2 / (fan_in + fan_out)) , using  nfan . This method is described in [1] and also known as Xavier initialization. Examples julia> using Statistics\n\njulia> round(std(Flux.glorot_normal(10, 1000)), digits=3)\n0.044f0\n\njulia> round(std(Flux.glorot_normal(1000, 10)), digits=3)\n0.044f0\n\njulia> round(std(Flux.glorot_normal(1000, 1000)), digits=3)\n0.032f0\n\njulia> Dense(10 => 1000, tanh; init = Flux.glorot_normal(gain=100))\nDense(10 => 1000, tanh)  # 11_000 parameters\n\njulia> round(std(ans.weight), sigdigits=3)\n4.45f0 References [1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\"  Proceedings of the thirteenth international conference on artificial intelligence and statistics . 2010. source"},{"id":510,"pagetitle":"Weight Initialisation","title":"Flux.kaiming_uniform","ref":"/flux/stable/utilities/#Flux.kaiming_uniform","content":" Flux.kaiming_uniform  —  Function kaiming_uniform([rng], size...; gain = √2) -> Array\nkaiming_uniform([rng]; kw...) -> Function Return an  Array{Float32}  of the given  size  containing random numbers drawn from a uniform distribution on the interval  [-x, x] , where  x = gain * sqrt(3/fan_in)  using  nfan . This method is described in [1] and also known as He initialization. Examples julia> round.(extrema(Flux.kaiming_uniform(100, 10)), digits=3)\n(-0.774f0, 0.774f0)\n\njulia> round.(extrema(Flux.kaiming_uniform(10, 100)), digits=3)\n(-0.245f0, 0.244f0)\n\njulia> round.(extrema(Flux.kaiming_uniform(100, 100)), digits=3)\n(-0.245f0, 0.245f0) References [1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\"  Proceedings of the IEEE international conference on computer vision . 2015. source"},{"id":511,"pagetitle":"Weight Initialisation","title":"Flux.kaiming_normal","ref":"/flux/stable/utilities/#Flux.kaiming_normal","content":" Flux.kaiming_normal  —  Function kaiming_normal([rng], size...; gain = √2) -> Array\nkaiming_normal([rng]; kw...) -> Function Return an  Array{Float32}  of the given  size  containing random numbers taken from a normal distribution standard deviation  gain / sqrt(fan_in) , using  nfan . This method is described in [1] and also known as He initialization. Examples julia> using Statistics\n\njulia> round(std(Flux.kaiming_normal(10, 1000)), digits=3)\n0.045f0\n\njulia> round(std(Flux.kaiming_normal(1000, 10)), digits=3)\n0.447f0\n\njulia> round(std(Flux.kaiming_normal(1000, 1000)), digits=3)\n0.045f0 References [1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\"  Proceedings of the IEEE international conference on computer vision . 2015. source"},{"id":512,"pagetitle":"Weight Initialisation","title":"Flux.truncated_normal","ref":"/flux/stable/utilities/#Flux.truncated_normal","content":" Flux.truncated_normal  —  Function truncated_normal([rng], size...; mean = 0, std = 1, lo = -2, hi = 2) -> Array\ntruncated_normal([rng]; kw...) -> Function Return an  Array{Float32}  of the given  size  where each element is drawn from a truncated normal distribution. The numbers are distributed like  filter(x -> lo<=x<=hi, mean .+ std .* randn(100)) . The values are generated by sampling a Uniform(0, 1) ( rand() ) and then applying the inverse CDF of the truncated normal distribution. This method works best when  lo ≤ mean ≤ hi . Examples julia> using Statistics\n\njulia> Flux.truncated_normal(3, 4) |> summary\n\"3×4 Matrix{Float32}\"\n\njulia> round.(extrema(Flux.truncated_normal(10^6)); digits=3)\n(-2.0f0, 2.0f0)\n\njulia> round(std(Flux.truncated_normal(10^6; lo = -100, hi = 100)))\n1.0f0 source"},{"id":513,"pagetitle":"Weight Initialisation","title":"Flux.orthogonal","ref":"/flux/stable/utilities/#Flux.orthogonal","content":" Flux.orthogonal  —  Function orthogonal([rng], size...; gain = 1) -> Array\northogonal([rng]; kw...) -> Function Return an  Array{Float32}  of the given  size  which is a (semi) orthogonal matrix, as described in [1]. Cannot construct a vector, i.e.  length(size) == 1  is forbidden. For  length(size) > 2 , a  prod(size[1:(end - 1)])  by  size[end]  orthogonal matrix is computed before reshaping it to the original dimensions. Examples julia> W = Flux.orthogonal(5, 7);\n\njulia> summary(W)\n\"5×7 Matrix{Float32}\"\n\njulia> W * W' ≈ I(5)\ntrue\n\njulia> W2 = Flux.orthogonal(7, 5);\n\njulia> W2 * W2' ≈ I(7)\nfalse\n\njulia> W2' * W2 ≈ I(5)\ntrue\n\njulia> W3 = Flux.orthogonal(3, 3, 2, 4);\n\njulia> transpose(reshape(W3, :, 4)) * reshape(W3, :, 4) ≈ I(4)\ntrue References [1] Saxe, McClelland, Ganguli. \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\", ICLR 2014, https://arxiv.org/abs/1312.6120 source"},{"id":514,"pagetitle":"Weight Initialisation","title":"Flux.sparse_init","ref":"/flux/stable/utilities/#Flux.sparse_init","content":" Flux.sparse_init  —  Function sparse_init([rng], rows, cols; sparsity, std = 0.01) -> Array\nsparse_init([rng]; kw...) -> Function Return a  Matrix{Float32}  of size  rows, cols  where each column contains a fixed fraction of zero elements given by  sparsity . Non-zero elements are normally distributed with a mean of zero and standard deviation  std . This method is described in [1]. Examples julia> count(iszero, Flux.sparse_init(10, 10, sparsity=1/5))\n20\n\njulia> sum(0 .== Flux.sparse_init(10, 11, sparsity=0.9), dims=1)\n1×11 Matrix{Int64}:\n 9  9  9  9  9  9  9  9  9  9  9\n\njulia> Dense(3 => 10, tanh; init=Flux.sparse_init(sparsity=0.5))\nDense(3 => 10, tanh)  # 40 parameters\n\njulia> count(iszero, ans.weight, dims=1)\n1×3 Matrix{Int64}:\n 5  5  5 References [1] Martens, J, \"Deep learning via Hessian-free optimization\"  Proceedings of the 27th International Conference on International Conference on Machine Learning . 2010. source"},{"id":515,"pagetitle":"Weight Initialisation","title":"Flux.identity_init","ref":"/flux/stable/utilities/#Flux.identity_init","content":" Flux.identity_init  —  Function identity_init(size...; gain=1, shift=0) -> Array\nidentity_init(; kw...) -> Function Return an  Array{Float32}  of the given  size  which yields an identity mapping when used as parameters in most Flux layers. Use  gain  to scale the identity by a constant. Often useful in the context of transfer learning, i.e when one wants to add more capacity to a model but start from the same mapping. Has the following behaviour 1D: A  Vector  of  zeros  (useful for an identity bias) 2D: An identity matrix (useful for an identity matrix multiplication) More than 2D: A dense block array of center tap spatial filters (useful for an identity convolution) Some caveats:  Not all layers will be identity mapping when used with this init. Exceptions include recurrent layers and normalization layers. Layers must have  input_size == output_size  for identity mapping to be possible. When this is not the case, extra dimensions of the array are padded with zeros. For convolutional layers, in addition to the above, the kernel sizes must also be odd and padding must be applied so that output feature maps have the same size as input feature maps, e.g by using  SamePad . Use keyword  shift  (integer or tuple) to apply circular shift to the output, equivalent to  Base.circshift(identity_init(size...), shift) . For consistency with other initialisers, it accepts  rng::AbstractRNG  as an optional first argument. But this is ignored, since the result is not random. Examples julia> Flux.identity_init(3,5)\n3×5 Matrix{Float32}:\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n\njulia> Dense(5 => 3, relu, init=Flux.identity_init)([1,-2,3,-4,5])\n3-element Vector{Float32}:\n 1.0\n 0.0\n 3.0\n\njulia> Flux.identity_init(3,3,2; gain=100)\n3×3×2 Array{Float32, 3}:\n[:, :, 1] =\n   0.0  0.0  0.0\n 100.0  0.0  0.0\n   0.0  0.0  0.0\n\n[:, :, 2] =\n 0.0    0.0  0.0\n 0.0  100.0  0.0\n 0.0    0.0  0.0\n\njulia> x4 = cat([1 2 3; 4 5 6; 7 8 9]; dims=4);\n\njulia> Conv((2,2), 1 => 1, init=Flux.identity_init(gain=10), pad=SamePad())(x4)\n3×3×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 10.0  20.0  30.0\n 40.0  50.0  60.0\n 70.0  80.0  90.0 source"},{"id":516,"pagetitle":"Weight Initialisation","title":"Flux.ones32","ref":"/flux/stable/utilities/#Flux.ones32","content":" Flux.ones32  —  Function ones32(size...) = ones(Float32, size...) Return an  Array{Float32}  of the given  size  filled with 1s. source"},{"id":517,"pagetitle":"Weight Initialisation","title":"Flux.zeros32","ref":"/flux/stable/utilities/#Flux.zeros32","content":" Flux.zeros32  —  Function zeros32(size...) = zeros(Float32, size...) Return an  Array{Float32}  of the given  size  filled with 0s. source"},{"id":518,"pagetitle":"Weight Initialisation","title":"Flux.rand32","ref":"/flux/stable/utilities/#Flux.rand32","content":" Flux.rand32  —  Function rand32([rng], size...) Return an  Array{Float32}  of the given  size , filled like  rand . When the size is not provided,  rand32(rng::AbstractRNG)  returns a function. source"},{"id":519,"pagetitle":"Weight Initialisation","title":"Flux.randn32","ref":"/flux/stable/utilities/#Flux.randn32","content":" Flux.randn32  —  Function randn32([rng], size...) Return an  Array{Float32}  of the given  size , filled like  randn . When the size is not provided,  randn32(rng::AbstractRNG)  returns a function. source"},{"id":520,"pagetitle":"Weight Initialisation","title":"Flux.create_bias","ref":"/flux/stable/utilities/#Flux.create_bias","content":" Flux.create_bias  —  Function create_bias(weights, bias, size...) Return a bias parameter for a layer, based on the value given to the constructor's keyword  bias=bias . bias == true  creates a trainable array of the given size, of the same type as  weights , initialised to zero. bias == false  returns  false , which is understood by AD to be non-differentiable. bias::AbstractArray  uses the array provided, provided it has the correct size. It will also correct the  eltype  to match that of  weights . source These functions call:"},{"id":521,"pagetitle":"Weight Initialisation","title":"Flux.rng_from_array","ref":"/flux/stable/utilities/#Flux.rng_from_array","content":" Flux.rng_from_array  —  Function rng_from_array(x) Create an instance of the RNG most appropriate for  x . The current defaults are: x isa CuArray :  CUDA.default_rng() x isa AbstractArray : `Random.default_rng() source"},{"id":522,"pagetitle":"Weight Initialisation","title":"Flux.nfan","ref":"/flux/stable/utilities/#Flux.nfan","content":" Flux.nfan  —  Function nfan(n_out, n_in=1) -> Tuple\nnfan(dims...)\nnfan(dims::Tuple) For a layer characterized by dimensions  dims , return a tuple  (fan_in, fan_out) , where  fan_in  is the number of input neurons connected to an output one, and  fan_out  is the number of output neurons connected to an input one. This function is mainly used by weight initializers, e.g.,  kaiming_normal . Examples julia> layer = Dense(10, 20);\n\njulia> Flux.nfan(size(layer.weight))\n(10, 20)\n\njulia> layer = Conv((3, 3), 2=>10);\n\njulia> Flux.nfan(size(layer.weight))\n(18, 90) source"},{"id":523,"pagetitle":"Weight Initialisation","title":"Changing the type of all parameters","ref":"/flux/stable/utilities/#Changing-the-type-of-all-parameters","content":" Changing the type of all parameters The default  eltype  for models is  Float32  since models are often trained/run on GPUs. The  eltype  of model  m  can be changed to  Float64  by  f64(m) :"},{"id":524,"pagetitle":"Weight Initialisation","title":"Flux.f64","ref":"/flux/stable/utilities/#Flux.f64","content":" Flux.f64  —  Function f64(m) Converts the  eltype  of model's  floating point  parameters to  Float64 . Recurses into structs marked with  @functor . See also  f32  and  f16 . source"},{"id":525,"pagetitle":"Weight Initialisation","title":"Flux.f32","ref":"/flux/stable/utilities/#Flux.f32","content":" Flux.f32  —  Function f32(m) Converts the  eltype  of model's  floating point  parameters to  Float32  (which is Flux's default). Recurses into structs marked with  @functor . See also  f64  and  f16 . source"},{"id":526,"pagetitle":"Weight Initialisation","title":"Flux.f16","ref":"/flux/stable/utilities/#Flux.f16","content":" Flux.f16  —  Function f16(m) Converts the  eltype  of model's  floating point  parameters to  Float16 . Recurses into structs marked with  @functor . Support for  Float16  is limited on many CPUs. Julia may convert to  Float32  for each operation, which is slow. See also  f32  and  f64 . Example julia> m = Chain(Dense(784, 2048, relu), Dense(2048, 10))  # all Float32\nChain(\n  Dense(784 => 2048, relu),             # 1_607_680 parameters\n  Dense(2048 => 10),                    # 20_490 parameters\n)                   # Total: 4 arrays, 1_628_170 parameters, 6.211 MiB.\n\njulia> m |> f16  # takes half the memory\nChain(\n  Dense(784 => 2048, relu),             # 1_607_680 parameters\n  Dense(2048 => 10),                    # 20_490 parameters\n)                   # Total: 4 arrays, 1_628_170 parameters, 3.106 MiB. source"},{"id":529,"pagetitle":"Home","title":"NNlib.jl","ref":"/nnlib/stable/#NNlib.jl","content":" NNlib.jl NNlib  provides a library of functions useful for neural networks, such as softmax, sigmoid, batched multiplication, convolutions and pooling. Many of these are used by  Flux.jl , which loads this package, but they may be used independently. For use with automatic differentiation, this package defines gradients using  ChainRules.jl . These will be seen by various packages including  Zygote.jl . GPU support is provided as package extensions. In order to load the extensions, use the imports using NNlib, CUDA, cuDNN for CUDA support, or using NNlib, AMDGPU for AMDGPU support."},{"id":532,"pagetitle":"Reference","title":"Reference","ref":"/nnlib/stable/reference/#Reference","content":" Reference The API reference of  NNlib ."},{"id":533,"pagetitle":"Reference","title":"Activation Functions","ref":"/nnlib/stable/reference/#Activation-Functions","content":" Activation Functions Non-linearities that go between layers of your model. Note that, unless otherwise stated, activation functions operate on scalars. To apply them to an array you can call  σ.(xs) ,  relu.(xs)  and so on."},{"id":534,"pagetitle":"Reference","title":"NNlib.celu","ref":"/nnlib/stable/reference/#NNlib.celu","content":" NNlib.celu  —  Function celu(x, α=1) = x ≥ 0 ? x : α * (exp(x/α) - 1) Activation function from  \"Continuously Differentiable Exponential Linear Units\" . julia> lineplot(celu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ celu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠔⠒⠋⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⠤⠤⠤⠤⠔⠒⠒⠒⠊⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> celu(-10f0)\n-0.9999546f0 source"},{"id":535,"pagetitle":"Reference","title":"NNlib.elu","ref":"/nnlib/stable/reference/#NNlib.elu","content":" NNlib.elu  —  Function elu(x, α=1) = x > 0 ? x : α * (exp(x) - 1) Exponential Linear Unit activation function. See  \"Fast and Accurate Deep Network Learning by Exponential Linear Units\" . You can also specify the coefficient explicitly, e.g.  elu(x, 1) . julia> lineplot(elu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐       \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ elu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│       \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠔⠒⠋⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n        -1 │⠤⠤⠤⠤⠔⠒⠒⠒⠊⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           └────────────────────────────────────────┘       \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀       \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀       \n\njulia> elu(-10f0)\n-0.9999546f0\n\njulia> elu(-10f0, 2)\n-1.9999092f0 source"},{"id":536,"pagetitle":"Reference","title":"NNlib.gelu","ref":"/nnlib/stable/reference/#NNlib.gelu","content":" NNlib.gelu  —  Function gelu(x) = 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x^3))) Activation function from  \"Gaussian Error Linear Units\" . julia> lineplot(gelu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊│ gelu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⣀⡠⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⣤⣤⣤⣤⣤⣤⣤⣤⡤⠤⠤⠤⠤⠤⠤⠤⣤⣤⣤⡤⡧⠶⠶⠭⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> lineplot(gelu, -5, 0, height=7);\n\njulia> lineplot!(ans, swish)\n             ┌────────────────────────────────────────┐         \n           0 │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠒⠒⠤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ gelu(x) \n             │⠑⠒⠢⠤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇│ swish(x)\n             │⠀⠀⠀⠀⠀⠈⠉⠒⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⠁│         \n   f(x)      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇⠀│         \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⣄⠀⠀⠀⠀⠀⢠⡞⠀⠀│         \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⢄⣀⣀⡤⢣⠃⠀⠀│         \n        -0.2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠇⠀⠀⠀│         \n             └────────────────────────────────────────┘         \n             ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0⠀         \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":537,"pagetitle":"Reference","title":"NNlib.hardsigmoid","ref":"/nnlib/stable/reference/#NNlib.hardsigmoid","content":" NNlib.hardsigmoid  —  Function hardσ(x) = max(0, min(1, (x + 3) / 6)) Piecewise linear approximation of  sigmoid . julia> lineplot(hardsigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐         \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋⠉⠉⠉⠉⠉⠉⠉⠉│ hardσ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡠⠔⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⡗⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠋⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⠤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐     \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          └────────────────────────────────────────┘     \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀      source"},{"id":538,"pagetitle":"Reference","title":"NNlib.sigmoid_fast","ref":"/nnlib/stable/reference/#NNlib.sigmoid_fast","content":" NNlib.sigmoid_fast  —  Function sigmoid_fast(x) This is a faster, and very slightly less accurate, version of  sigmoid . For `x::Float32, perhaps 3 times faster, and maximum errors 2 eps instead of 1. See also  tanh_fast . julia> sigmoid(0.2f0)\n0.54983395f0\n\njulia> sigmoid_fast(0.2f0)\n0.54983395f0\n\njulia> hardσ(0.2f0)\n0.53333336f0 source"},{"id":539,"pagetitle":"Reference","title":"NNlib.hardtanh","ref":"/nnlib/stable/reference/#NNlib.hardtanh","content":" NNlib.hardtanh  —  Function hardtanh(x) = max(-1, min(1, x)) Segment-wise linear approximation of  tanh , much cheaper to compute. See  \"Large Scale Machine Learning\" . See also  tanh_fast . julia> lineplot(hardtanh, -2, 2, height=7)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⠔⠋⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ hardtanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⣀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡷⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠖⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠖⠋⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⠔⠋⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x\n\njulia> lineplot(tanh, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠤⠒⠒⠒⠊⠉⠉⠉│ tanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡷⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠔⠊⠁⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⣀⣀⣀⡠⠤⠤⠤⠖⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":540,"pagetitle":"Reference","title":"NNlib.tanh_fast","ref":"/nnlib/stable/reference/#NNlib.tanh_fast","content":" NNlib.tanh_fast  —  Function tanh_fast(x) This is a faster but slighly less accurate version of  tanh . Where Julia's  tanh  function has an error under 2 eps, this may be wrong by 5 eps, a reduction by less than one decimal digit.  For  x::Float32  this is usually about 10 times faster, with a smaller speedup for  x::Float64 . For any other number types, it just calls  tanh . See also  sigmoid_fast . julia> tanh(0.5f0)\n0.46211717f0\n\njulia> tanh_fast(0.5f0)\n0.46211714f0\n\njulia> hard_tanh(0.5f0)\n0.5f0 source"},{"id":541,"pagetitle":"Reference","title":"NNlib.leakyrelu","ref":"/nnlib/stable/reference/#NNlib.leakyrelu","content":" NNlib.leakyrelu  —  Function leakyrelu(x, a=0.01) = max(a*x, x) Leaky  Rectified Linear Unit  activation function. You can also specify the coefficient explicitly, e.g.  leakyrelu(x, 0.01) . julia> lineplot(x -> leakyrelu(x, 0.5), -2, 2, height=7)\n           ┌────────────────────────────────────────┐       \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ #42(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│       \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⣤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠤⠒⠒⠋⠉⠁⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n        -1 │⣀⣀⠤⠤⠒⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           └────────────────────────────────────────┘       \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀       \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀       \n\njulia> leakyrelu(-10f0, 0.2)\n-2.0f0\n\njulia> leakyrelu(-10f0, 0.02)\n-0.5f0 source"},{"id":542,"pagetitle":"Reference","title":"NNlib.lisht","ref":"/nnlib/stable/reference/#NNlib.lisht","content":" NNlib.lisht  —  Function lisht(x) = x * tanh(x) Activation function from   \"LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent ...\" julia> lineplot(lisht, -2, 2, height=7)\n          ┌────────────────────────────────────────┐         \n        2 │⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔│ lisht(x)\n          │⠀⠈⠑⢦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀│         \n          │⠀⠀⠀⠀⠈⠣⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠁⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⠔⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⢄⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡠⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⠦⣄⣀⣀⣇⣀⣀⠤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> lineplot!(ans, logcosh)\n          ┌────────────────────────────────────────┐           \n        2 │⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔│ lisht(x)  \n          │⠀⠈⠑⢦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀│ logcosh(x)\n          │⠢⣄⠀⠀⠈⠣⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⠀⠀⣀⠔│           \n   f(x)   │⠀⠈⠑⠢⣀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠁⠀⣀⠔⠊⠁⠀│           \n          │⠀⠀⠀⠀⠀⠉⠢⢄⡀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⠔⠋⠀⡠⠔⠋⠁⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠦⣌⡓⢄⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡠⠖⣁⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀│           \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠪⠷⣦⣄⣀⣀⣇⣀⣀⣤⠶⠕⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n          └────────────────────────────────────────┘           \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀           \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            source"},{"id":543,"pagetitle":"Reference","title":"NNlib.logcosh","ref":"/nnlib/stable/reference/#NNlib.logcosh","content":" NNlib.logcosh  —  Function logcosh(x) Return  log(cosh(x))  which is computed in a numerically stable way. julia> lineplot(logcosh, -5, 5, height=7)\n          ┌────────────────────────────────────────┐           \n        5 │⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ logcosh(x)\n          │⠉⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋│           \n          │⠀⠀⠀⠑⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⠀│           \n   f(x)   │⠀⠀⠀⠀⠀⠀⠑⠦⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠊⠁⠀⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⠦⡀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⠦⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠢⢄⣀⣀⣇⣀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n          └────────────────────────────────────────┘           \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀           \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            source"},{"id":544,"pagetitle":"Reference","title":"NNlib.logsigmoid","ref":"/nnlib/stable/reference/#NNlib.logsigmoid","content":" NNlib.logsigmoid  —  Function logσ(x) Return  log(σ(x))  which is computed in a numerically stable way. julia> lineplot(logsigmoid, -5, 5, height=7)\n           ┌────────────────────────────────────────┐        \n         0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡧⠤⠔⠒⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ logσ(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠉⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⢀⡤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⣀⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⡤⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -6 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":545,"pagetitle":"Reference","title":"NNlib.mish","ref":"/nnlib/stable/reference/#NNlib.mish","content":" NNlib.mish  —  Function mish(x) = x * tanh(softplus(x)) Activation function from  \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\" . julia> lineplot(mish, -5, 5, height=7)\n           ┌────────────────────────────────────────┐        \n         5 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋│ mish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠒⠁⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠔⠋⠁⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣧⣔⣊⣁⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀│        \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":546,"pagetitle":"Reference","title":"NNlib.relu","ref":"/nnlib/stable/reference/#NNlib.relu","content":" NNlib.relu  —  Function relu(x) = max(0, x) Rectified Linear Unit  activation function. julia> lineplot(relu, -2, 2, height=7)\n          ┌────────────────────────────────────────┐        \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋│ relu(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠊⠁⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀⠀⠀⠀⠀│        \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⡠⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⠔⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n          └────────────────────────────────────────┘        \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":547,"pagetitle":"Reference","title":"NNlib.relu6","ref":"/nnlib/stable/reference/#NNlib.relu6","content":" NNlib.relu6  —  Function relu6(x) = min(max(0, x), 6) Rectified Linear Unit  activation function capped at 6. See  \"Convolutional Deep Belief Networks\"  from CIFAR-10. julia> lineplot(relu6, -10, 10, height=7)\n          ┌────────────────────────────────────────┐         \n        6 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠎⠉⠉⠉⠉⠉⠉⠉⠉│ relu6(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⡤⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⡠⠎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡔⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡧⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-10⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀10⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":548,"pagetitle":"Reference","title":"NNlib.rrelu","ref":"/nnlib/stable/reference/#NNlib.rrelu","content":" NNlib.rrelu  —  Function rrelu(x, lo=1/8, hi=1/3) = max(a*x, x)\n# where `a` is randomly sampled from uniform distribution `U(lo, hi)` Randomized Leaky Rectified Linear Unit activation function. See  \"Empirical Evaluation of Rectified Activations\"  You can also specify the bound explicitly, e.g.  rrelu(x, 0.0, 1.0) . julia> lineplot(rrelu, -20, 10, height=7)\n            ┌────────────────────────────────────────┐         \n         10 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋│ rrelu(x)\n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀│         \n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)     │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⠤⣤⣤⢤⣤⣤⠤⠤⠤⢼⠮⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│         \n            │⣰⢀⣆⡄⣄⡄⡠⡰⠦⠷⡜⢢⠷⠳⠢⠊⠉⠉⠀⠀⠁⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n            │⠃⠉⠙⠘⠃⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        -10 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n            └────────────────────────────────────────┘         \n            ⠀-20⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀10⠀         \n            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> extrema(rrelu.(fill(-10f0, 1000)))\n(-3.3316886f0, -1.2548422f0) source"},{"id":549,"pagetitle":"Reference","title":"NNlib.selu","ref":"/nnlib/stable/reference/#NNlib.selu","content":" NNlib.selu  —  Function selu(x) = λ * (x ≥ 0 ? x : α * (exp(x) - 1))\n\nλ ≈ 1.05070...\nα ≈ 1.67326... Scaled exponential linear units. See  \"Self-Normalizing Neural Networks\" . julia> lineplot(selu, -3, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ selu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⠒│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⣀⠤⠖⠊⠉⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⡠⠤⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⣉⠭⠛⡏⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⡤⠤⠒⠊⠉⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -2 │⠤⠤⠖⠒⠒⠒⠒⠒⠒⠒⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> selu(-10f0)\n-1.7580194f0 source"},{"id":550,"pagetitle":"Reference","title":"NNlib.sigmoid","ref":"/nnlib/stable/reference/#NNlib.sigmoid","content":" NNlib.sigmoid  —  Function σ(x) = 1 / (1 + exp(-x)) Classic  sigmoid  activation function. Unicode  σ  can be entered as  \\sigma  then tab, in many editors. The ascii name  sigmoid  is also exported. See also  sigmoid_fast . julia> using UnicodePlots\n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐     \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          └────────────────────────────────────────┘     \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀     \n\njulia> sigmoid === σ\ntrue source"},{"id":551,"pagetitle":"Reference","title":"NNlib.softplus","ref":"/nnlib/stable/reference/#NNlib.softplus","content":" NNlib.softplus  —  Function softplus(x) = log(exp(x) + 1) See  \"Deep Sparse Rectifier Neural Networks\" , JMLR 2011. julia> lineplot(softplus, -3, 3, height=7)\n          ┌────────────────────────────────────────┐            \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ softplus(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀│            \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠔⠊⠁⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡠⠤⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠤⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        0 │⣀⣀⣀⣀⣀⣀⣀⡠⠤⠤⠤⠤⠔⠒⠒⠚⠉⠉⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          └────────────────────────────────────────┘            \n          ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀            \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> lineplot!(ans, relu)\n          ┌────────────────────────────────────────┐            \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ softplus(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠│ relu(x)    \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⡴⠞⠋⠁│            \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⡴⠞⠋⠁⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡠⢤⡲⠝⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠤⠒⠊⣉⠥⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        0 │⣀⣀⣀⣀⣀⣀⣀⣠⣤⣤⣤⣤⣔⣒⣒⣚⣉⣉⣁⣀⣇⠴⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          └────────────────────────────────────────┘            \n          ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀            \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> softplus(16f0)\n16.0f0 source"},{"id":552,"pagetitle":"Reference","title":"NNlib.softshrink","ref":"/nnlib/stable/reference/#NNlib.softshrink","content":" NNlib.softshrink  —  Function softshrink(x, λ=0.5) =\n    (x ≥ λ ? x - λ : (-λ ≥ x ? x + λ : 0)) See  \"Softshrink Activation Function\" . julia> lineplot(softshrink, -2, 2, height=7)\n           ┌────────────────────────────────────────┐              \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀│ softshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡤⠔⠒⠉⠁│              \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⠒⠋⠁⠀⠀⠀⠀⠀⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⡤⠤⠤⠤⠤⠤⠤⡧⠤⠤⠤⠤⠶⠮⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⠀⠀⠀⠀⠀⢀⣀⠤⠖⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⠀⣀⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -2 │⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀              \n\njulia> lineplot!(ans, tanhshrink)\n           ┌────────────────────────────────────────┐              \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀│ softshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡤⠔⠒⣉⡡│ tanhshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⣒⣋⠥⠤⠒⠊⠉⠁⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⣤⣤⣤⡤⠤⠤⠤⠤⠤⠤⡷⠶⠶⠶⠶⠶⠾⠿⠯⠭⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⢀⣀⡠⠤⠖⢒⣋⠭⠗⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⠊⣉⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -2 │⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀\n\njulia> softshrink.((-10f0, 10f0))\n(-9.5f0, 9.5f0) source"},{"id":553,"pagetitle":"Reference","title":"NNlib.softsign","ref":"/nnlib/stable/reference/#NNlib.softsign","content":" NNlib.softsign  —  Function softsign(x) = x / (1 + |x|) See  \"Quadratic Polynomials Learn Better Image Features\"  (2009). julia> lineplot(softsign, -5, 5, height=7)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⠤⠤⠤⠤⠤│ softsign(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⡤⠖⠒⠋⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⡔⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⠤⠤⠒⠋⠁⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⠒⠒⠒⠒⠒⠊⠉⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> lineplot!(ans, tanh)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡤⠖⠊⠉⠉⠉⣉⣉⣉⣉⣉⠭⠭⠭⠭⠭│ softsign(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡔⣃⡤⠖⠒⠋⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│ tanh(x)    \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣧⡞⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡴⠃⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⠤⠤⠒⢋⠕⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⣒⣒⣒⣒⣒⣊⣉⣉⣉⣉⣁⣀⣀⡠⠤⠒⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> softsign(1f0)\n0.5f0\n\njulia> softsign(100f0)\n0.990099f0 source"},{"id":554,"pagetitle":"Reference","title":"NNlib.swish","ref":"/nnlib/stable/reference/#NNlib.swish","content":" NNlib.swish  —  Function swish(x) = x * σ(x) Self-gated activation function. See  \"Swish: a Self-Gated Activation Function\" . julia> lineplot(swish, -2, 2, height=7)\n           ┌────────────────────────────────────────┐         \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤│ swish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋⠁⠀│         \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀│         \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⢀⣀⡤⠔⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⣤⣤⡤⡧⠴⠶⠯⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│         \n           │⠉⠑⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠉⠉⠉⠉⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n           └────────────────────────────────────────┘         \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀         \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":555,"pagetitle":"Reference","title":"NNlib.hardswish","ref":"/nnlib/stable/reference/#NNlib.hardswish","content":" NNlib.hardswish  —  Function hardswish(x) = x * hardσ(x) Hard-Swish activation function. See  \"Searching for MobileNetV3\" . julia> lineplot(hardswish, -2, 5, height = 7)\n           ┌────────────────────────────────────────┐             \n         5 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠔⠒⠉│ hardswish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠔⠒⠉⠁⠀⠀⠀⠀│             \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠖⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⣤⣤⣖⣚⣉⣁⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀│             \n        -1 │⠉⠒⠒⠒⠒⠉⠉⠉⠉⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           └────────────────────────────────────────┘             \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀             \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀             \n\njulia> lineplot(hardswish, -4, 0, height = 7);\n\njulia> lineplot!(ans, swish)\n             ┌────────────────────────────────────────┐             \n           0 │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⢣⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡜│ hardswish(x)\n             │⠒⠒⠢⠤⢄⣀⡀⠀⠀⠀⠀⠱⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠎⠀│ swish(x)    \n             │⠀⠀⠀⠀⠀⠀⠈⠉⠑⠒⠦⢄⣘⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡴⠃⠀⠀│             \n   f(x)      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⡖⠦⢄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⢔⠏⠁⠀⠀⠀│             \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠣⣄⠀⠉⠑⠒⠦⠤⢄⣀⣀⣀⣀⡠⠤⠖⣊⠕⠁⠀⠀⠀⠀⠀│             \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⠤⡀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀│             \n        -0.4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠒⠢⠤⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n             └────────────────────────────────────────┘             \n             ⠀-4⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0⠀             \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀             \n\njulia> hardswish.(-5:5)'\n1×11 adjoint(::Vector{Float64}) with eltype Float64:\n -0.0  -0.0  -0.0  -0.333333  -0.333333  0.0  0.666667  1.66667  3.0  4.0  5.0 source"},{"id":556,"pagetitle":"Reference","title":"NNlib.tanhshrink","ref":"/nnlib/stable/reference/#NNlib.tanhshrink","content":" NNlib.tanhshrink  —  Function tanhshrink(x) = x - tanh(x) See  \"Tanhshrink Activation Function\" . julia> lineplot(tanhshrink, -3, 3, height=7)\n           ┌────────────────────────────────────────┐              \n         3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ tanhshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠊│              \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⣀⡠⠤⠒⠊⠉⠁⠀⠀⠀⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⣤⡤⠤⠤⠤⠤⠤⠤⡷⠶⠶⠶⠶⠶⠮⠭⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⠀⠀⠀⠀⣀⡠⠴⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⡠⠴⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀              \n\njulia> tanhshrink.((-10f0, 10f0))\n(-9.0f0, 9.0f0) source"},{"id":557,"pagetitle":"Reference","title":"NNlib.trelu","ref":"/nnlib/stable/reference/#NNlib.trelu","content":" NNlib.trelu  —  Function trelu(x, theta=1) = x > theta ? x : 0 Threshold gated rectified linear activation function. See  \"Zero-bias autoencoders and the benefits of co-adapting features\" julia> lineplot(trelu, -2, 4, height=7)\n          ┌────────────────────────────────────────┐         \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋│ trelu(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠴⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣠⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⣀⣀⣀⣀⣀⣀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀4⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":558,"pagetitle":"Reference","title":"Attention","ref":"/nnlib/stable/reference/#Attention","content":" Attention"},{"id":559,"pagetitle":"Reference","title":"NNlib.dot_product_attention","ref":"/nnlib/stable/reference/#NNlib.dot_product_attention","content":" NNlib.dot_product_attention  —  Function dot_product_attention(query, key, value, [bias]; [fdrop, mask, nheads]) Multihead dot product attention used in transformer architectures. The input arrays must have the first two dimensions given by the number of features and the sequence length, then an arbitrary number of batch dimensions or none. Returns the attention output array of size  (v_dim, q_len, batch_size...)  and the attention scores of size  (kv_len, q_len, nheads, batch_size...) . See also  dot_product_attention_scores  if you only need the attention scores. Arguments query : Query array of size  (qk_dim, q_len, batch_size...) . key : Key array of size  (qk_dim, kv_len, batch_size...) . value : Value array of size  (v_dim, kv_len, batch_size...) . bias : Either  nothing  or an array broadcastable to size  (kv_len, q_len, nheads, batch_size) .         It will be added to the attention scores before applying the softmax. Default  nothing . fdrop : A dropout function or layer to be applied on the attention scores right after the softmax.          Default  identity  (no dropout). mask : Either  nothing  or a boolean array broadcastable to size  (kv_len, q_len, nheads, batch_size) .         The mask is applied to the attention scores just before the softmax.         See  make_causal_mask  fore creating causal masks. Default  nothing . nheads : Number of heads to split the input arrays into. Default  1 . Examples q, k, v = rand(10, 20, 2), rand(10, 30, 2), rand(20, 30, 2)\ny, α = dot_product_attention(q, k, v) source"},{"id":560,"pagetitle":"Reference","title":"NNlib.dot_product_attention_scores","ref":"/nnlib/stable/reference/#NNlib.dot_product_attention_scores","content":" NNlib.dot_product_attention_scores  —  Function dot_product_attention_scores(query, key, [bias]; [fdrop, mask]) Return the attention scores for the  dot_product_attention . Input arrays must have dimensions  (num_features ÷ nheads, nheads, sequence_length, batch_size) . See  dot_product_attention  for more details. source"},{"id":561,"pagetitle":"Reference","title":"NNlib.make_causal_mask","ref":"/nnlib/stable/reference/#NNlib.make_causal_mask","content":" NNlib.make_causal_mask  —  Function make_causal_mask(x, dims=2) Return a boolean square matrix  m  of the same type as  x  and of side  size(x, dims) . Its elements are set such that  m[i, j] == i ≤ j . Can be used to mask the attention scores in  dot_product_attention . source"},{"id":562,"pagetitle":"Reference","title":"Softmax","ref":"/nnlib/stable/reference/#Softmax","content":" Softmax Flux 's  logitcrossentropy  uses  NNlib.softmax  internally."},{"id":563,"pagetitle":"Reference","title":"NNlib.softmax","ref":"/nnlib/stable/reference/#NNlib.softmax","content":" NNlib.softmax  —  Function softmax(x; dims = 1) Softmax  turns input array  x  into probability distributions that sum to 1 along the dimensions specified by  dims . It is semantically equivalent to the following: softmax(x; dims = 1) = exp.(x) ./ sum(exp.(x), dims = dims) with additional manipulations enhancing numerical stability. For a matrix input  x  it will by default ( dims = 1 ) treat it as a batch of vectors, with each column independent. Keyword  dims = 2  will instead treat rows independently, and so on. See also  logsoftmax . Examples julia> softmax([1, 2, 3])\n3-element Vector{Float64}:\n 0.09003057317038046\n 0.24472847105479764\n 0.6652409557748218\n\njulia> softmax([1 2 3; 2 2 2])  # dims=1\n2×3 Matrix{Float64}:\n 0.268941  0.5  0.731059\n 0.731059  0.5  0.268941\n\njulia> softmax([1 2 3; 2 2 2]; dims=2)\n2×3 Matrix{Float64}:\n 0.0900306  0.244728  0.665241\n 0.333333   0.333333  0.333333 Note that, when used with Flux.jl,  softmax  must not be passed to layers like  Dense  which accept an activation function. The activation is broadcasted over the result, thus applies to individual numbers. But  softmax  always needs to see the whole column. julia> using Flux\n\njulia> x = randn(Float32, 4, 4, 3, 13);\n\njulia> model = Chain(Conv((4, 4), 3 => 8, tanh), Flux.flatten, Dense(8 => 7), softmax);\n\njulia> model(x) |> size\n(7, 13)\n\njulia> Dense(4 => 7, softmax)(x)\nERROR: `softmax(x)` called with a number, but it expects an array.  source"},{"id":564,"pagetitle":"Reference","title":"NNlib.logsoftmax","ref":"/nnlib/stable/reference/#NNlib.logsoftmax","content":" NNlib.logsoftmax  —  Function logsoftmax(x; dims = 1) Computes the log of softmax in a more numerically stable way than directly taking  log.(softmax(xs)) . Commonly used in computing cross entropy loss. It is semantically equivalent to the following: logsoftmax(x; dims = 1) = x .- log.(sum(exp.(x), dims = dims)) See also  softmax . source"},{"id":565,"pagetitle":"Reference","title":"Pooling","ref":"/nnlib/stable/reference/#Pooling","content":" Pooling Flux 's  AdaptiveMaxPool ,  AdaptiveMeanPool ,  GlobalMaxPool ,  GlobalMeanPool ,  MaxPool ,  MeanPool  and  lpnormpool  use  NNlib.PoolDims ,  NNlib.maxpool ,  NNlib.meanpool  and  NNlib.lpnormpool  as their backend."},{"id":566,"pagetitle":"Reference","title":"NNlib.PoolDims","ref":"/nnlib/stable/reference/#NNlib.PoolDims","content":" NNlib.PoolDims  —  Type PoolDims(x_size::NTuple{M}, k::Union{NTuple{L, Int}, Int};\n        stride=k, padding=0, dilation=1)  where {M, L} Dimensions for a \"pooling\" operation that can have an arbitrary input size, kernel size, stride, dilation, and channel count.  Used to dispatch onto efficient implementations at compile-time. source"},{"id":567,"pagetitle":"Reference","title":"NNlib.maxpool","ref":"/nnlib/stable/reference/#NNlib.maxpool","content":" NNlib.maxpool  —  Function maxpool(x, k::NTuple{N, Integer}; pad=0, stride=k) Perform max pool operation with window size  k  on input tensor  x . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always  length(k) == ndim(x) - 2 pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k . source"},{"id":568,"pagetitle":"Reference","title":"NNlib.meanpool","ref":"/nnlib/stable/reference/#NNlib.meanpool","content":" NNlib.meanpool  —  Function meanpool(x, k::NTuple{N, Integer}; pad=0, stride=k) Perform mean pool operation with window size  k  on input tensor  x . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always length(k) == ndim(x) - 2` pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k . source"},{"id":569,"pagetitle":"Reference","title":"NNlib.lpnormpool","ref":"/nnlib/stable/reference/#NNlib.lpnormpool","content":" NNlib.lpnormpool  —  Function lpnormpool(x, p::Real, k::NTuple{N, Integer}; pad=0, stride=k) Perform Lp pool operation with value of the Lp norm  p  and window size  k  on input tensor  x , also known as LPPool in pytorch. This pooling operator from  Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always length(k) == ndim(x) - 2` p  is restricted to  0 < p < Inf . pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k . For all elements  x  in a size  k  window, lpnormpool computes  (∑ᵢ xᵢ^p)^(1 / p)  as an element of the output. Thus  lpnormpool(x, 1, k) ./ prod(k) ≈ meanpool(x, k)  and  lpnormpool(x, 2, k).^2 ./ prod(k) ≈ meanpool(x.^2, k) . source"},{"id":570,"pagetitle":"Reference","title":"Padding","ref":"/nnlib/stable/reference/#Padding","content":" Padding"},{"id":571,"pagetitle":"Reference","title":"NNlib.pad_reflect","ref":"/nnlib/stable/reference/#NNlib.pad_reflect","content":" NNlib.pad_reflect  —  Function pad_reflect(x, pad::Tuple; [dims])\npad_reflect(x, pad::Int; [dims]) Pad the array  x  reflecting its values across the border. pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_repeat ,  pad_symmetric ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_reflect(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 5  2  5  8  5  2\n 4  1  4  7  4  1\n 5  2  5  8  5  2\n 6  3  6  9  6  3\n 5  2  5  8  5  2\n 4  1  4  7  4  1 source"},{"id":572,"pagetitle":"Reference","title":"NNlib.pad_symmetric","ref":"/nnlib/stable/reference/#NNlib.pad_symmetric","content":" NNlib.pad_symmetric  —  Function pad_symmetric(x, pad::Tuple; [dims])\npad_symmetric(x, pad::Int; [dims]) Pad the array  x  reflecting its values symmetrically across the border, i.e. the border values of  x  are present in the padding values, in contrast to  pad_reflect . pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_repeat ,  pad_reflect ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_symmetric(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 1  1  4  7  7  4\n 1  1  4  7  7  4\n 2  2  5  8  8  5\n 3  3  6  9  9  6\n 3  3  6  9  9  6\n 2  2  5  8  8  5 source"},{"id":573,"pagetitle":"Reference","title":"NNlib.pad_circular","ref":"/nnlib/stable/reference/#NNlib.pad_circular","content":" NNlib.pad_circular  —  Function pad_circular(x, pad::Tuple; [dims])\npad_circular(x, pad::Int; [dims]) Pad the array  x  \"circularly\" across the border by wrapping around values from the opposite side of  x .  pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  The pad length on either side in any dimension must not exceed the size of  x  in that dimension, i.e.  pad_circular  is not able to create abitrary sized tilings of  x . See also  pad_repeat ,  pad_reflect ,  pad_symmetric , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_circular(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 9  3  6  9  3  6\n 7  1  4  7  1  4\n 8  2  5  8  2  5\n 9  3  6  9  3  6\n 7  1  4  7  1  4\n 8  2  5  8  2  5 source"},{"id":574,"pagetitle":"Reference","title":"NNlib.pad_repeat","ref":"/nnlib/stable/reference/#NNlib.pad_repeat","content":" NNlib.pad_repeat  —  Function pad_repeat(x, pad::Tuple; [dims])\npad_repeat(x, pad::Int; [dims]) Pad the array  x  repeating the values on the border. pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_reflect ,  pad_symmetric ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_repeat(r, (1,2,3,4))\n6×10 Matrix{Int64}:\n 1  1  1  1  4  7  7  7  7  7\n 1  1  1  1  4  7  7  7  7  7\n 2  2  2  2  5  8  8  8  8  8\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9 source"},{"id":575,"pagetitle":"Reference","title":"NNlib.pad_constant","ref":"/nnlib/stable/reference/#NNlib.pad_constant","content":" NNlib.pad_constant  —  Function pad_constant(x, pad::Tuple, val = 0; [dims = :])\npad_constant(x, pad::Int, val = 0; [dims = :]) Pad the array  x  with the constant value  val . pad  can be a tuple of integers. If it is of some length  2 * length(dims)  that specifies the left and right padding size for each of the dimensions in  dims  as  (l1, r1, ..., ln, rn) .  If supplied with a tuple of length  length(dims)  instead, it applies symmetric padding. If  dims  is not given, it defaults to all dimensions. For integer  pad  input, it is applied on both sides on every dimension in  dims . See also  pad_zeros ,  pad_repeat ,  pad_reflect ,  pad_symmetric , and  pad_circular . julia> r = reshape(1:4, 2, 2)\n2×2 reshape(::UnitRange{Int64}, 2, 2) with eltype Int64:\n 1  3\n 2  4\n\njulia> pad_constant(r, (1, 2, 3, 4), 8)\n5×9 Matrix{Int64}:\n 8  8  8  8  8  8  8  8  8\n 8  8  8  1  3  8  8  8  8\n 8  8  8  2  4  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n\njulia> pad_constant(r, 1, 8)\n4×4 Matrix{Int64}:\n 8  8  8  8\n 8  1  3  8\n 8  2  4  8\n 8  8  8  8\n\njulia> r = reshape(1:27, 3, 3, 3)\n3×3×3 reshape(::UnitRange{Int64}, 3, 3, 3) with eltype Int64:\n[:, :, 1] =\n 1  4  7\n 2  5  8\n 3  6  9\n\n[:, :, 2] =\n 10  13  16\n 11  14  17\n 12  15  18\n\n[:, :, 3] =\n 19  22  25\n 20  23  26\n 21  24  27\n\njulia> pad_constant(r, (2,1), dims = 1) # assymetric padding\n6×3×3 Array{Int64, 3}:\n[:, :, 1] =\n 0  0  0\n 0  0  0\n 1  4  7\n 2  5  8\n 3  6  9\n 0  0  0\n\n[:, :, 2] =\n  0   0   0\n  0   0   0\n 10  13  16\n 11  14  17\n 12  15  18\n  0   0   0\n\n[:, :, 3] =\n  0   0   0\n  0   0   0\n 19  22  25\n 20  23  26\n 21  24  27\n  0   0   0\n\njulia> pad_constant(r, (2,1, 3), dims = (1,2)) # padding must always be either the same length as dims, or double it\nERROR: ArgumentError: Could not parse padding (2, 1, 3) and dims (1, 2)\nStacktrace:\n[...] source"},{"id":576,"pagetitle":"Reference","title":"NNlib.pad_zeros","ref":"/nnlib/stable/reference/#NNlib.pad_zeros","content":" NNlib.pad_zeros  —  Function pad_zeros(x, pad::Tuple; [dims])\npad_zeros(x, pad::Int; [dims]) Pad the array  x  with zeros. Equivalent to  pad_constant  with the constant equal to 0.  source"},{"id":577,"pagetitle":"Reference","title":"Convolution","ref":"/nnlib/stable/reference/#Convolution","content":" Convolution Flux 's  Conv  and  CrossCor  layers use  NNlib.DenseConvDims  and  NNlib.conv  internally. NNlib.conv  supports complex datatypes on CPU and CUDA devices. !!! AMDGPU MIOpen supports only cross-correlation (flipkernel=true).     Therefore for every regular convolution (flipkernel=false)     kernel is flipped before calculation.     For better performance, use cross-correlation (flipkernel=true)     and manually flip the kernel before  NNlib.conv  call.      Flux  handles this automatically, this is only required for direct calls."},{"id":578,"pagetitle":"Reference","title":"NNlib.conv","ref":"/nnlib/stable/reference/#NNlib.conv","content":" NNlib.conv  —  Function conv(x, w; stride = 1, pad = 0, dilation = 1, flipped = false, groups = 1) Apply convolution filter  w  to input  x .  x  and  w  are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively.  x  and  w  may have real or complex element types. source"},{"id":579,"pagetitle":"Reference","title":"NNlib.ConvDims","ref":"/nnlib/stable/reference/#NNlib.ConvDims","content":" NNlib.ConvDims  —  Type ConvDims Type system-level information about convolution dimensions. Critical for things like  im2col!()  to generate efficient code, and helpful to reduce the number of kwargs getting passed around. source"},{"id":580,"pagetitle":"Reference","title":"NNlib.depthwiseconv","ref":"/nnlib/stable/reference/#NNlib.depthwiseconv","content":" NNlib.depthwiseconv  —  Function depthwiseconv(x, w; stride=1, pad=0, dilation=1, flipped=false) Depthwise convolution operation with filter  w  on input  x .  x  and  w  are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively. source"},{"id":581,"pagetitle":"Reference","title":"NNlib.DepthwiseConvDims","ref":"/nnlib/stable/reference/#NNlib.DepthwiseConvDims","content":" NNlib.DepthwiseConvDims  —  Type DepthwiseConvDims Concrete subclass of  ConvDims  for a depthwise convolution.  Differs primarily due to characterization by C in, C mult, rather than C in, C out.  Useful to be separate from DenseConvDims primarily for channel calculation differences. source"},{"id":582,"pagetitle":"Reference","title":"NNlib.DenseConvDims","ref":"/nnlib/stable/reference/#NNlib.DenseConvDims","content":" NNlib.DenseConvDims  —  Type DenseConvDims Concrete subclass of  ConvDims  for a normal, dense, conv2d/conv3d. source"},{"id":583,"pagetitle":"Reference","title":"NNlib.unfold","ref":"/nnlib/stable/reference/#NNlib.unfold","content":" NNlib.unfold  —  Function unfold(x, kernel_size; stride = 1, pad = 0, dilation = 0, flipped = true) Places sliding windows of x into a container tensor of size  (num_windows, window_size, batchsize) . The window size is determined by the  prod(spatial dims of kernel)*input_channels . The number of sliding windows will match those of convolution ( conv ) with the same kernel_size and arguments. Note that by default  conv  flips the spatial dimensions of its kernel (default  flipped=false ), whereas  unfold  does not (default  flipped=true ).  Uses  NNlib.im2col!  as backend.  See also  fold , the adjoint/transpose operator  and a potential inverse of  unfold . Example The below example demonstrates that  unfold  uses the same sliding windows as  conv . In general  batched_mul  +  unfold  should not be used to achieve convolution. julia> x = reshape([100 2 3 40 5 6 700], 7, 1, 1);  # 1D data, 1 channel, batch of 1\n\njulia> w = reshape([1 0 -1], 3, 1, 1);  # 1D conv kernel of length 3\n\njulia> kws = (pad=1, stride=2, flipped=true);  # use same args for conv and unfold\n\njulia> z = NNlib.unfold(x, size(w); kws...) \n4×3×1 Array{Int64, 3}:\n[:, :, 1] =\n  0  100   2\n  2    3  40\n 40    5   6\n  6  700   0\n\njulia> y1 = conv(x, w; kws...)\n4×1×1 Array{Int64, 3}:\n[:, :, 1] =\n  -2\n -38\n  34\n   6\n\njulia> y2 = z ⊠ w  # ⊠ (\\boxtimes) is NNlib.batched_mul\n4×1×1 Array{Int64, 3}:\n[:, :, 1] =\n  -2\n -38\n  34\n   6 source"},{"id":584,"pagetitle":"Reference","title":"NNlib.fold","ref":"/nnlib/stable/reference/#NNlib.fold","content":" NNlib.fold  —  Function fold(y, output_size, kernel_size; stride = 1, pad = 0, dilation = 0, flipped = true) The adjoint/transpose operator of  unfold . It accumulates sliding windows from the output of  unfold  into a container tensor of size  output_size . An inverse to  unfold  may be obtained (in some cases) by using  fold  and accounting for scaling issues  with a divisor (see example). Uses  NNlib.col2im!  as backend.  See also  unfold . Example julia> x = reshape([100 2 3 40 5 6 700], 7, 1, 1);  # 1D data, 1 channel, batch of 1\n\njulia> y = NNlib.unfold(x, (3,1,1))  # sliding window of size 3\n5×3×1 Array{Int64, 3}:\n[:, :, 1] =\n 100   2    3\n   2   3   40\n   3  40    5\n  40   5    6\n   5   6  700\n\njulia> z = NNlib.fold(y, size(x), (3,1,1))  # sum of contributions in y. 100 appears once, 40 three times\n7×1×1 Array{Int64, 3}:\n[:, :, 1] =\n 100\n   4\n   9\n 120\n  15\n  12\n 700\n\njulia> divisor = NNlib.fold(NNlib.unfold(ones(size(x)...), (3,1,1)), size(x), (3,1,1))\n7×1×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.0\n 2.0\n 3.0\n 3.0\n 3.0\n 2.0\n 1.0\n\njulia> z ./ divisor \n7×1×1 Array{Float64, 3}:\n[:, :, 1] =\n 100.0\n   2.0\n   3.0\n  40.0\n   5.0\n   6.0\n 700.0 In general, an inverse to  unfold  does not exist if  divisor  contains zeros. source"},{"id":585,"pagetitle":"Reference","title":"Upsampling","ref":"/nnlib/stable/reference/#Upsampling","content":" Upsampling Flux 's  Upsample  layer uses  NNlib.upsample_nearest ,  NNlib.upsample_bilinear , and  NNlib.upsample_trilinear  as its backend. Additionally,  Flux 's  PixelShuffle  layer uses  NNlib.pixel_shuffle  as its backend."},{"id":586,"pagetitle":"Reference","title":"NNlib.upsample_nearest","ref":"/nnlib/stable/reference/#NNlib.upsample_nearest","content":" NNlib.upsample_nearest  —  Function upsample_nearest(x, scale::NTuple{S,Int})\nupsample_nearest(x; size::NTuple{S,Int}) Upsamples the array  x  by integer multiples along the first  S  dimensions. Subsequent dimensions of  x  are not altered. Either the  scale  factors or the final output  size  can be specified. See also  upsample_bilinear , for two dimensions of an  N=4  array. Example julia> upsample_nearest([1 2 3; 4 5 6], (2, 3))\n4×9 Matrix{Int64}:\n 1  1  1  2  2  2  3  3  3\n 1  1  1  2  2  2  3  3  3\n 4  4  4  5  5  5  6  6  6\n 4  4  4  5  5  5  6  6  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6]; size=(4, 9))  # equivalent\ntrue\n\njulia> upsample_nearest([1 2 3; 4 5 6], (2,))\n4×3 Matrix{Int64}:\n 1  2  3\n 1  2  3\n 4  5  6\n 4  5  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6], size=(4,))\ntrue source Missing docstring. Missing docstring for  ∇upsample_nearest . Check Documenter's build log for details."},{"id":587,"pagetitle":"Reference","title":"NNlib.upsample_linear","ref":"/nnlib/stable/reference/#NNlib.upsample_linear","content":" NNlib.upsample_linear  —  Function upsample_linear(x::AbstractArray{T,3}, scale::Real; align_corners::Bool = true)\nupsample_linear(x::AbstractArray{T,3}; size::Integer, align_corners::Bool = true) Upsamples the first dimension of the array  x  by the upsample provided  scale , using linear interpolation. As an alternative to using  scale , the resulting array  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale*S1, S2, S3) , where  S1, S2, S3 = size(x) . source"},{"id":588,"pagetitle":"Reference","title":"NNlib.∇upsample_linear","ref":"/nnlib/stable/reference/#NNlib.∇upsample_linear","content":" NNlib.∇upsample_linear  —  Function ∇upsample_linear(Δ::AbstractArray{T,3}; size::Integer, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Size of the image upsampled in the first place Outputs dx : Downsampled version of  Δ source"},{"id":589,"pagetitle":"Reference","title":"NNlib.upsample_bilinear","ref":"/nnlib/stable/reference/#NNlib.upsample_bilinear","content":" NNlib.upsample_bilinear  —  Function upsample_bilinear(x::AbstractArray{T,4}, scale::NTuple{2,Real}; align_corners::Bool = true)\nupsample_bilinear(x::AbstractArray{T,4}; size::NTuple{2,Integer}, align_corners::Bool = true) Upsamples the first 2 dimensions of the array  x  by the upsample factors stored in  scale , using bilinear interpolation. As an alternative to using  scale , the resulting image  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale[1]*S1, scale[2]*S2, S3, S4) , where  S1, S2, S3, S4 = size(x) . Examples julia> x = reshape(Float32[1 2 3; 4 5 6], (2,3,1,1))\n2×3×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  2.0  3.0\n 4.0  5.0  6.0\n\njulia> upsample_bilinear(x, (2, 3))\n4×9×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  1.25  1.5  1.75  2.0  2.25  2.5  2.75  3.0\n 2.0  2.25  2.5  2.75  3.0  3.25  3.5  3.75  4.0\n 3.0  3.25  3.5  3.75  4.0  4.25  4.5  4.75  5.0\n 4.0  4.25  4.5  4.75  5.0  5.25  5.5  5.75  6.0\n\njulia> ans == upsample_bilinear(x; size=(4, 9))  # specify ouput size instead\ntrue\n\njulia> upsample_bilinear(x, (2.5, 3.5))  # non-integer scaling factors are allowed\n5×10×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0   1.22222  1.44444  1.66667  1.88889  …  2.33333  2.55556  2.77778  3.0\n 1.75  1.97222  2.19444  2.41667  2.63889     3.08333  3.30556  3.52778  3.75\n 2.5   2.72222  2.94444  3.16667  3.38889     3.83333  4.05556  4.27778  4.5\n 3.25  3.47222  3.69444  3.91667  4.13889     4.58333  4.80556  5.02778  5.25\n 4.0   4.22222  4.44444  4.66667  4.88889     5.33333  5.55556  5.77778  6.0 source"},{"id":590,"pagetitle":"Reference","title":"NNlib.∇upsample_bilinear","ref":"/nnlib/stable/reference/#NNlib.∇upsample_bilinear","content":" NNlib.∇upsample_bilinear  —  Function ∇upsample_bilinear(Δ::AbstractArray{T,4}; size::NTuple{2,Integer}, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Lateral (W,H) size of the image upsampled in the first place Outputs dx : Downsampled version of  Δ source"},{"id":591,"pagetitle":"Reference","title":"NNlib.upsample_trilinear","ref":"/nnlib/stable/reference/#NNlib.upsample_trilinear","content":" NNlib.upsample_trilinear  —  Function upsample_trilinear(x::AbstractArray{T,5}, scale::NTuple{3,Real}; align_corners::Bool = true)\nupsample_trilinear(x::AbstractArray{T,5}; size::NTuple{3,Integer}, align_corners::Bool = true) Upsamples the first 3 dimensions of the array  x  by the upsample factors stored in  scale , using trilinear interpolation. As an alternative to using  scale , the resulting image  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale[1]*S1, scale[2]*S2, scale[3]*S3, S4, S5) , where  S1, S2, S3, S4, S5 = size(x) . Examples upsample_trilinear(x, (2, 3, 4))\nupsample_trilinear(x; size=(4, 9, 11))  # specify ouput size instead\nupsample_trilinear(x, (2.5, 3.5, pi))  # non-integer scaling factors are allowed source"},{"id":592,"pagetitle":"Reference","title":"NNlib.∇upsample_trilinear","ref":"/nnlib/stable/reference/#NNlib.∇upsample_trilinear","content":" NNlib.∇upsample_trilinear  —  Function ∇upsample_trilinear(Δ::AbstractArray{T,5}; size::NTuple{3,Integer}, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Lateral size & depth (W,H,D) of the image upsampled in the first place Outputs dx : Downsampled version of  Δ source"},{"id":593,"pagetitle":"Reference","title":"NNlib.pixel_shuffle","ref":"/nnlib/stable/reference/#NNlib.pixel_shuffle","content":" NNlib.pixel_shuffle  —  Function pixel_shuffle(x, r::Integer) Pixel shuffling operation, upscaling by a factor  r . For 4-arrays representing  N  images, the operation converts input  size(x) == (W, H, r^2*C, N)  to output of size  (r*W, r*H, C, N) . For  D -dimensional data, it expects  ndims(x) == D+2  with channel and batch dimensions, and divides the number of channels by  r^D . Used in super-resolution networks to upsample towards high resolution features. Reference: Shi et. al., \"Real-Time Single Image and Video Super-Resolution ...\", CVPR 2016, https://arxiv.org/abs/1609.05158 Examples julia> x = [10i + j + channel/10 for i in 1:2, j in 1:3, channel in 1:4, batch in 1:1]\n2×3×4×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  12.1  13.1\n 21.1  22.1  23.1\n\n[:, :, 2, 1] =\n 11.2  12.2  13.2\n 21.2  22.2  23.2\n\n[:, :, 3, 1] =\n 11.3  12.3  13.3\n 21.3  22.3  23.3\n\n[:, :, 4, 1] =\n 11.4  12.4  13.4\n 21.4  22.4  23.4\n\njulia> pixel_shuffle(x, 2)  # 4 channels used up as 2x upscaling of image dimensions\n4×6×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  11.3  12.1  12.3  13.1  13.3\n 11.2  11.4  12.2  12.4  13.2  13.4\n 21.1  21.3  22.1  22.3  23.1  23.3\n 21.2  21.4  22.2  22.4  23.2  23.4\n\njulia> y = [i + channel/10 for i in 1:3, channel in 1:6, batch in 1:1]\n3×6×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.2  1.3  1.4  1.5  1.6\n 2.1  2.2  2.3  2.4  2.5  2.6\n 3.1  3.2  3.3  3.4  3.5  3.6\n\njulia> pixel_shuffle(y, 2)  # 1D image, with 6 channels reduced to 3\n6×3×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.3  1.5\n 1.2  1.4  1.6\n 2.1  2.3  2.5\n 2.2  2.4  2.6\n 3.1  3.3  3.5\n 3.2  3.4  3.6 source"},{"id":594,"pagetitle":"Reference","title":"Batched Operations","ref":"/nnlib/stable/reference/#Batched-Operations","content":" Batched Operations Flux 's  Bilinear  layer uses  NNlib.batched_mul  internally."},{"id":595,"pagetitle":"Reference","title":"NNlib.batched_mul","ref":"/nnlib/stable/reference/#NNlib.batched_mul","content":" NNlib.batched_mul  —  Function batched_mul(A, B) -> C\nA ⊠ B  # \\boxtimes Batched matrix multiplication. Result has  C[:,:,k...] == A[:,:,k...] * B[:,:,k...]  where  k...  represent  any indices in the last dimensions. If  ndims(A) == ndims(B) == 3  and  size(B,3) == 1  then instead  C[:,:,k] == A[:,:,k] * B[:,:,1] , and similarly for  A . To transpose each matrix, apply  batched_transpose  to the array, or  batched_adjoint  for conjugate-transpose: julia> A, B = randn(2,5,17), randn(5,9,17);\n\njulia> A ⊠ B |> size\n(2, 9, 17)\n\njulia> batched_adjoint(A) |> size\n(5, 2, 17)\n\njulia> batched_mul(A, batched_adjoint(randn(9,5,17))) |> size\n(2, 9, 17)\n\njulia> A ⊠ randn(5,9,1) |> size\n(2, 9, 17)\n\njulia> batched_transpose(A) == PermutedDimsArray(A, (2,1,3))\ntrue The equivalent  PermutedDimsArray  may be used in place of  batched_transpose . Other permutations are also handled by BLAS, provided that the batch index  k  is not the first dimension of the underlying array. Thus  PermutedDimsArray(::Array, (1,3,2))  and  PermutedDimsArray(::Array, (3,1,2))  are fine. However,  A = PermutedDimsArray(::Array, (3,2,1))  is not acceptable to BLAS, since the batch dimension is the contiguous one:  stride(A,3) == 1 . This will be copied, as doing so is faster than  batched_mul_generic! . Both this  copy  and  batched_mul_generic!  produce  @debug  messages, and setting for instance  ENV[\"JULIA_DEBUG\"] = NNlib  will display them. source batched_mul(A::Array{T,3}, B::Matrix)\nbatched_mul(A::Matrix, B::Array{T,3})\nA ⊠ B This is always matrix-matrix multiplication, but either  A  or  B  may lack a batch index. When  B  is a matrix, result has  C[:,:,k] == A[:,:,k] * B[:,:]  for all  k . When  A  is a matrix, then  C[:,:,k] == A[:,:] * B[:,:,k] . This can also be done by reshaping and calling  * , for instance  A ⊡ B  using TensorCore.jl, but is implemented here using  batched_gemm  instead of  gemm . julia> randn(16,8,32) ⊠ randn(8,4) |> size\n(16, 4, 32)\n\njulia> randn(16,8,32) ⊠ randn(8,4,1) |> size  # equivalent\n(16, 4, 32)\n\njulia> randn(16,8) ⊠ randn(8,4,32) |> size\n(16, 4, 32) See also  batched_vec  to regard  B  as a batch of vectors,  A[:,:,k] * B[:,k] . source"},{"id":596,"pagetitle":"Reference","title":"NNlib.batched_mul!","ref":"/nnlib/stable/reference/#NNlib.batched_mul!","content":" NNlib.batched_mul!  —  Function batched_mul!(C, A, B) -> C\nbatched_mul!(C, A, B, α=1, β=0) In-place batched matrix multiplication, equivalent to  mul!(C[:,:,k], A[:,:,k], B[:,:,k], α, β)  for all  k . If  size(B,3) == 1  then every batch uses  B[:,:,1]  instead. This will call  batched_gemm!  whenever possible. For real arrays this means that, for  X ∈ [A,B,C] , either  strides(X,1)==1  or  strides(X,2)==1 , the latter may be caused by  batched_transpose  or by for instance  PermutedDimsArray(::Array, (3,1,2)) . Unlike  batched_mul  this will never make a copy. For complex arrays, the wrapper made by  batched_adjoint  must be outermost to be seen. In this case the strided accepted by BLAS are more restricted, if  stride(C,1)==1  then only  stride(AorB::BatchedAdjoint,2) == 1  is accepted. source"},{"id":597,"pagetitle":"Reference","title":"NNlib.batched_adjoint","ref":"/nnlib/stable/reference/#NNlib.batched_adjoint","content":" NNlib.batched_adjoint  —  Function batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A) Equivalent to applying  transpose  or  adjoint  to each matrix  A[:,:,k] . These exist to control how  batched_mul  behaves, as it operates on such matrix slices of an array with  ndims(A)==3 . PermutedDimsArray(A, (2,1,3))  is equivalent to  batched_transpose(A) , and is also understood by  batched_mul  (and more widely supported elsewhere). BatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S} Lazy wrappers analogous to  Transpose  and  Adjoint , returned by  batched_transpose  etc. source"},{"id":598,"pagetitle":"Reference","title":"NNlib.batched_transpose","ref":"/nnlib/stable/reference/#NNlib.batched_transpose","content":" NNlib.batched_transpose  —  Function batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A) Equivalent to applying  transpose  or  adjoint  to each matrix  A[:,:,k] . These exist to control how  batched_mul  behaves, as it operates on such matrix slices of an array with  ndims(A)==3 . PermutedDimsArray(A, (2,1,3))  is equivalent to  batched_transpose(A) , and is also understood by  batched_mul  (and more widely supported elsewhere). BatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S} Lazy wrappers analogous to  Transpose  and  Adjoint , returned by  batched_transpose  etc. source"},{"id":599,"pagetitle":"Reference","title":"NNlib.batched_vec","ref":"/nnlib/stable/reference/#NNlib.batched_vec","content":" NNlib.batched_vec  —  Function batched_vec(A::Array{T,3}, B::Matrix)\nbatched_vec(A::Array{T,3}, b::Vector) Batched matrix-vector multiplication: the result has  C[:,:,k] == A[:,:,k] * B[:,k]  for all  k , or else  C[:,:,k] == A[:,:,k] * b  for  b::Vector . With the same argument types,  batched_mul(A, B)  would regard  B  as a fixed matrix, not a batch of vectors. Both reshape and then call  batched_mul(::Array{T,3}, ::Array{T,3}) . julia> A, B, b = randn(16,8,32), randn(8,32), randn(8);\n\njulia> batched_vec(A,B) |> size\n(16, 32)\n\njulia> batched_vec(A,b) |> size\n(16, 32) source"},{"id":600,"pagetitle":"Reference","title":"Gather and Scatter","ref":"/nnlib/stable/reference/#Gather-and-Scatter","content":" Gather and Scatter Flux 's  Embedding  layer uses  NNlib.gather  as its backend."},{"id":601,"pagetitle":"Reference","title":"NNlib.gather","ref":"/nnlib/stable/reference/#NNlib.gather","content":" NNlib.gather  —  Function NNlib.gather(src, idx) -> dst Reverse operation of  scatter . Gathers data from source  src  and writes it in a destination  dst  according to the index array  idx . For each  k  in  CartesianIndices(idx) , assign values to  dst  according to dst[:, ... , k] .= src[:, ... , idx[k]...] Notice that if  idx  is a vector containing integers and  src  is a matrix, previous expression simplifies to dst[:, k] .= src[:, idx[k]] and  k  will run over  1:length(idx) . The elements of  idx  can be integers or integer tuples and may be repeated. A single  src  column can end up being copied into zero, one, or multiple  dst  columns. See  gather!  for an in-place version. Examples julia> NNlib.gather([1,20,300,4000], [2,4,2])\n3-element Vector{Int64}:\n   20\n 4000\n   20\n\njulia> NNlib.gather([1 2 3; 4 5 6], [1,3,1,3,1])\n2×5 Matrix{Int64}:\n 1  3  1  3  1\n 4  6  4  6  4 source gather(src, IJK...) Convert the tuple of integer vectors  IJK  to a tuple of  CartesianIndex  and call  gather  on it:  gather(src, CartesianIndex.(IJK...)) . Examples julia> src = reshape([1:15;], 3, 5)\n3×5 Matrix{Int64}:\n 1  4  7  10  13\n 2  5  8  11  14\n 3  6  9  12  15\n\njulia> NNlib.gather(src, [1, 2], [2, 4])\n2-element Vector{Int64}:\n  4\n 11 source"},{"id":602,"pagetitle":"Reference","title":"NNlib.gather!","ref":"/nnlib/stable/reference/#NNlib.gather!","content":" NNlib.gather!  —  Function NNlib.gather!(dst, src, idx) Reverse operation of  scatter! . Gathers data from source  src  and writes it in destination  dst  according to the index array  idx . For each  k  in  CartesianIndices(idx) , assign values to  dst  according to dst[:, ... , k] .= src[:, ... , idx[k]...] Notice that if  idx  is a vector containing integers, and both  dst  and  src  are matrices, previous expression simplifies to dst[:, k] .= src[:, idx[k]] and  k  will run over  1:length(idx) . The elements of  idx  can be integers or integer tuples and may be repeated. A single  src  column can end up being copied into zero, one, or multiple  dst  columns. See  gather  for an allocating version. source"},{"id":603,"pagetitle":"Reference","title":"NNlib.scatter","ref":"/nnlib/stable/reference/#NNlib.scatter","content":" NNlib.scatter  —  Function NNlib.scatter(op, src, idx; [init, dstsize]) Scatter operation allocating a destination array  dst  and calling  scatter!(op, dst, src, idx)  on it. If keyword  init  is provided, it is used to initialize the content of  dst . Otherwise, the init values is inferred from the reduction operator  op  for some common operators (e.g.  init = 0  for  op = + ). If  dstsize  is provided, it will be used to define the size of destination array, otherwise it will be inferred by  src  and  idx . See  scatter!  for full details on how  idx  works. Examples julia> NNlib.scatter(+, [10,100,1000], [3,1,2])\n3-element Vector{Int64}:\n  100\n 1000\n   10\n\njulia> NNlib.scatter(+, [1 2 3 4; 5 6 7 8], [2,1,1,5])\n2×5 Matrix{Int64}:\n  5  1  0  0  4\n 13  5  0  0  8\n\njulia> NNlib.scatter(*, [10,200,3000], [1,4,2]; init = 10, dstsize = 6)\n6-element Vector{Int64}:\n   100\n 30000\n    10\n  2000\n    10\n    10 source"},{"id":604,"pagetitle":"Reference","title":"NNlib.scatter!","ref":"/nnlib/stable/reference/#NNlib.scatter!","content":" NNlib.scatter!  —  Function NNlib.scatter!(op, dst, src, idx) Scatter operation, which writes data in  src  into  dst  at locations  idx . A binary reduction operator  op  is applied during the scatter. For each index  k  in  idx , accumulates values in  dst  according to dst[:, ..., idx[k]...] = (op).(dst[:, ..., idx[k]...], src[:, ..., k...]) See also  scatter ,  gather . Arguments op : Operations to be applied on  dst  and  src , e.g.  + ,  - ,  * ,  / ,  max ,  min  and  mean . dst : The destination for  src  to aggregate to. This argument will be mutated. src : The source data for aggregating. idx : The mapping for aggregation from source (index) to destination (value).        The  idx  array can contain either integers or tuples. Examples julia> NNlib.scatter!(+, ones(3), [10,100], [1,3])\n3-element Vector{Float64}:\n  11.0\n   1.0\n 101.0\n\njulia> NNlib.scatter!(*, fill(0.5, 2, 4), [1 10; 100 1000], [3,2])\n2×4 Matrix{Float64}:\n 0.5    5.0   0.5  0.5\n 0.5  500.0  50.0  0.5 source"},{"id":605,"pagetitle":"Reference","title":"Sampling","ref":"/nnlib/stable/reference/#Sampling","content":" Sampling"},{"id":606,"pagetitle":"Reference","title":"NNlib.grid_sample","ref":"/nnlib/stable/reference/#NNlib.grid_sample","content":" NNlib.grid_sample  —  Function grid_sample(input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros) Given  input , compute output by sampling  input  values at pixel locations from  grid . Uses bilinear interpolation to calculate output values. This implementation assumes the extrema ( -1  and  1 ) are considered as referring to the center points of the input’s corner pixels (i.e. align corners is  true ). Arguments input : Input array in  (W_in, H_in, C, N)  shape. grid : Input grid in  (2, W_out, H_out, N)  shape.   Where for each  (W_out, H_out, N)  grid contains  (x, y)    coordinates that specify sampling locations normalized by the  input  shape. Therefore,  x  and  y  should have values in  [-1, 1]  range.   For example,  (x = -1, y = -1)  is the left-top pixel of  input ,   and  (x = 1, y = 1)  is the right-bottom pixel of  input . Out-of-bound values are handled according to the  padding_mode . padding_mode : Out-of-bound padding.    :zeros  to use  0  for out-of-bound grid locations.    :border  to use border values for out-of-bound grid locations.   Default is  :zeros . Returns (W_out, H_out, C, N)  sampled grid from  input . Examples In the example below, grid contains two out-of-bound sampling locations, which are handled differently, depending on the  padding_mode . julia> x = reshape(collect(1.0:4.0), (2, 2, 1, 1))\n2×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 2.0  4.0\n\njulia> grid = Array{Float64}(undef, 2, 3, 2, 1);\n\njulia> grid[:, 1, 1, 1] .= (-3, -1);\n\njulia> grid[:, 2, 1, 1] .= (0, -1);\n\njulia> grid[:, 3, 1, 1] .= (1, -1);\n\njulia> grid[:, 1, 2, 1] .= (-1, 1);\n\njulia> grid[:, 2, 2, 1] .= (0, 1);\n\njulia> grid[:, 3, 2, 1] .= (3, 1);\n\njulia> grid_sample(x, grid; padding_mode=:zeros)\n3×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 0.0  3.0\n 1.5  3.5\n 2.0  0.0\n\njulia> grid_sample(x, grid; padding_mode=:border)\n3×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 1.5  3.5\n 2.0  4.0 source"},{"id":607,"pagetitle":"Reference","title":"NNlib.∇grid_sample","ref":"/nnlib/stable/reference/#NNlib.∇grid_sample","content":" NNlib.∇grid_sample  —  Function ∇grid_sample(Δ::AbstractArray{T, 4}, input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros) where T Arguments Δ : Input gradient in  (W_out, H_out, C, N)  shape   (same as output of the primal computation). input : Input from primal computation in  (W_in, H_in, C, N)  shape. grid : Grid from primal computation in  (2, W_out, H_out, N)  shape. padding_mode : Out-of-bound padding.    :zeros  to use  0  for out-of-bound grid locations.    :border  to use border values for out-of-bound grid locations.   Should be the same as in primal computation.   Default is  :zeros . Returns dinput  (same shape as  input ) and  dgrid  (same shape as  grid ) gradients. source"},{"id":608,"pagetitle":"Reference","title":"Losses","ref":"/nnlib/stable/reference/#Losses","content":" Losses"},{"id":609,"pagetitle":"Reference","title":"NNlib.ctc_loss","ref":"/nnlib/stable/reference/#NNlib.ctc_loss","content":" NNlib.ctc_loss  —  Function ctc_loss(ŷ, y) Computes the connectionist temporal classification loss between  ŷ  and  y .  ŷ  must be a classes-by-time matrices, i.e., each row represents a class and each column represents a time step. Additionally, the  logsoftmax  function will be applied to  ŷ , so  ŷ  must be the raw activation values from the neural network and not, for example, the activations after being passed through a  softmax  activation function.  y  must be a 1D array of the labels associated with  ŷ . The blank label is assumed to be the last label category in  ŷ , so it is equivalent to  size(ŷ, 1) . Used for sequence-to-sequence classification problems such as speech recognition and handwriting recognition where the exact time-alignment of the output (e.g., letters) is not needed to solve the problem. See  Graves et al. (2006)  or  Graves (2012)  for mathematical details. source"},{"id":610,"pagetitle":"Reference","title":"Miscellaneous","ref":"/nnlib/stable/reference/#Miscellaneous","content":" Miscellaneous"},{"id":611,"pagetitle":"Reference","title":"NNlib.logsumexp","ref":"/nnlib/stable/reference/#NNlib.logsumexp","content":" NNlib.logsumexp  —  Function logsumexp(x; dims = :) Computes  log.(sum(exp.(x); dims))  in a numerically stable way. Without  dims  keyword this returns a scalar. See also  logsoftmax . source"},{"id":612,"pagetitle":"Reference","title":"NNlib.glu","ref":"/nnlib/stable/reference/#NNlib.glu","content":" NNlib.glu  —  Function glu(x, dim = 1) The gated linear unit from the  \"Language Modeling with Gated Convolutional Networks\"  paper. Calculates  a .* sigmoid(b) , where  x  is split in half along given dimension  dim  to form  a  and  b . source"},{"id":613,"pagetitle":"Reference","title":"NNlib.within_gradient","ref":"/nnlib/stable/reference/#NNlib.within_gradient","content":" NNlib.within_gradient  —  Function within_gradient(x) --> Bool Returns  false  except when used inside a  gradient  call, when it returns  true . Useful for Flux regularisation layers which behave differently during training and inference. This should work with any ChainRules-based differentiation package, in which case  x  is ignored. But Tracker.jl overloads  with_gradient(x::TrackedArray) , thus for widest use you should pass it an array whose gradient is of interest. There is also an overload for ForwardDiff.jl's  Dual  types (and arrays of them). Examples julia> using ForwardDiff, Zygote, NNlib\n\njulia> f_good(x) = if NNlib.within_gradient(x)\n                     @show 10x\n                   else\n                     x\n                   end;\n\njulia> Zygote.withgradient(f_good, 1.0)\n10x = 10.0\n(val = 10.0, grad = (10.0,))\n\njulia> ForwardDiff.derivative(f_good, 1.0)\n10x = Dual{ForwardDiff.Tag{typeof(f_good), Float64}}(10.0,10.0)\n10.0\n\njulia> f_bad(x, y) = if any(NNlib.within_gradient, (x, y))\n                       @show x * y\n                     else\n                       x / y\n                     end;\n\njulia> Zygote.withgradient(f_bad, 2.0, 3.0)\n(val = 0.6666666666666666, grad = (0.3333333333333333, -0.2222222222222222))\n\njulia> ForwardDiff.derivative(x -> f_bad(x, 3.0), 2.0)\nx * y = Dual{ForwardDiff.Tag{var\"#9#10\", Float64}}(6.0,3.0)\n3.0 What goes wrong in  f_bad  is that Zygote knows  any  to be non-differentiable, and thus completely ignores its contents. This is not a perfect mechanism, and the only style recommended is precisely that of  f_good  above. source"},{"id":614,"pagetitle":"Reference","title":"NNlib.bias_act!","ref":"/nnlib/stable/reference/#NNlib.bias_act!","content":" NNlib.bias_act!  —  Function bias_act!(σ, x, b) This is equivalent to  x .= σ.(x .+ b) , also replacing  sigmoid  &  tanh  with  sigmoid_fast  &  tanh_fast . It will only overwrite  x  when  x isa StridedArray{<:AbstractFloat} . When used within a gradient, it will overwrite only when  σ  has a method of  derivatives_given_output  which does not need the input at all. Such methods are defined by e.g.  @scalar_rule relu(x) Ω > 0  where the derivative contains only  Ω  (the output) not  x . Warning This is not safe to use if  x  is still needed for the gradient of some other function. Incorrect use will give silently wrong answers. It is intended mainly for Flux layers, in which the previous operation is known to be safe, e.g.  bias_act!(σ, weight * input, bias)  for a  Dense  layer. source"},{"id":619,"pagetitle":"Home","title":"Functors.jl","ref":"/functors/stable/#Functors.jl","content":" Functors.jl Functors.jl provides a set of tools to represent  functors . Functors are a powerful means to apply functions to generic objects without changing their structure. The most straightforward use is to traverse a complicated nested structure as a tree, and apply a function  f  to every field it encounters along the way. For large models it can be cumbersome or inefficient to work with parameters as one big, flat vector, and structs help manage complexity; but it may be desirable to easily operate over all parameters at once, e.g. for changing precision or applying an optimiser update step."},{"id":620,"pagetitle":"Home","title":"Basic Usage and Implementation","ref":"/functors/stable/#Basic-Usage-and-Implementation","content":" Basic Usage and Implementation When one marks a structure as  @functor  it means that Functors.jl is allowed to look into the fields of the instances of the struct and modify them. This is achieved through  Functors.fmap . The workhorse of fmap is actually a lower level function, functor: julia> using Functors\n\njulia> struct Foo\n         x\n         y\n       end\n\njulia> @functor Foo\n\njulia> foo = Foo(1, [1, 2, 3]) # notice all the elements are integers\n\njulia> xs, re = Functors.functor(foo)\n((x = 1, y = [1, 2, 3]), var\"#21#22\"())\n\njulia> re(map(float, xs)) # element types have been switched out for floating point numbers\nFoo(1.0, [1.0, 2.0, 3.0]) functor  returns the parts of the object that can be inspected, as well as a reconstruction function (shown as  re ) that takes those values and restructures them back into an object of the original type. To include only certain fields of a struct, one can pass a tuple of field names to  @functor : julia> struct Baz\n         x\n         y\n       end\n\njulia> @functor Baz (x,)\n\njulia> model = Baz(1, 2)\nBaz(1, 2)\n\njulia> fmap(float, model)\nBaz(1.0, 2) Any field not in the list will be passed through as-is during reconstruction. This is done by invoking the default constructor, so structs that define custom inner constructors are expected to provide one that acts like the default."},{"id":621,"pagetitle":"Home","title":"Appropriate Use","ref":"/functors/stable/#Appropriate-Use","content":" Appropriate Use Not everything should be a functor! Due to its generic nature it is very attractive to mark several structures as  @functor  when it may not be quite safe to do so. Typically, since any function  f  is applied to the leaves of the tree, but it is possible for some functions to require dispatching on the specific type of the fields causing some methods to be missed entirely. Examples of this include element types of arrays which typically have their own mathematical operations defined. Adding a  @functor  to such a type would end up missing methods such as  +(::MyElementType, ::MyElementType) . Think  RGB  from Colors.jl."},{"id":624,"pagetitle":"API","title":"Functors.fmap","ref":"/functors/stable/api/#Functors.fmap","content":" Functors.fmap  —  Function fmap(f, x, ys...; exclude = Functors.isleaf, walk = Functors.DefaultWalk()[, prune]) A structure and type preserving  map . By default it transforms every leaf node (identified by  exclude , default  isleaf ) by applying  f , and otherwise traverses  x  recursively using  functor . Optionally, it may also be associated with objects  ys  with the same tree structure. In that case,  f  is applied to the corresponding leaf nodes in  x  and  ys . Examples julia> fmap(string, (x=1, y=(2, 3)))\n(x = \"1\", y = (\"2\", \"3\"))\n\njulia> nt = (a = [1,2], b = [23, (45,), (x=6//7, y=())], c = [8,9]);\n\njulia> fmap(println, nt)\n[1, 2]\n23\n45\n6//7\n()\n[8, 9]\n(a = nothing, b = Any[nothing, (nothing,), (x = nothing, y = nothing)], c = nothing)\n\njulia> fmap(println, nt; exclude = x -> x isa Array)\n[1, 2]\nAny[23, (45,), (x = 6//7, y = ())]\n[8, 9]\n(a = nothing, b = nothing, c = nothing)\n\njulia> twice = [1, 2];  # println only acts once on this\n\njulia> fmap(println, (i = twice, ii = 34, iii = [5, 6], iv = (twice, 34), v = 34.0))\n[1, 2]\n34\n[5, 6]\n34\n34.0\n(i = nothing, ii = nothing, iii = nothing, iv = (nothing, nothing), v = nothing)\n\njulia> d1 = Dict(\"x\" => [1,2], \"y\" => 3);\n\njulia> d2 = Dict(\"x\" => [4,5], \"y\" => 6, \"z\" => \"an_extra_value\");\n\njulia> fmap(+, d1, d2) == Dict(\"x\" => [5, 7], \"y\" => 9) # Note that \"z\" is ignored\ntrue Mutable objects which appear more than once are only handled once (by caching  f(x)  in an  IdDict ). Thus the relationship  x.i === x.iv[1]  will be preserved. An immutable object which appears twice is not stored in the cache, thus  f(34)  will be called twice, and the results will agree only if  f  is pure. By default,  Tuple s,  NamedTuple s, and some other container-like types in Base have children to recurse into. Arrays of numbers do not. To enable recursion into new types, you must provide a method of  functor , which can be done using the macro  @functor : julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> struct Bar; x; end\n\njulia> @functor Bar\n\njulia> m = Foo(Bar([1,2,3]), (4, 5, Bar(Foo(6, 7))));\n\njulia> fmap(x -> 10x, m)\nFoo(Bar([10, 20, 30]), (40, 50, Bar(Foo(60, 70))))\n\njulia> fmap(string, m)\nFoo(Bar(\"[1, 2, 3]\"), (\"4\", \"5\", Bar(Foo(\"6\", \"7\"))))\n\njulia> fmap(string, m, exclude = v -> v isa Bar)\nFoo(\"Bar([1, 2, 3])\", (4, 5, \"Bar(Foo(6, 7))\")) To recurse into custom types without reconstructing them afterwards, use  fmapstructure . For advanced customization of the traversal behaviour, pass a custom  walk  function that subtypes  Functors.AbstractWalk . The call  fmap(f, x, ys...; walk = mywalk)  will wrap  mywalk  in  ExcludeWalk  then  CachedWalk . Here,  ExcludeWalk  is responsible for applying  f  at excluded nodes. For a low-level interface for executing a user-constructed walk, see  execute . julia> struct MyWalk <: Functors.AbstractWalk end\n\njulia> (::MyWalk)(recurse, x) = x isa Bar ? \"hello\" :\n                                            Functors.DefaultWalk()(recurse, x)\n\njulia> fmap(x -> 10x, m; walk = MyWalk())\nFoo(\"hello\", (40, 50, \"hello\")) The behaviour when the same node appears twice can be altered by giving a value to the  prune  keyword, which is then used in place of all but the first: julia> twice = [1, 2];\n\njulia> fmap(float, (x = twice, y = [1,2], z = twice); prune = missing)\n(x = [1.0, 2.0], y = [1.0, 2.0], z = missing) source"},{"id":625,"pagetitle":"API","title":"Functors.@functor","ref":"/functors/stable/api/#Functors.@functor","content":" Functors.@functor  —  Macro @functor T\n@functor T (x,) Adds methods to  functor  allowing recursion into objects of type  T , and reconstruction. Assumes that  T  has a constructor accepting all of its fields, which is true unless you have provided an inner constructor which does not. By default all fields of  T  are considered  children ;  this can be restricted be restructed by providing a tuple of field names. Examples julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> Functors.children(Foo(1,2))\n(x = 1, y = 2)\n\njulia> _, re = Functors.functor(Foo(1,2));\n\njulia> re((10, 20))\nFoo(10, 20)\n\njulia> struct TwoThirds a; b; c; end\n\njulia> @functor TwoThirds (a, c)\n\njulia> ch2, re3 = Functors.functor(TwoThirds(10,20,30));\n\njulia> ch2\n(a = 10, c = 30)\n\njulia> re3((\"ten\", \"thirty\"))\nTwoThirds(\"ten\", 20, \"thirty\")\n\njulia> fmap(x -> 10x, TwoThirds(Foo(1,2), Foo(3,4), 56))\nTwoThirds(Foo(10, 20), Foo(3, 4), 560) source"},{"id":626,"pagetitle":"API","title":"Functors.@leaf","ref":"/functors/stable/api/#Functors.@leaf","content":" Functors.@leaf  —  Macro @leaf T Define  functor  for the type  T  so that   isleaf(x::T) == true . source"},{"id":627,"pagetitle":"API","title":"Functors.functor","ref":"/functors/stable/api/#Functors.functor","content":" Functors.functor  —  Function Functors.functor(x) = functor(typeof(x), x) Returns a tuple containing, first, a  NamedTuple  of the children of  x  (typically its fields), and second, a reconstruction funciton. This controls the behaviour of  fmap . Methods should be added to  functor(::Type{T}, x)  for custom types, usually using the macro  @functor . source"},{"id":628,"pagetitle":"API","title":"Functors.children","ref":"/functors/stable/api/#Functors.children","content":" Functors.children  —  Function Functors.children(x) Return the children of  x  as defined by  functor . Equivalent to  functor(x)[1] . source"},{"id":629,"pagetitle":"API","title":"Functors.isleaf","ref":"/functors/stable/api/#Functors.isleaf","content":" Functors.isleaf  —  Function Functors.isleaf(x) Return true if  x  has no  children  according to  functor . Examples julia> Functors.isleaf(1)\ntrue\n\njulia> Functors.isleaf([2, 3, 4])\ntrue\n\njulia> Functors.isleaf([\"five\", [6, 7]])\nfalse\n\njulia> Functors.isleaf([])\nfalse\n\njulia> Functors.isleaf((8, 9))\nfalse\n\njulia> Functors.isleaf(())\ntrue source"},{"id":630,"pagetitle":"API","title":"Functors.AbstractWalk","ref":"/functors/stable/api/#Functors.AbstractWalk","content":" Functors.AbstractWalk  —  Type AbstractWalk Any walk for use with  fmap  should inherit from this type. A walk subtyping  AbstractWalk  must satisfy the walk function interface: struct MyWalk <: AbstractWalk end\n\nfunction (::MyWalk)(recurse, x, ys...)\n  # implement this\nend The walk function is called on a node  x  in a Functors tree. It may also be passed associated nodes  ys...  in other Functors trees. The walk function recurses further into  (x, ys...)  by calling  recurse  on the child nodes. The choice of which nodes to recurse and in what order is custom to the walk. source"},{"id":631,"pagetitle":"API","title":"Functors.execute","ref":"/functors/stable/api/#Functors.execute","content":" Functors.execute  —  Function execute(walk, x, ys...) Execute a  walk  that recursively calls itself, starting at a node  x  in a Functors tree, as well as optional associated nodes  ys...  in other Functors trees. Any custom  walk  function that subtypes  Functors.AbstractWalk  is permitted. source"},{"id":632,"pagetitle":"API","title":"Functors.DefaultWalk","ref":"/functors/stable/api/#Functors.DefaultWalk","content":" Functors.DefaultWalk  —  Type DefaultWalk() The default walk behavior for Functors.jl. Walks all the  Functors.children  of trees  (x, ys...)  based on the structure of  x . The resulting mapped child nodes are restructured into the type of  x . See  fmap  for more information. source"},{"id":633,"pagetitle":"API","title":"Functors.StructuralWalk","ref":"/functors/stable/api/#Functors.StructuralWalk","content":" Functors.StructuralWalk  —  Type StructuralWalk() A structural variant of  Functors.DefaultWalk . The recursion behavior is identical, but the mapped children are not restructured. See  fmapstructure  for more information. source"},{"id":634,"pagetitle":"API","title":"Functors.ExcludeWalk","ref":"/functors/stable/api/#Functors.ExcludeWalk","content":" Functors.ExcludeWalk  —  Type ExcludeWalk(walk, fn, exclude) A walk that recurses nodes  (x, ys...)  according to  walk , except when  exclude(x)  is true. Then,  fn(x, ys...)  is applied instead of recursing further. Typically wraps an existing  walk  for use with  fmap . source"},{"id":635,"pagetitle":"API","title":"Functors.CachedWalk","ref":"/functors/stable/api/#Functors.CachedWalk","content":" Functors.CachedWalk  —  Type CachedWalk(walk[; prune]) A walk that recurses nodes  (x, ys...)  according to  walk  and storing the output of the recursion in a cache indexed by  x  (based on object ID). Whenever the cache already contains  x , either: prune  is specified, then it is returned, or prune  is unspecified, and the previously cached recursion of  (x, ys...)  returned. Typically wraps an existing  walk  for use with  fmap . source"},{"id":636,"pagetitle":"API","title":"Functors.CollectWalk","ref":"/functors/stable/api/#Functors.CollectWalk","content":" Functors.CollectWalk  —  Type CollectWalk() A walk that recurses into a node  x  via  Functors.children , storing the recursion history in a cache. The resulting ordered recursion history is returned. See  fcollect  for more information. source"},{"id":637,"pagetitle":"API","title":"Functors.AnonymousWalk","ref":"/functors/stable/api/#Functors.AnonymousWalk","content":" Functors.AnonymousWalk  —  Type AnonymousWalk(walk_fn) Wrap a  walk_fn  so that  AnonymousWalk(walk_fn) isa AbstractWalk . This type only exists for backwards compatability and should not be directly used. Attempting to wrap an existing  AbstractWalk  is a no-op (i.e. it is not wrapped). source"},{"id":638,"pagetitle":"API","title":"Functors.IterateWalk","ref":"/functors/stable/api/#Functors.IterateWalk","content":" Functors.IterateWalk  —  Type IterateWalk() A walk that walks all the  Functors.children  of trees  (x, ys...)   and concatenates the iterators of the children via  Iterators.flatten . The resulting iterator is returned. When used with  fmap , the provided function  f  should return an iterator. For example, to iterate through the square of every scalar value: julia> x = ([1, 2, 3], 4, (5, 6, [7, 8]));\n\njulia> make_iterator(x) = x isa AbstractVector ? x.^2 : (x^2,);\n\njulia> iter = fmap(make_iterator, x; walk=Functors.IterateWalk(), cache=nothing);\n\njulia> collect(iter)\n8-element Vector{Int64}:\n  1\n  4\n  9\n 16\n 25\n 36\n 49\n 64 We can also simultaneously iterate through multiple functors: julia> y = ([8, 7, 6], 5, (4, 3, [2, 1]));\n\njulia> make_zipped_iterator(x, y) = zip(make_iterator(x), make_iterator(y));\n\njulia> zipped_iter = fmap(make_zipped_iterator, x, y; walk=Functors.IterateWalk(), cache=nothing);\n\njulia> collect(zipped_iter)\n8-element Vector{Tuple{Int64, Int64}}:\n (1, 64)\n (4, 49)\n (9, 36)\n (16, 25)\n (25, 16)\n (36, 9)\n (49, 4)\n (64, 1) source"},{"id":639,"pagetitle":"API","title":"Functors.fmapstructure","ref":"/functors/stable/api/#Functors.fmapstructure","content":" Functors.fmapstructure  —  Function fmapstructure(f, x; exclude = isleaf) Like  fmap , but doesn't preserve the type of custom structs. Instead, it returns a  NamedTuple  (or a  Tuple , or an array), or a nested set of these. Useful for when the output must not contain custom structs. Examples julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> m = Foo([1,2,3], [4, (5, 6), Foo(7, 8)]);\n\njulia> fmapstructure(x -> 2x, m)\n(x = [2, 4, 6], y = Any[8, (10, 12), (x = 14, y = 16)])\n\njulia> fmapstructure(println, m)\n[1, 2, 3]\n4\n5\n6\n7\n8\n(x = nothing, y = Any[nothing, (nothing, nothing), (x = nothing, y = nothing)]) source"},{"id":640,"pagetitle":"API","title":"Functors.fcollect","ref":"/functors/stable/api/#Functors.fcollect","content":" Functors.fcollect  —  Function fcollect(x; exclude = v -> false) Traverse  x  by recursing each child of  x  as defined by  functor  and collecting the results into a flat array, ordered by a breadth-first traversal of  x , respecting the iteration order of  children  calls. Doesn't recurse inside branches rooted at nodes  v  for which  exclude(v) == true . In such cases, the root  v  is also excluded from the result. By default,  exclude  always yields  false . See also  children . Examples julia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> struct Bar; x; end\n\njulia> @functor Bar\n\njulia> struct TypeWithNoChildren; x; y; end\n\njulia> m = Foo(Bar([1,2,3]), TypeWithNoChildren(:a, :b))\nFoo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n\njulia> fcollect(m)\n4-element Vector{Any}:\n Foo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n Bar([1, 2, 3])\n [1, 2, 3]\n TypeWithNoChildren(:a, :b)\n\njulia> fcollect(m, exclude = v -> v isa Bar)\n2-element Vector{Any}:\n Foo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n TypeWithNoChildren(:a, :b)\n\njulia> fcollect(m, exclude = v -> Functors.isleaf(v))\n2-element Vector{Any}:\n Foo(Bar([1, 2, 3]), TypeWithNoChildren(:a, :b))\n Bar([1, 2, 3]) source"},{"id":645,"pagetitle":"Home","title":"Zygote","ref":"/zygote/stable/#Zygote-1","content":" Zygote Welcome! Zygote extends the Julia language to support  differentiable programming . With Zygote you can write down any Julia code you feel like – including using existing Julia packages – then get gradients and optimise your program. Deep learning, ML and probabilistic programming are all different kinds of differentiable programming that you can do with Zygote. At least, that's the idea. We're still in beta so expect some adventures."},{"id":646,"pagetitle":"Home","title":"Setup","ref":"/zygote/stable/#Setup-1","content":" Setup Zygote can be installed from the package manager in Julia's REPL: ] add Zygote"},{"id":647,"pagetitle":"Home","title":"Taking Gradients","ref":"/zygote/stable/#Taking-Gradients-1","content":" Taking Gradients Zygote is easy to understand since, at its core, it has a one-function API ( pullback ), along with a few simple conveniences. Before explaining  pullback , we'll look at the higher-level function  gradient . gradient  calculates derivatives. For example, the derivative of  $3x^2 + 2x + 1$  is  $6x + 2$ , so when  x = 5 ,  dx = 32 . julia> using Zygote\n\njulia> gradient(x -> 3x^2 + 2x + 1, 5)\n(32.0,) gradient  returns a tuple, with a gradient for each argument to the function. julia> gradient((a, b) -> a*b, 2, 3)\n(3.0, 2.0) This will work equally well if the arguments are arrays, structs, or any other Julia type, but the function should return a scalar (like a loss or objective  $l$ , if you're doing optimisation / ML). julia> W = rand(2, 3); x = rand(3);\n\njulia> gradient(W -> sum(W*x), W)[1]\n2×3 Array{Float64,2}:\n 0.0462002  0.817608  0.979036\n 0.0462002  0.817608  0.979036\n\njulia> gradient(x -> 3x^2 + 2x + 1, 1//4)\n(7//2,) Control flow is fully supported, including recursion. julia> function pow(x, n)\n         r = 1\n         for i = 1:n\n           r *= x\n         end\n         return r\n       end\npow (generic function with 1 method)\n\njulia> gradient(x -> pow(x, 3), 5)\n(75.0,)\n\njulia> pow2(x, n) = n <= 0 ? 1 : x*pow2(x, n-1)\npow2 (generic function with 1 method)\n\njulia> gradient(x -> pow2(x, 3), 5)\n(75.0,) Data structures are also supported, including mutable ones like dictionaries. Arrays are currently immutable, though  this may change  in future. julia> d = Dict()\nDict{Any, Any}()\n\njulia> gradient(5) do x\n         d[:x] = x\n         d[:x] * d[:x]\n       end\n(10.0,)\n\njulia> d[:x]\n5"},{"id":648,"pagetitle":"Home","title":"Structs and Types","ref":"/zygote/stable/#Structs-and-Types-1","content":" Structs and Types Julia makes it easy to work with custom types, and Zygote makes it easy to differentiate them. For example, given a simple  Point  type: import Base: +, -\n\nstruct Point\n  x::Float64\n  y::Float64\nend\n\na::Point + b::Point = Point(a.x + b.x, a.y + b.y)\na::Point - b::Point = Point(a.x - b.x, a.y - b.y)\ndist(p::Point) = sqrt(p.x^2 + p.y^2) julia> a = Point(1, 2)\nPoint(1.0, 2.0)\n\njulia> b = Point(3, 4)\nPoint(3.0, 4.0)\n\njulia> dist(a + b)\n7.211102550927978\n\njulia> gradient(a -> dist(a + b), a)[1]\n(x = 0.5547001962252291, y = 0.8320502943378437) Zygote's default representation of the \"point adjoint\" is a named tuple with gradients for both fields, but this can of course be customised too. This means we can do something very powerful: differentiating through Julia libraries, even if they weren't designed for this. For example,  colordiff  might be a smarter loss function on colours than simple mean-squared-error: julia> using Colors\n\njulia> colordiff(RGB(1, 0, 0), RGB(0, 1, 0))\n86.60823557376344\n\njulia> gradient(colordiff, RGB(1, 0, 0), RGB(0, 1, 0))\n((r = 0.4590887719632896, g = -9.598786801605689, b = 14.181383399012862), (r = -1.7697549557037275, g = 28.88472330558805, b = -0.044793892637761346))"},{"id":649,"pagetitle":"Home","title":"Explicit and Implicit Parameters","ref":"/zygote/stable/#Explicit-and-Implicit-Parameters-1","content":" Explicit and Implicit Parameters It's easy to work with even very large and complex models, and there are few ways to do this. Autograd-style models pass around a collection of weights. Depending on how you write your model, there are multiple ways to  explicitly  take gradients with respect to parameters. For example, the function  linear  accepts the parameters as an argument to the model. So, we directly pass in the parameters,  θ , as an argument to the function being differentiated."},{"id":650,"pagetitle":"Home","title":"Zygote.gradient","ref":"/zygote/stable/#Zygote.gradient-Tuple{Any, Vararg{Any}}","content":" Zygote.gradient  —  Method gradient(f, args...) Returns a tuple containing  ∂f/∂x  for each argument  x , the derivative (for scalar  x ) or the gradient. f(args...)  must be a real number, see  jacobian  for array output. See also  withgradient  to keep the value  f(args...) , and  pullback  for value and back-propagator. julia> gradient(*, 2.0, 3.0, 5.0)\n(15.0, 10.0, 6.0)\n\njulia> gradient(x -> sum(abs2,x), [7.0, 11.0, 13.0])\n([14.0, 22.0, 26.0],)\n\njulia> gradient([7, 11], 0, 1) do x, y, d\n         p = size(x, d)\n         sum(x.^p .+ y)\n       end\n([14.0, 22.0], 2.0, nothing) source julia> linear(θ, x) = θ[:W] * x .+ θ[:b]\nlinear (generic function with 1 method)\n\njulia> x = rand(5);\n\njulia> θ = Dict(:W => rand(2, 5), :b => rand(2))\nDict{Any,Any} with 2 entries:\n  :b => [0.0430585, 0.530201]\n  :W => [0.923268 … 0.589691]\n\n# Alternatively, use a named tuple or struct rather than a dict.\n# θ = (W = rand(2, 5), b = rand(2))\n\njulia> θ̄ = gradient(θ -> sum(linear(θ, x)), θ)[1]\nDict{Any,Any} with 2 entries:\n  :b => [1.0, 1.0]\n  :W => [0.628998 … 0.433006] We can combine the role of the dictionary and the function here by making a callable struct which contains the parameters, equivalent to a closure. Passed explicitly to  gradient , we get a named tuple with the same field names: julia> struct Linear\n         W\n         b\n       end\n\njulia> (l::Linear)(x) = l.W * x .+ l.b\n\njulia> model = Linear(rand(2, 5), rand(2))\nLinear([0.267663 … 0.334385], [0.0386873, 0.0203294])\n\njulia> x = rand(5);\n\njulia> dmodel = gradient(model -> sum(model(x)), model)[1]\n(W = [0.652543 … 0.683588], b = [1.0, 1.0]) Zygote also supports another way to take gradients, via  implicit parameters . Here the loss function takes zero arguments, but the variables of interest are indicated by a special  Params  object. The function  linear  which depends on  W  and  b  is executed when the loss function  () -> sum(linear(x))  is called, and hence this dependence is visible to Zygote:"},{"id":651,"pagetitle":"Home","title":"Zygote.gradient","ref":"/zygote/stable/#Zygote.gradient","content":" Zygote.gradient  —  Function gradient(() -> loss(), ps::Params) -> Grads Gradient with implicit parameters. Takes a zero-argument function, and returns a dictionary-like container, whose keys are arrays  x in ps . See also  withgradient  to keep the value  loss() . julia> x = [1 2 3; 4 5 6]; y = [7, 8]; z = [1, 10, 100];\n\njulia> g = gradient(Params([x, y])) do\n         sum(x .* y .* z')\n       end\nGrads(...)\n\njulia> g[x]\n2×3 Matrix{Float64}:\n 7.0  70.0  700.0\n 8.0  80.0  800.0\n\njulia> haskey(g, z)  # only x and y are parameters\nfalse source julia> W = rand(2, 5); b = rand(2);\n\njulia> linear(x) = W * x .+ b\nlinear (generic function with 2 methods)\n\njulia> grads = gradient(() -> sum(linear(x)), Params([W, b]))\nGrads(...)\n\njulia> grads[W], grads[b] # access gradients using arrays as keys\n([0.652543 … 0.683588], [1.0, 1.0]) Here  grads  is a dictionary-like object, whose keys are the same parameters we indicated in  Params . (In fact it wraps a dictionary using  objectid(W)  as keys, which does not change if the values in  W  are mutated). This implicit style is the one presently used by  Flux.jl , a closely related machine learning library. It uses structs like  Linear  above to define layers, and the function  Flux.params(model)  returns a  Params  object containing all the parameters of all layers. See  its documentation  for more details. When using Zygote for most other purposes, however, the explicit style is usually preferred."},{"id":654,"pagetitle":"Custom Adjoints","title":"Custom Adjoints","ref":"/zygote/stable/adjoints/#Custom-Adjoints-1","content":" Custom Adjoints Prefer to use ChainRulesCore to define custom adjoints Zygote supports the use of  ChainRulesCore  to define custom sensitivities. It is preferred to define the custom sensitivities using  ChainRulesCore.rrule  as they will work for many AD systems, not just Zygote. These sensitivities can be added in your own package, or for Base/StdLib functions they can be added to  ChainRules.jl . To define custom sensitivities using ChainRulesCore, define a  ChainRulesCore.rrule(f, args...; kwargs...) . Head to  ChainRules project's documentation  for more information.  If you are defining your custom adjoints using ChainRulesCore then you do not need to read this page , and can consider it as documenting a legacy feature. This page exists to describe how Zygote works, and how adjoints can be directly defined for Zygote. Defining adjoints this way does not make them accessible to other AD systems, but does let you do things that directly depend on how Zygote works. It allows for specific definitions of adjoints that are only defined for Zygote (which might work differently to more generic definitions defined for all AD). The  @adjoint  macro is an important part of Zygote's interface; customising your backwards pass is not only possible but widely used and encouraged. While there are specific utilities available for common things like gradient clipping, understanding adjoints will give you the most flexibility. We first give a bit more background on what these pullback things are."},{"id":655,"pagetitle":"Custom Adjoints","title":"Pullbacks","ref":"/zygote/stable/adjoints/#Pullbacks-1","content":" Pullbacks gradient  is really just syntactic sugar around the more fundamental function  pullback . julia> using Zygote\n\njulia> y, back = Zygote.pullback(sin, 0.5);\n\njulia> y\n0.479425538604203 pullback  gives two outputs: the result of the original function,  sin(0.5) , and a  pullback , here called  back .  back  implements the gradient computation for  sin , accepting a derivative and producing a new one. In mathematical terms, it implements a vector-Jacobian product. Where  $y = f(x)$  and the gradient  $\\frac{\\partial l}{\\partial x}$  is written  $\\bar{x}$ , the pullback  $\\mathcal{B}_y$  computes: \\[\\bar{x} = \\frac{\\partial l}{\\partial x} = \\frac{\\partial l}{\\partial y} \\frac{\\partial y}{\\partial x} = \\mathcal{B}_y(\\bar{y})\\] To make this concrete, take the function  $y = \\sin(x)$ .  $\\frac{\\partial y}{\\partial x} = \\cos(x)$ , so the pullback is  $\\bar{y} \\cos(x)$ . In other words  pullback(sin, x)  behaves the same as dsin(x) = (sin(x), ȳ -> (ȳ * cos(x),)) gradient  takes a function  $l = f(x)$  and assumes  $l̄ = \\frac{\\partial l}{\\partial l} = 1$  and feeds this in to the pullback. In the case of  sin , julia> function gradsin(x)\n         _, back = dsin(x)\n         back(1)\n       end\ngradsin (generic function with 1 method)\n\njulia> gradsin(0.5)\n(0.8775825618903728,)\n\njulia> cos(0.5)\n0.8775825618903728 More generally julia> function mygradient(f, x...)\n         _, back = Zygote.pullback(f, x...)\n         back(1)\n       end\nmygradient (generic function with 1 method)\n\njulia> mygradient(sin, 0.5)\n(0.8775825618903728,) The rest of this section contains more technical detail. It can be skipped if you only need an intuition for pullbacks; you generally won't need to worry about it as a user. If  $x$  and  $y$  are vectors,  $\\frac{\\partial y}{\\partial x}$  becomes a Jacobian. Importantly, because we are implementing reverse mode we actually left-multiply the Jacobian, i.e.  v'J , rather than the more usual  J*v . Transposing  v  to a row vector and back  (v'J)'  is equivalent to  J'v  so our gradient rules actually implement the  adjoint  of the Jacobian. This is relevant even for scalar code: the adjoint for  y = sin(x)  is  x̄ = cos(x)'*ȳ ; the conjugation is usually moot but gives the correct behaviour for complex code. \"Pullbacks\" are therefore sometimes called \"vector-Jacobian products\" (VJPs), and we refer to the reverse mode rules themselves as \"adjoints\". Zygote has many adjoints for non-mathematical operations such as for indexing and data structures. Though these can still be seen as linear functions of vectors, it's not particularly enlightening to implement them with an actual matrix multiply. In these cases it's easiest to think of the adjoint as a kind of inverse. For example, the gradient of a function that takes a tuple to a struct (e.g.  y = Complex(a, b) ) will generally take a struct to a tuple ( (ȳ.re, ȳ.im) ). The gradient of a  getindex y = x[i...]  is a  setindex! x̄[i...] = ȳ , etc."},{"id":656,"pagetitle":"Custom Adjoints","title":"Custom Adjoints","ref":"/zygote/stable/adjoints/#Custom-Adjoints-2","content":" Custom Adjoints We can extend Zygote to a new function with the  @adjoint  function. julia> mul(a, b) = a*b;\n\njulia> using Zygote: @adjoint\n\njulia> @adjoint mul(a, b) = mul(a, b), c̄ -> (c̄*b, c̄*a)\n\njulia> gradient(mul, 2, 3)\n(3.0, 2.0) It might look strange that we write  mul(a, b)  twice here. In this case we want to call the normal  mul  function for the pullback pass, but you may also want to modify the pullback pass (for example, to capture intermediate results in the pullback)."},{"id":657,"pagetitle":"Custom Adjoints","title":"Custom Types","ref":"/zygote/stable/adjoints/#Custom-Types-1","content":" Custom Types One good use for custom adjoints is to customise how your own types behave during differentiation. For example, in our  Point  example we noticed that the adjoint is a named tuple, rather than another point. import Base: +, -\n\nstruct Point\n  x::Float64\n  y::Float64\nend\n\nwidth(p::Point) = p.x\nheight(p::Point) = p.y\n\na::Point + b::Point = Point(width(a) + width(b), height(a) + height(b))\na::Point - b::Point = Point(width(a) - width(b), height(a) - height(b))\ndist(p::Point) = sqrt(width(p)^2 + height(p)^2) julia> gradient(a -> dist(a), Point(1, 2))[1]\n(x = 0.4472135954999579, y = 0.8944271909999159) Fundamentally, this happens because of Zygote's default adjoint for  getfield . julia> gradient(a -> a.x, Point(1, 2))\n((x = 1, y = nothing),) We can overload this by modifying the getters  height  and  width . julia> @adjoint width(p::Point) = p.x, x̄ -> (Point(x̄, 0),)\n\njulia> @adjoint height(p::Point) = p.y, ȳ -> (Point(0, ȳ),)\n\njulia> Zygote.refresh() # currently needed when defining new adjoints\n\njulia> gradient(a -> height(a), Point(1, 2))\n(Point(0.0, 1.0),)\n\njulia> gradient(a -> dist(a), Point(1, 2))[1]\nPoint(0.4472135954999579, 0.8944271909999159) If you do this you should also overload the  Point  constructor, so that it can handle a  Point  gradient (otherwise this function will error). julia> @adjoint Point(a, b) = Point(a, b), p̄ -> (p̄.x, p̄.y)\n\njulia> gradient(x -> dist(Point(x, 1)), 1)\n(0.7071067811865475,)"},{"id":658,"pagetitle":"Custom Adjoints","title":"Advanced Adjoints","ref":"/zygote/stable/adjoints/#Advanced-Adjoints-1","content":" Advanced Adjoints We usually use custom adjoints to add gradients that Zygote can't derive itself (for example, because they  ccall  to BLAS). But there are some more advanced and fun things we can to with  @adjoint ."},{"id":659,"pagetitle":"Custom Adjoints","title":"Gradient Hooks","ref":"/zygote/stable/adjoints/#Gradient-Hooks-1","content":" Gradient Hooks julia> hook(f, x) = x\nhook (generic function with 1 method)\n\njulia> @adjoint hook(f, x) = x, x̄ -> (nothing, f(x̄)) hook  doesn't seem that interesting, as it doesn't do anything. But the fun part is in the adjoint; it's allowing us to apply a function  f  to the gradient of  x . julia> gradient((a, b) -> hook(-, a)*b, 2, 3)\n(-3.0, 2.0) We could use this for debugging or modifying gradients (e.g. gradient clipping). julia> gradient((a, b) -> hook(ā -> @show(ā), a)*b, 2, 3)\nā = 3.0\n(3.0, 2.0) Zygote provides both  hook  and  @showgrad  so you don't have to write these yourself."},{"id":660,"pagetitle":"Custom Adjoints","title":"Checkpointing","ref":"/zygote/stable/adjoints/#Checkpointing-1","content":" Checkpointing A more advanced example is checkpointing, in which we save memory by re-computing the pullback pass of a function during the backwards pass. To wit: julia> checkpoint(f, x) = f(x)\ncheckpoint (generic function with 1 method)\n\njulia> @adjoint checkpoint(f, x) = f(x), ȳ -> Zygote._pullback(f, x)[2](ȳ)\n\njulia> gradient(x -> checkpoint(sin, x), 1)\n(0.5403023058681398,) If a function has side effects we'll see that the pullback pass happens twice, as expected. julia> foo(x) = (println(x); sin(x))\nfoo (generic function with 1 method)\n\njulia> gradient(x -> checkpoint(foo, x), 1)\n1\n1\n(0.5403023058681398,)"},{"id":661,"pagetitle":"Custom Adjoints","title":"Gradient Reflection","ref":"/zygote/stable/adjoints/#Gradient-Reflection-1","content":" Gradient Reflection It's easy to check whether the code we're running is currently being differentiated. isderiving() = false\n\n@adjoint isderiving() = true, _ -> nothing A more interesting example is to actually detect how many levels of nesting are going on. nestlevel() = 0\n\n@adjoint nestlevel() = nestlevel()+1, _ -> nothing Demo: julia> function f(x)\n         println(nestlevel(), \" levels of nesting\")\n         return x\n       end\nf (generic function with 1 method)\n\njulia> grad(f, x) = gradient(f, x)[1]\ngrad (generic function with 1 method)\n\njulia> f(1);\n0 levels of nesting\n\njulia> grad(f, 1);\n1 levels of nesting\n\njulia> grad(x -> x*grad(f, x), 1);\n2 levels of nesting"},{"id":664,"pagetitle":"Complex Differentiation","title":"Complex Differentiation","ref":"/zygote/stable/complex/#Complex-Differentiation-1","content":" Complex Differentiation Complex numbers add some difficulty to the idea of a \"gradient\". To talk about  gradient(f, x)  here we need to talk a bit more about  f . If  f  returns a real number, things are fairly straightforward. For  $c = x + yi$  and   $z = f(c)$ , we can define the adjoint  $\\bar c = \\frac{\\partial z}{\\partial x} + \\frac{\\partial z}{\\partial y}i = \\bar x + \\bar y i$  (note that  $\\bar c$  means gradient, and  $c'$  means conjugate). It's exactly as if the complex number were just a pair of reals  (re, im) . This works out of the box. julia> using Zygote\n\njulia> gradient(c -> abs2(c), 1+2im)\n(2.0 + 4.0im,) However, while this is a very pragmatic definition that works great for gradient descent, it's not quite aligned with the mathematical notion of the derivative: i.e.  $f(c + \\epsilon) \\approx f(c) + \\bar c \\epsilon$ . In general, such a  $\\bar c$  is not possible for complex numbers except when  f  is  holomorphic  (or  analytic ). Roughly speaking this means that the function is defined over  c  as if it were a normal real number, without exploiting its complex structure – it can't use  real ,  imag ,  conj , or anything that depends on these like  abs2  ( abs2(x) = x*x' ). (This constraint also means there's no overlap with the Real case above; holomorphic functions always return complex numbers for complex input.) But most \"normal\" numerical functions –  exp ,  log , anything that can be represented by a Taylor series – are fine. Fortunately it's also possible to get these derivatives; they are the conjugate of the gradients for the real part. julia> gradient(x -> real(log(x)), 1+2im)[1] |> conj\n0.2 - 0.4im We can check that this function is holomorphic – and thus that the gradient we got out is sensible – by checking the Cauchy-Riemann equations. In other words this should give the same answer: julia> -im*gradient(x -> imag(log(x)), 1+2im)[1] |> conj\n0.2 - 0.4im Notice that this fails in a non-holomorphic case,  f(x) = log(x') : julia> gradient(x -> real(log(x')), 1+2im)[1] |> conj\n0.2 - 0.4im\n\njulia> -im*gradient(x -> imag(log(x')), 1+2im)[1] |> conj\n-0.2 + 0.4im In cases like these, all bets are off. The gradient can only be described with more information; either a 2x2 Jacobian (a generalisation of the Real case, where the second column is now non-zero), or by the two Wirtinger derivatives (a generalisation of the holomorphic case, where  $\\frac{∂ f}{∂ z'}$  is now non-zero). To get these efficiently, as we would a Jacobian, we can just call the backpropagators twice. function jacobi(f, x)\n  y, back = Zygote.pullback(f, x)\n  back(1)[1], back(im)[1]\nend\n\nfunction wirtinger(f, x)\n  du, dv = jacobi(f, x)\n  (du' + im*dv')/2, (du + im*dv)/2\nend julia> wirtinger(x -> 3x^2 + 2x + 1, 1+2im)\n(8.0 + 12.0im, 0.0 + 0.0im)\n\njulia> wirtinger(x -> abs2(x), 1+2im)\n(1.0 - 2.0im, 1.0 + 2.0im)"},{"id":667,"pagetitle":"Glossary","title":"Glossary","ref":"/zygote/stable/glossary/#Glossary-1","content":" Glossary Differentiation is a minefield of conflicting and overlapping terminology, partly because the ideas have been re-discovered in many different fields (e.g. calculus and differential geometry, the traditional AD community, deep learning, finance, etc.) Many of these terms are not well-defined and others may disagree on the details. Nevertheless, we aim to at least say how  we  use these terms, which will be helpful when reading over Zygote issues, discussions and source code. The list is certainly not complete; if you see new terms you'd like defined, or would like to add one yourself, please do open an issue or PR. Adjoint : See  pullback . Used when defining new pullbacks (i.e. the  @adjoint  macro) since this involves defining the adjoint of the Jacobian, in most cases. Backpropagation : Essentially equivalent to \"reverse-mode AD\". Used particularly in the machine learning world to refer to simple chains of functions  f(g(h(x))) , but has generalised beyond that. Derivative : Given a scalar function  $y = f(x)$ , the derivative is  $\\frac{\\partial y}{\\partial x}$ . \"Partial\" is taken for granted in AD; there's no interesting distinction between partial and total derivatives for our purposes. It's all in the eye of the beholder. Differential : Given a function  $f(x)$ , the linearisation  $\\partial f$  such that  $f(x + \\epsilon) \\approx f(x) + \\partial f \\epsilon$ . This is a generalisation of the derivative since it applies to, for example, vector-to-vector functions ( $\\partial f$  is a Jacobian) and holomorphic complex functions ( $\\partial f$  is the first Wirtinger derivative). This is  not , in general, what Zygote calculates, though differentials can usually be derived from gradients. IR : Intermediate Representation. Essentially source code, but usually lower level – e.g. control flow constructs like loops and branches have all been replaced by  goto s. The idea is that it's harder for humans to read/write but easier to manipulate programmatically. Worth looking at SSA form as a paradigmatic example. Gradient : See  sensitivity . There is no technical difference in Zygote's view, though \"gradient\" sometimes distinguishes the sensitivity we actually want from e.g. the internal ones that Zygote produces as it backpropagates. Graph : ML people tend to think of models as \"computation graphs\", but this is no more true than any program is a graph. In fact, pretty much anything is a graph if you squint hard enough. This also refers to the data structure that e.g. TensorFlow and PyTorch build to represent your model, but see  trace  for that. Pullback : Given  $y = f(x)$  the function  $\\bar x = back(̄\\bar y)$ . In other words, the function  back  in  y, back = Zygote.pullback(f, x) . Sensitivity : Used to refer to the gradient  $\\bar x = \\frac{\\partial l}{\\partial x}$  with some scalar loss  $l$ . In other words, you have a value  $x$  (which need not be scalar) at some point in your program, and  $\\bar x$  tells you how you should change that value to decrease the loss. In the AD world, sometimes used to refer to adjoint rules. Source to Source Differentiation : Or Source Code Transformation (SCT). As opposed to  tracing  programs to simplify them, an alternative is to operate directly on a language's source code or IR, generating new source code for pullbacks. This describes Zygote, Swift for TensorFlow, Tapenade and a few other old ADs that worked on C source files. Zygote and Swift are unusual in that they work on in-memory IR rather than text source. To an extent, tracing ADs can be viewed as source transform of a Wengert list / trace. The key difference is that the trace is a lossy representation of the original semantics, which causes problems with e.g. control flow. Systems which can preserve some of those semantics (e.g. autograph) begin to blur the line here, though they are still not nearly as expressive as language IRs. Symbolic Differentiation : Used to refer to differentiation of \"mathematical expressions\", that is, things like  3x^2 + sin(x) . Often distinguished from AD, though this is somewhat arbitrary; you can happily produce a symbolic adjoint for a Wengert list, the only difference being that you're allowed to make variable bindings. So it's really just a special case of AD on an unusually limited language. Tape : This term can refer to pretty much any part of an AD implementation. In particular confusion is caused by conflating the  trace  with the set of values sometimes closed over by a  pullback . Autograd has a combined trace/closure data structure which is usually described as the tape. On the other hand, PyTorch described their implementation as tape-free because the trace/closure is stored as a DAG rather than a vector, so basically all bets are off here. Trace : A recording of each mathematical operation used by a program, made at runtime and usually forming a Wengert list. Traces may or may not also record actual runtime values (e.g. PyTorch vs. TensorFlow). They can often be treated as an IR and compiled, but are distinguished from true IRs in that they unroll and inline all control flow, functions and data structures. The tracing process can be thought of as a kind of partial evaluation, though tracers are typically much less worried about losing information. Vector-Jacobian product : see  pullback . So called because all pullbacks are linear functions that can be represented by (left) multiplication with the Jacobian matrix. Wengert List : A set of simple variable assignments and mathematical expressions, forming a directed graph. Can be thought of as a limited programming language with variable bindings and numerical functions but no control flow or data structures. If you  trace  a program for AD it will typically take this form."},{"id":670,"pagetitle":"Internals","title":"Internals","ref":"/zygote/stable/internals/#Internals-1","content":" Internals"},{"id":671,"pagetitle":"Internals","title":"What Zygote Does","ref":"/zygote/stable/internals/#What-Zygote-Does-1","content":" What Zygote Does These notebooks  and the  Zygote paper  provide useful background on Zygote's transform; this page is particularly focused on implementation details. Before we think about AD, we'll consider some simple cases. We can start by defining a function that produces pullbacks,  J , explicitly for some simple functions. J(::typeof(sin), x) = sin(x), ȳ -> ȳ*cos(x)\nJ(::typeof(cos), x) = cos(x), ȳ -> -ȳ*sin(x)\nJ(::typeof(*), a, b) = a*b, c̄ -> (b*c̄, a*c̄) Now we can call  J  to take a gradient. gradient(f, x...) = J(f, x...)[2](1)\n\ngradient(sin, 1) # (0.540,)\n\ngradient(*, 2, 3) # (3, 2) Now consider a composite function that calls two simple ones: function foo(x)\n  a = sin(x)\n  b = cos(a)\n  return b\nend We can easily differentiate  foo  if we can differentiate the functions it calls. If we can get pullbacks via  J , the pullback for  foo  looks as follows. Where the forward pass calculates  x -> a -> b , the backwards takes  b̄ -> ā -> x̄  via the pullbacks. function J(::typeof(foo), x)\n  a, da = J(sin, x)\n  b, db = J(cos, a)\n  return b, function(b̄)\n    ā, = db(b̄)\n    x̄, = da(ā)\n    return x̄\n  end\nend\n\ngradient(foo, 1) # (-0.403,) Things get just a little more complex when control flow is involved. You can see that the derived adjoint for  pow  mirrors the original function, except that the loop runs in reverse. The easiest way to see why it looks like this is to imagine unrolling the loop  n  times, working out the adjoint, and then turning it back into a loop. function pow(x, n) # x^n\n  r = 1\n  for _ = 1:n\n    r *= x\n  end\n  return r\nend\n\nfunction J(::typeof(pow), x, n)\n  r = 1\n  Js = []\n  for i = 1:n\n    r, back = J(*, r, x)\n    push!(Js, back)\n  end\n  return r, function(r̄)\n    x̄ = 0\n    for i = n:-1:1\n      r̄, x̄′ = Js[i](r̄)\n      x̄ += x̄′\n    end\n    return (x̄, 0)\n  end\nend\n\ngradient(pow, 2, 3) # (12, 0) Despite being reasonably fiddly, this is a fully mechanical transformation, so the only remaining thing is to automate it – a small matter of programming."},{"id":672,"pagetitle":"Internals","title":"Closures","ref":"/zygote/stable/internals/#Closures-1","content":" Closures The  J  function here corresponds to  pullback  in Zygote. However,  pullback  is actually a wrapper around the lower level  _pullback  function. julia> y, back = Zygote._pullback(sin, 0.5);\n\njulia> back(1)\n(nothing, 0.8775825618903728) Why the extra  nothing  here? This actually represents the gradient of the function  sin . This is often  nothing , but when we have closures the function contains data we need gradients for. julia> f = let a = 3; x -> x*a; end\n#19 (generic function with 1 method)\n\njulia> y, back = Zygote._pullback(f, 2);\n\njulia> back(1)\n((a = 2,), 3) This is a minor point for the most part, but  _pullback  will come up in future examples."},{"id":673,"pagetitle":"Internals","title":"Entry Points","ref":"/zygote/stable/internals/#Entry-Points-1","content":" Entry Points You might notice that Zygote is, in effect,  just a macro . We could happily implement Zygote by writing definitions like @differentiable foo(x) = sin(cos(x)) which would expand to generate an appropriate overload to  J . As long as every function we want to differentiate is annotated, this will work just fine. However, it's obviously not ideal to have to annotate every function inside every Julia package in order to make it differentiable. This is where generated functions come in. Making  J  a  generated function  allows us to apply the Zygote macro on an as-needed basis; calling  J(f, x...)  looks up the code for  f(x...) , transforms it, and then behaves as if you had defined  J  for that specific function ahead of time. When we look up the code, we actually get  lowered  (desugared) code rather than an AST. julia> foo(x) = baz(bar(x))\nfoo (generic function with 1 method)\n\njulia> @code_lowered foo(1)\nCodeInfo(\n1 ─ %1 = (Main.bar)(x)\n│   %2 = (Main.baz)(%1)\n└──      return %2 We convert the code to SSA form using Julia's built-in IR data structure, after which it looks like this. julia> Zygote.@code_ir foo(1)\n1 1 ─ %1 = (Main.bar)(_2)::Any\n  │   %2 = (Main.baz)(%1)::Any\n  └──      return %2 (There isn't much difference unless there's some control flow.) The code is then differentiated by the code in  compiler/reverse.jl . You can see the output with  @code_adjoint . julia> Zygote.@code_adjoint foo(1)\n1 1 ─ %1  = (Zygote._pullback)(_2, Zygote.unwrap, Main.bar)::Any\n  │   %2  = (Base.getindex)(%1, 1)::Any\n  │         (Base.getindex)(%1, 2)::Any\n  │   %4  = (Zygote._pullback)(_2, %2, _4)::Any\n  │   %5  = (Base.getindex)(%4, 1)::Any\n  │         (Base.getindex)(%4, 2)::Any\n  │   %7  = (Zygote._pullback)(_2, Zygote.unwrap, Main.baz)::Any\n  │   %8  = (Base.getindex)(%7, 1)::Any\n  │         (Base.getindex)(%7, 2)::Any\n  │   %10 = (Zygote._pullback)(_2, %8, %5)::Any\n  │   %11 = (Base.getindex)(%10, 1)::Any\n  │         (Base.getindex)(%10, 2)::Any\n  └──       return %11\n  1 ─ %1  = Δ()::Any\n1 │   %2  = (@12)(%1)::Any\n  │   %3  = (Zygote.gradindex)(%2, 1)::Any\n  │   %4  = (Zygote.gradindex)(%2, 2)::Any\n  │         (@9)(%3)::Any\n  │   %6  = (@6)(%4)::Any\n  │   %7  = (Zygote.gradindex)(%6, 1)::Any\n  │   %8  = (Zygote.gradindex)(%6, 2)::Any\n  │         (@3)(%7)::Any\n  │   %10 = (Zygote.tuple)(nothing, %8)::Any\n  └──       return %10\n, [1]) This code is quite verbose, mainly due to all the tuple unpacking ( gradindex  is just like  getindex , but handles  nothing  gracefully). There are two pieces of IR here, one for the modified pullback pass and one for the pullback closure. The  @  nodes allow the closure to refer to values from the pullback pass, and the  Δ()  represents the incoming gradient  ȳ . In essence, this is just what we wrote above by hand for  J(::typeof(foo), x) . compiler/emit.jl  lowers this code into runnable IR (e.g. by turning  @  references into  getfield s and stacks), and it's then turned back into lowered code for Julia to run."},{"id":674,"pagetitle":"Internals","title":"Closure Conversion","ref":"/zygote/stable/internals/#Closure-Conversion-1","content":" Closure Conversion There are no closures in lowered Julia code, so we can't actually emit one directly in lowered code. To work around this we have a trick: we have a generic struct like struct Pullback{F}\n  data\nend We can put whatever we want in  data , and the  F  will be the signature for the  original  call, like  Tuple{typeof(foo),Int} . When the pullback gets called it hits  another generated function  which emits the pullback code. In hand written code this would look like: struct Pullback{F}\n  data\nend\n\nfunction J(::typeof(foo), x)\n  a, da = J(sin, x)\n  b, db = J(cos, a)\n  return b, Pullback{typeof(foo)}((da, db))\nend\n\nfunction (p::Pullback{typeof(foo)})(b̄)\n  da, db = p.data[1], p.data[2]\n  ā = db(b̄)\n  x̄ = da(ā)\n  return x̄\nend"},{"id":675,"pagetitle":"Internals","title":"Debugging","ref":"/zygote/stable/internals/#Debugging-1","content":" Debugging Say some of our code is throwing an error. bad(x) = x\n\nZygote.@adjoint bad(x) = x, _ -> error(\"bad\")\n\nfoo(x) = bad(sin(x))\n\ngradient(foo, 1) # error! Zygote can usually give a stacktrace pointing right to the issue here, but in some cases there are compiler crashes that make this harder. In these cases it's best to (a) use  _pullback  and (b) take advantage of Zygote's recursion to narrow down the problem function. julia> y, back = Zygote._pullback(foo, 1);\n\njulia> back(1) # just make up a value here, it just needs to look similar to `y`\nERROR: bad\n\n# Ok, so we try functions that foo calls\n\njulia> y, back = Zygote._pullback(sin, 1);\n\njulia> back(1)\n(nothing, 0.5403023058681398)\n\n# Looks like that's fine\n\njulia> y, back = Zygote._pullback(bad, 1);\n\njulia> back(1) # ok, here's our issue. Lather, rinse, repeat.\nERROR: bad Of course, our goal is that you never have to do this, but until Zygote is more mature it can be a useful way to narrow down test cases."},{"id":678,"pagetitle":"Limitations","title":"Design Limitations","ref":"/zygote/stable/limitations/#Design-Limitations-1","content":" Design Limitations Zygote aims to support differentiating any Julia code, but it still has a few limitations. Notably, you might encounter errors when trying to differentiate: array mutation, try / catch  statements, \"foreign call\" expressions. This section gives examples where each of these errors occurs, as well as possible work-arounds. Below, it also describes some known bugs in expressions Zygote ought to be able to handle."},{"id":679,"pagetitle":"Limitations","title":"Array mutation","ref":"/zygote/stable/limitations/#Array-mutation-1","content":" Array mutation Array mutation is by far the most commonly encountered Zygote limitation. Automatic differentiation (AD) systems like Zygote are built on basic principles of calculus where we encounter  pure  functions. This means that the function,  $y = f(x)$ , does not modify  $x$  and only produces the output  $y$  based on  $x$ . If we have a chain of functions, such as  $y = h(g(f(x)))$ , we can apply the chain rule to differentiate it. AD systems are built to programmatically apply the chain rule to a series of function calls. Unfortunately, typical programs do not behave this way. We might allocate some memory,  x , then call a function  y = f!(x)  that modifies  x  to produce the output  y . This mutating behavior is a  side-effect  of  f! . Side-effects are difficult for AD systems to handle, because the must track changes to mutated variables and store older versions of the variable. For these reasons, Zygote does not handle array mutation for now. Let's explore this with a more concrete example. Here we define a simple mutating function,  f! , which modifies the elements of its input argument,  x , in place. function f!(x)\n  x .= 2 .* x\n\n  return x\nend Let's see what happens when we differentiate  f! julia> gradient(rand(3)) do x\n         sum(f!(x))\n       end\nERROR: Mutating arrays is not supported -- called copyto!(Vector{Float64}, ...)\nThis error occurs when you ask Zygote to differentiate operations that change\nthe elements of arrays in-place (e.g. setting values with x .= ...)\n\nPossible fixes:\n- avoid mutating operations (preferred)\n- or read the documentation and solutions for this error\n  https://fluxml.ai/Zygote.jl/latest/limitations\n\nStacktrace:\n  ... We got an error message and a long stacktrace. The error informs us that our code performs array mutation by calling  copyto!  (we might not have directly called this function, but it is being invoked somewhere in the call stack). We see that our code includes  x .= ...  which is given as an example of array mutation. Other examples of mutating operations include: setting values ( x .= ... ) appending/popping values ( push!(x, v)  /  pop!(x) ) calling mutating functions ( mul!(C, A, B) ) Warning Non-mutating functions might also use mutation under the hood. This can be done for performance reasons or code re-use. function g!(x, y)\n  x .= 2 .* y\n\n  return x\nend\ng(y) = g!(similar(y), y) Here  g  is a \"non-mutating function,\" and it indeed does not mutate  y , its only argument. But it still allocates a new array and calls  g!  on this array which will result in a mutating operation. You may encounter such functions when working with another package. Specifically for array mutation, we can use  Zygote.Buffer  to re-write our function. For example, let's fix the function  g!  above. function g!(x, y)\n  x .= 2 .* y\n\n  return x\nend\n\nfunction g(y)\n  x = Zygote.Buffer(y) # Buffer supports syntax like similar\n  g!(x, y)\n  return copy(x) # this step makes the Buffer immutable (w/o actually copying)\nend\n\njulia> gradient(rand(3)) do y\n         sum(g(y))\n       end\n([2.0, 2.0, 2.0],)"},{"id":680,"pagetitle":"Limitations","title":"Try-catch statements","ref":"/zygote/stable/limitations/#Try-catch-statements-1","content":" Try-catch statements Any expressions involving  try / catch  statements is not supported. function tryme(x)\n  try\n    2 * x\n  catch e\n    throw(e)\n  end\nend\n\njulia> gradient(rand(3)) do x\n         sum(tryme(x))\n       end\nERROR: Compiling Tuple{typeof(tryme), Vector{Float64}}: try/catch is not supported.\nRefer to the Zygote documentation for fixes.\nhttps://fluxml.ai/Zygote.jl/latest/limitations\n\nStacktrace:\n  ... Here  tryme  uses a  try / catch  statement, and Zygote throws an error when trying to differentiate it as expected.  try / catch  expressions are used for error handling, but they are less common in Julia compared to some other languages."},{"id":681,"pagetitle":"Limitations","title":"Foreign call expressions","ref":"/zygote/stable/limitations/#Foreign-call-expressions-1","content":" Foreign call expressions Foreign call expressions refer to expressions that call external libraries such as code written in C or Fortran. You may want to read more about these calls in the  Julia documentation . Scientific computing libraries in Julia may call established C or Fortran libraries under the hood. Since the underlying code for a foreign call expression is not in Julia, it is not possible for Zygote to differentiate this expression. Below, we define a function that calls a standard C function,  clock . This function returns the Unix clock as an  Int32 . julia> jclock(x) = ccall(:clock, Int32, ()) * 2\njclock (generic function with 1 method)\n\njulia> jclock(2)\n30921278\n\njulia> gradient(jclock, rand())\nERROR: Can't differentiate foreigncall expression\nYou might want to check the Zygote limitations documentation.\nhttps://fluxml.ai/Zygote.jl/latest/limitations\n\nStacktrace:\n  ... jclock  will multiply the result of our C function by an argument. When we try to differentiate with respect to this argument, we get an  foreigncall  error."},{"id":682,"pagetitle":"Limitations","title":"Solutions","ref":"/zygote/stable/limitations/#Solutions-1","content":" Solutions For all of the errors above, the suggested solutions are similar. You have the following possible work arounds available (in order of preference): avoid the error-inducing operation (e.g. do not use mutating functions) define a  custom  ChainRulesCore.rrule open an  issue on Zygote Avoiding the operation is simple, just don't do it! If you are using a mutating function, try to use a non-mutating variant. If you are using  try / catch  statements, try to use more graceful error handling such as returning  nothing  or another sentinel value. Recall that array mutation can also be avoided by using  Zygote.Buffer  as discussed above. Sometimes, we cannot avoid expressions that Zygote cannot differentiate, but we may be able to manually derive a gradient. In these cases, you can write  a custom  rrule  using ChainRules.jl. Please refer to the linked ChainRules documentation for how to do this.  This solution is the only solution available for foreign call expressions.  Below, we provide a custom  rrule  for  jclock . jclock(x) = ccall(:clock, Int32, ()) * x\n\nfunction ChainRulesCore.rrule(::typeof(jclock), x)\n  y = jclock(x)\n  pb(ȳ) = (ChainRulesCore.NoTangent(), ȳ * y)\n\n  return y, pb\nend\n\njulia> gradient(jclock, rand())\n(674298.4243400148,) Lastly, if the code causing problems can be fixed, but it is package code instead of your code, then you should open an issue. For functions built into Julia or its standard libraries, you can open an issue with Zygote.jl or ChainRules.jl. For functions in other packages, you can open an issue with the corresponding package issue tracker."},{"id":683,"pagetitle":"Limitations","title":"Known Issues","ref":"/zygote/stable/limitations/#Known-Issues-1","content":" Known Issues Zygote's issue tracker has the current list of open  bugs . There are some general principles about things you may wish to avoid if you can:"},{"id":684,"pagetitle":"Limitations","title":"mutable struct s","ref":"/zygote/stable/limitations/#mutable-structs-1","content":" mutable struct s Zygote has limited support for mutation, and in particular will allow you to change a field in some  mutable struct X; a; b; end  by setting  x.a = val . However, this has  many limitations  and should be avoided if possible. The simple solution is to use only immutable  struct s.  If you need to modify them, using something like  @set  from  Accessors.jl  should work well. This returns a new object, but does not have side-effects on other copies of it. "},{"id":685,"pagetitle":"Limitations","title":"Re-using variable names","ref":"/zygote/stable/limitations/#Re-using-variable-names-1","content":" Re-using variable names It is common to accumulate values in a loop by re-binding the same variable name to a new value many times, for example: function mysum(x::Real, n::Int)\n  tot = 0.0\n  for i in 1:n\n    tot += x^n  # binds symbol `tot` to new value\n  end\n  return tot\nend However, sometimes such re-binding confuses Zygote, especially if the type of the value changes. Especially if the variable is \"boxed\", as will happen if you re-bind from within a closure (such as the function created by a  do  block)."},{"id":686,"pagetitle":"Limitations","title":"Second derivatives","ref":"/zygote/stable/limitations/#Second-derivatives-1","content":" Second derivatives In principle Zygote supports taking derivatives of derivatives. There are, however, a few problems: Quite a few of its rules are not written in a way that is itself differentiable. For instance they may work by making an array then writing into it, which is mutation of the sort forbidden above.  The complexity of the code grows rapidly, as Zygote differentiates its own un-optimised output. Reverse mode over reverse mode is seldom the best algorithm. The issue tracker has a label for  second order , which will outline where the bodies are buried. Often using a different AD system over Zygote is a better solution. This is what  hessian  does, using ForwardDiff over Zygote, but other combinations are possible. (Note that rules defined here mean that Zygote over ForwardDiff is translated to ForwardDiff over ForwardDiff.)"},{"id":689,"pagetitle":"Profiling","title":"Debugging in Time and Space","ref":"/zygote/stable/profiling/#Debugging-in-Time-and-Space-1","content":" Debugging in Time and Space Because Zygote generates Julia code for the backwards pass, many of Julia's normal profiling and performance debugging tools work well on it out of the box."},{"id":690,"pagetitle":"Profiling","title":"Performance Profiling","ref":"/zygote/stable/profiling/#Performance-Profiling-1","content":" Performance Profiling Julia's  sampling profiler  is useful for understanding performance. We recommend  running the profiler in Juno , but the terminal or  ProfileView.jl  also work well. The bars indicate time taken in both the forwards and backwards passes at that line. The canopy chart on the right shows us each function call as a block, arranged so that when  f  calls  g ,  g  gets a block just below  f , which is bigger the longer it took to run. If we dig down the call stack we'll eventually find the adjoints for things like  matmul , which we can click on to view. The trace inside the adjoint can be used to distinguish time taken by the forwards and backwards passes."},{"id":691,"pagetitle":"Profiling","title":"Memory Profiling","ref":"/zygote/stable/profiling/#Memory-Profiling-1","content":" Memory Profiling Reverse-mode AD typically uses memory proportional to the number of operations in the program, so long-running programs can also suffer memory usage issues. Zygote includes a space profiler to help debug these issues. Like the time profiler, it shows a canopy chart, but this time hovering over it displays the number of bytes stored by each line of the program. Note that this currently only works inside Juno."},{"id":692,"pagetitle":"Profiling","title":"Reflection","ref":"/zygote/stable/profiling/#Reflection-1","content":" Reflection Julia's code and type inference reflection tools can also be useful, though Zygote's use of closures can make the output noisy. To see the code Julia runs you should use the low-level  _pullback  method and the pullback it returns. This will directly show either the derived adjoint code or the code for a custom adjoint, if there is one. julia> using Zygote: Context, _pullback\n\njulia> add(a, b) = a+b\n\njulia> @code_typed _pullback(Context(), add, 1, 2)\nCodeInfo(\n1 ─ %1 = (Base.getfield)(args, 1)::Int64\n│   %2 = (Base.getfield)(args, 2)::Int64\n│   %3 = (Base.add_int)(%1, %2)::Int64\n│   %4 = (Base.tuple)(%3, $(QuoteNode(∂(add))))::PartialTuple(Tuple{Int64,typeof(∂(add))}, Any[Int64, Const(∂(add), false)])\n└──      return %4\n) => Tuple{Int64,typeof(∂(add))}\n\njulia> y, back = _pullback(Context(), add, 1, 2)\n(3, ∂(add))\n\njulia> @code_typed back(1)\nCodeInfo(\n1 ─ %1 = (Base.mul_int)(Δ, 1)::Int64\n│   %2 = (Base.mul_int)(Δ, 1)::Int64\n│   %3 = (Zygote.tuple)(nothing, %1, %2)::PartialTuple(Tuple{Nothing,Int64,Int64}, Any[Const(nothing, false), Int64, Int64])\n└──      return %3\n) => Tuple{Nothing,Int64,Int64}"},{"id":697,"pagetitle":"Utilities","title":"Utilities","ref":"/zygote/stable/utils/#Utilities-1","content":" Utilities Zygote's gradients can be used to construct a Jacobian (by repeated evaluation) or a Hessian (by taking a second derivative)."},{"id":698,"pagetitle":"Utilities","title":"Zygote.jacobian","ref":"/zygote/stable/utils/#Zygote.jacobian","content":" Zygote.jacobian  —  Function jacobian(f, args...) -> Tuple For each array  a ∈ args  this returns a matrix with  Ja[k,i] = ∂y[k]/∂a[i]  where  y = f(args...)  is usually a vector. Arrays of higher dimension are treated like  vec(a) , or  vec(y)  for output. For scalar  x::Number ∈ args , the result is a vector  Jx[k] = ∂y[k]/∂x , while for scalar  y  all results have just one row. With any other argument type, no result is produced, even if  gradient  would work. This reverse-mode Jacobian needs to evaluate the pullback once for each element of  y . Doing so is usually only efficient when  length(y)  is small compared to  length(a) , otherwise forward mode is likely to be better. See also  withjacobian ,  hessian ,  hessian_reverse . Examples julia> jacobian(a -> 100*a[1:3].^2, 1:7)[1]  # first index (rows) is output\n3×7 Matrix{Int64}:\n 200    0    0  0  0  0  0\n   0  400    0  0  0  0  0\n   0    0  600  0  0  0  0\n\njulia> jacobian((a,x) -> a.^2 .* x, [1,2,3], 1)  # scalar argument has vector jacobian\n([2 0 0; 0 4 0; 0 0 6], [1, 4, 9])\n\njulia> jacobian((a,d) -> prod(a, dims=d), [1 2; 3 4; 5 6], 2)\n([2 0 … 0 0; 0 4 … 3 0; 0 0 … 0 5], [0, 0, 0]) Warning For arguments of any type except  Number  &  AbstractArray , the result is  nothing . julia> jacobian((a,s) -> a.^length(s), [1,2,3], \"str\")\n([3 0 0; 0 12 0; 0 0 27], nothing)\n\njulia> jacobian((a,t) -> sum(a .* t[1]) + t[2], [1,2,3], (4,5))\n([4 4 4], nothing)\n\njulia> gradient((a,t) -> sum(a .* t[1]) + t[2], [1,2,3], (4,5))  # gradient undersands the tuple\n([4 4 4], (6, 1)) source jacobian(loss, ::Params) Like  gradient  with implicit parameters, this method takes a zero-argument function and returns an  IdDict -like object, now containing the Jacobian for each parameter. Examples julia> xs = [1 2; 3 4]; ys = [5,7,9];\n\njulia> Jxy = jacobian(() -> ys[1:2] .+ sum(xs.^2), Params([xs, ys]))\nGrads(...)\n\njulia> Jxy[ys]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  1  0\n\njulia> Jxy[xs]\n2×4 Matrix{Int64}:\n 2  6  4  8\n 2  6  4  8 source"},{"id":699,"pagetitle":"Utilities","title":"Zygote.hessian","ref":"/zygote/stable/utils/#Zygote.hessian","content":" Zygote.hessian  —  Function hessian(f, x) Construct the Hessian  ∂²f/∂x² , where  x  is a real number or an array, and  f(x)  is a real number. When  x  is an array, the result is a matrix  H[i,j] = ∂²f/∂x[i]∂x[j] , using linear indexing  x[i]  even if the argument is higher-dimensional. This uses forward over reverse, ForwardDiff over Zygote, calling  hessian_dual(f, x) . See  hessian_reverse  for an all-Zygote alternative. See also  diaghessian  to compute only the diagonal part. Examples julia> hessian(x -> x[1]*x[2], randn(2))\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> hessian(x -> sum(x.^3), [1 2; 3 4])  # uses linear indexing of x\n4×4 Matrix{Int64}:\n 6   0   0   0\n 0  18   0   0\n 0   0  12   0\n 0   0   0  24\n\njulia> hessian(sin, pi/2)\n-1.0 source"},{"id":700,"pagetitle":"Utilities","title":"Zygote.hessian_reverse","ref":"/zygote/stable/utils/#Zygote.hessian_reverse","content":" Zygote.hessian_reverse  —  Function hessian_reverse(f, x) This should be equivalent to  hessian(f, x) , but implemented using reverse over reverse mode, all Zygote. (This is usually much slower, and more likely to find errors.) source"},{"id":701,"pagetitle":"Utilities","title":"Zygote.diaghessian","ref":"/zygote/stable/utils/#Zygote.diaghessian","content":" Zygote.diaghessian  —  Function diaghessian(f, args...) -> Tuple Diagonal part of the Hessian. Returns a tuple containing, for each argument  x ,  h  of the same shape with  h[i] = Hᵢᵢ = ∂²y/∂x[i]∂x[i] .  The original evaluation  y = f(args...)  must give a real number  y . For one vector argument  x , this is equivalent to  (diag(hessian(f,x)),) . Like  hessian  it uses ForwardDiff over Zygote.  Warning For arguments of any type except  Number  &  AbstractArray , the result is  nothing . Examples julia> diaghessian(x -> sum(x.^3), [1 2; 3 4])[1]\n2×2 Matrix{Int64}:\n  6  12\n 18  24\n\njulia> Diagonal(vec(ans)) == hessian(x -> sum(x.^3), [1 2; 3 4])  # full Hessian is diagonal\ntrue\n\njulia> diaghessian((x,y) -> sum(x .* y .* y'), [1 22; 333 4], [0.5, 0.666])  # two array arguments\n([0.0 0.0; 0.0 0.0], [2.0, 8.0])\n\njulia> diaghessian(atan, 1, 2)  # two scalar arguments\n(-0.16, 0.16)\n\njulia> hessian(xy -> atan(xy[1], xy[2]), [1, 2])  # full Hessian is not diagonal\n2×2 Matrix{Float64}:\n -0.16  -0.12\n -0.12   0.16 source Zygote also provides a set of helpful utilities. These are all \"user-level\" tools – in other words you could have written them easily yourself, but they live in Zygote for convenience. See  ChainRules.ignore_derivatives  if you want to exclude some of your code from the gradient calculation. This replaces previous Zygote-specific  ignore  and  dropgrad  functionality."},{"id":702,"pagetitle":"Utilities","title":"Zygote.withgradient","ref":"/zygote/stable/utils/#Zygote.withgradient","content":" Zygote.withgradient  —  Function withgradient(f, args...)\nwithgradient(f, ::Params) Returns both the value of the function and the  gradient , as a named tuple.  julia> y, ∇ = withgradient(/, 1, 2)\n(val = 0.5, grad = (0.5, -0.25))\n\njulia> ∇ == gradient(/, 1, 2)\ntrue Allows you to capture auxillary outputs, in addition to the scalar used by  gradient . To do this,  f  must return a Tuple or NamedTuple. Then it calculates  grad = gradient(first∘f, args...) but returns the whole val = f(args...)`: julia> withgradient([1,2,4]) do x\n          z = 1 ./ x\n          sum(z), z  # here z is an auxillary output\n       end\n(val = (1.75, [1.0, 0.5, 0.25]), grad = ([-1.0, -0.25, -0.0625],))\n\njulia> withgradient(3.0, 4.0) do x, y\n          (div = x/y, mul = x*y)\n       end\n(val = (div = 0.75, mul = 12.0), grad = (0.25, -0.1875)) Also supports implicit mode: julia> w = [3.0];\n\njulia> res = withgradient(() -> sum(abs2, w), Params([w]))\n(val = 9.0, grad = Grads(...))\n\njulia> res.grad[w]\n1-element Vector{Float64}:\n 6.0 source"},{"id":703,"pagetitle":"Utilities","title":"Zygote.withjacobian","ref":"/zygote/stable/utils/#Zygote.withjacobian","content":" Zygote.withjacobian  —  Function withjacobian(f, args...) Returns both the value  f(args...)  and the  jacobian  as a named tuple. julia> withjacobian(cumsum, [1,2,3])\n(val = [1, 3, 6], grad = ([1 0 0; 1 1 0; 1 1 1],)) source"},{"id":704,"pagetitle":"Utilities","title":"Zygote.@showgrad","ref":"/zygote/stable/utils/#Zygote.@showgrad","content":" Zygote.@showgrad  —  Macro @showgrad(x) -> x Much like  @show , but shows the gradient about to accumulate to  x . Useful for debugging gradients. julia> gradient(2, 3) do a, b\n         @showgrad(a)*b\n       end\n∂(a) = 3\n(3, 2) Note that the gradient depends on how the output of  @showgrad  is  used , and is not the  overall  gradient of the variable  a . For example: julia> gradient(2) do a\n     @showgrad(a)*a\n   end\n∂(a) = 2\n(4,)\n\njulia> gradient(2, 3) do a, b\n         @showgrad(a) # not used, so no gradient\n         a*b\n       end\n∂(a) = nothing\n(3, 2) source"},{"id":705,"pagetitle":"Utilities","title":"Zygote.hook","ref":"/zygote/stable/utils/#Zygote.hook","content":" Zygote.hook  —  Function hook(x̄ -> ..., x) -> x Gradient hooks. Allows you to apply an arbitrary function to the gradient for  x . julia> gradient(2, 3) do a, b\n         hook(ā -> @show(ā), a)*b\n       end\nā = 3\n(3, 2)\n\njulia> gradient(2, 3) do a, b\n         hook(-, a)*b\n       end\n(-3, 2) source"},{"id":706,"pagetitle":"Utilities","title":"Zygote.Buffer","ref":"/zygote/stable/utils/#Zygote.Buffer","content":" Zygote.Buffer  —  Type Buffer(xs, ...) Buffer  is an array-like type which is mutable when taking gradients. You can construct a  Buffer  with the same syntax as  similar  (e.g.  Buffer(xs, 5) ) and then use normal indexing. Finally, use  copy  to get back a normal array. For example: julia> function vstack(xs)\n           buf = Buffer(xs, length(xs), 5)\n           for i = 1:5\n             buf[:, i] = xs\n           end\n           return copy(buf)\n         end\nvstack (generic function with 1 method)\n\njulia> vstack([1, 2, 3])\n3×5 Array{Int64,2}:\n 1  1  1  1  1\n 2  2  2  2  2\n 3  3  3  3  3\n\njulia> gradient(x -> sum(vstack(x)), [1, 2, 3])\n([5.0, 5.0, 5.0],) Buffer  is not an  AbstractArray  and can't be used for linear algebra operations like matrix multiplication. This prevents it from being captured by pullbacks. copy  is a semantic copy, but does not allocate memory. Instead the  Buffer  is made immutable after copying. source"},{"id":707,"pagetitle":"Utilities","title":"Zygote.forwarddiff","ref":"/zygote/stable/utils/#Zygote.forwarddiff","content":" Zygote.forwarddiff  —  Function forwarddiff(f, x; chunk_threshold = ForwardDiff.DEFAULT_CHUNK_THRESHOLD) -> f(x) Runs  f(x)  as usual, but instructs Zygote to differentiate  f  using forward mode, rather than the usual reverse mode. The  chunk_threshold  argument controls the maximum chunk size (c.f. ForwardDiff documentation). Forward mode takes time linear in  length(x)  but only has constant memory overhead, and is very efficient for scalars, so in some cases this can be a useful optimisation. julia> function pow(x, n)\n         r = one(x)\n         for i = 1:n\n           r *= x\n         end\n         return r\n       end\npow (generic function with 1 method)\n\njulia> gradient(5) do x\n         forwarddiff(x) do x\n           pow(x, 2)\n         end\n       end\n(10,) Note that the function  f  will  drop gradients  for any closed-over values. julia> gradient(2, 3) do a, b\n         forwarddiff(a) do a\n           a*b\n         end\n       end\n(3, nothing) This can be rewritten by explicitly passing through  b , i.e. gradient(2, 3) do a, b\n  forwarddiff([a, b]) do (a, b)\n    a*b\n  end\nend source"},{"id":708,"pagetitle":"Utilities","title":"Zygote.checkpointed","ref":"/zygote/stable/utils/#Zygote.checkpointed","content":" Zygote.checkpointed  —  Function checkpointed(f, xs...) Use gradient checkpointing on the call  f(xs...) . This means that  checkpointed(f, xs...) === f(xs...) , but when computing the derivative intermediate results from the forward pass of  f  will not be stored. Instead the forward pass will be repeated, when computing the derivative. This saves memory at the cost of increasing execution time. Warning If  f  is not a pure function,  checkpointed  will likely give wrong results. source Params  and  Grads  can be copied to and from arrays using the  copy!  function."},{"id":709,"pagetitle":"Utilities","title":"Working with Grads","ref":"/zygote/stable/utils/#Working-with-Grads-1","content":" Working with Grads Map, broadcast, and iteration are supported for the dictionary-like  Grads  objects. These operations are value based and preserve the keys. using Zygote, Test\n\nw, x1, x2, b = rand(2), rand(2), rand(2), rand(2)\n\ngs1 = gradient(() -> sum(tanh.(w .* x1 .+ b)), Params([w, b]))\ngs2 = gradient(() -> sum(tanh.(w .* x2 .+ b)), Params([w, b]))\n\n# accumulate gradients\ngs = gs1 .+ gs2\n@test gs[w] ≈ gs1[w] + gs2[w]\n@test gs[b] ≈ gs1[b] + gs2[b]\n\n# gradients and IdDict interact nicely\n# note that an IdDict must be used for gradient algebra on the GPU\ngs .+= IdDict(p => randn(size(p)) for p in keys(gs))\n\n# clip gradients\nmap(x -> clamp.(x, -0.1, 0.1), gs)\n\n# clip gradients in-place\nforeach(x -> clamp!(x, -0.1, 0.1), gs)\n\nfor (p, g) in pairs(gs)\n  # do something with parameter `p` and corresponding gradient `g`\nend\n\n# note that gradients must be w.r.t. to the same parameter key set\ngs3 = gradient(() -> sum(tanh.(w .* x2)), Params([w]))\n# gs3 does not have the key b\n@test_throws ArgumentError gs1 .+ gs3"},{"id":712,"pagetitle":"Home","title":"NNlib.jl","ref":"/nnlib/stable/#NNlib.jl","content":" NNlib.jl NNlib  provides a library of functions useful for neural networks, such as softmax, sigmoid, batched multiplication, convolutions and pooling. Many of these are used by  Flux.jl , which loads this package, but they may be used independently. For use with automatic differentiation, this package defines gradients using  ChainRules.jl . These will be seen by various packages including  Zygote.jl . GPU support is provided as package extensions. In order to load the extensions, use the imports using NNlib, CUDA, cuDNN for CUDA support, or using NNlib, AMDGPU for AMDGPU support."},{"id":715,"pagetitle":"Reference","title":"Reference","ref":"/nnlib/stable/reference/#Reference","content":" Reference The API reference of  NNlib ."},{"id":716,"pagetitle":"Reference","title":"Activation Functions","ref":"/nnlib/stable/reference/#Activation-Functions","content":" Activation Functions Non-linearities that go between layers of your model. Note that, unless otherwise stated, activation functions operate on scalars. To apply them to an array you can call  σ.(xs) ,  relu.(xs)  and so on."},{"id":717,"pagetitle":"Reference","title":"NNlib.celu","ref":"/nnlib/stable/reference/#NNlib.celu","content":" NNlib.celu  —  Function celu(x, α=1) = x ≥ 0 ? x : α * (exp(x/α) - 1) Activation function from  \"Continuously Differentiable Exponential Linear Units\" . julia> lineplot(celu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ celu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠔⠒⠋⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⠤⠤⠤⠤⠔⠒⠒⠒⠊⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> celu(-10f0)\n-0.9999546f0 source"},{"id":718,"pagetitle":"Reference","title":"NNlib.elu","ref":"/nnlib/stable/reference/#NNlib.elu","content":" NNlib.elu  —  Function elu(x, α=1) = x > 0 ? x : α * (exp(x) - 1) Exponential Linear Unit activation function. See  \"Fast and Accurate Deep Network Learning by Exponential Linear Units\" . You can also specify the coefficient explicitly, e.g.  elu(x, 1) . julia> lineplot(elu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐       \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ elu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│       \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠔⠒⠋⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n        -1 │⠤⠤⠤⠤⠔⠒⠒⠒⠊⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           └────────────────────────────────────────┘       \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀       \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀       \n\njulia> elu(-10f0)\n-0.9999546f0\n\njulia> elu(-10f0, 2)\n-1.9999092f0 source"},{"id":719,"pagetitle":"Reference","title":"NNlib.gelu","ref":"/nnlib/stable/reference/#NNlib.gelu","content":" NNlib.gelu  —  Function gelu(x) = 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x^3))) Activation function from  \"Gaussian Error Linear Units\" . julia> lineplot(gelu, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊│ gelu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⣀⡠⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⣤⣤⣤⣤⣤⣤⣤⣤⡤⠤⠤⠤⠤⠤⠤⠤⣤⣤⣤⡤⡧⠶⠶⠭⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> lineplot(gelu, -5, 0, height=7);\n\njulia> lineplot!(ans, swish)\n             ┌────────────────────────────────────────┐         \n           0 │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠒⠒⠤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ gelu(x) \n             │⠑⠒⠢⠤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇│ swish(x)\n             │⠀⠀⠀⠀⠀⠈⠉⠒⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⠁│         \n   f(x)      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇⠀│         \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⣄⠀⠀⠀⠀⠀⢠⡞⠀⠀│         \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⢄⣀⣀⡤⢣⠃⠀⠀│         \n        -0.2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠇⠀⠀⠀│         \n             └────────────────────────────────────────┘         \n             ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0⠀         \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":720,"pagetitle":"Reference","title":"NNlib.hardsigmoid","ref":"/nnlib/stable/reference/#NNlib.hardsigmoid","content":" NNlib.hardsigmoid  —  Function hardσ(x) = max(0, min(1, (x + 3) / 6)) Piecewise linear approximation of  sigmoid . julia> lineplot(hardsigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐         \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋⠉⠉⠉⠉⠉⠉⠉⠉│ hardσ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡠⠔⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⡗⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠋⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⠤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐     \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          └────────────────────────────────────────┘     \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀      source"},{"id":721,"pagetitle":"Reference","title":"NNlib.sigmoid_fast","ref":"/nnlib/stable/reference/#NNlib.sigmoid_fast","content":" NNlib.sigmoid_fast  —  Function sigmoid_fast(x) This is a faster, and very slightly less accurate, version of  sigmoid . For `x::Float32, perhaps 3 times faster, and maximum errors 2 eps instead of 1. See also  tanh_fast . julia> sigmoid(0.2f0)\n0.54983395f0\n\njulia> sigmoid_fast(0.2f0)\n0.54983395f0\n\njulia> hardσ(0.2f0)\n0.53333336f0 source"},{"id":722,"pagetitle":"Reference","title":"NNlib.hardtanh","ref":"/nnlib/stable/reference/#NNlib.hardtanh","content":" NNlib.hardtanh  —  Function hardtanh(x) = max(-1, min(1, x)) Segment-wise linear approximation of  tanh , much cheaper to compute. See  \"Large Scale Machine Learning\" . See also  tanh_fast . julia> lineplot(hardtanh, -2, 2, height=7)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⠔⠋⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ hardtanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⣀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡷⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠖⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠖⠋⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⠔⠋⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x\n\njulia> lineplot(tanh, -2, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠤⠒⠒⠒⠊⠉⠉⠉│ tanh(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⢀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⡷⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠔⠊⠁⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -1 │⣀⣀⣀⡠⠤⠤⠤⠖⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":723,"pagetitle":"Reference","title":"NNlib.tanh_fast","ref":"/nnlib/stable/reference/#NNlib.tanh_fast","content":" NNlib.tanh_fast  —  Function tanh_fast(x) This is a faster but slighly less accurate version of  tanh . Where Julia's  tanh  function has an error under 2 eps, this may be wrong by 5 eps, a reduction by less than one decimal digit.  For  x::Float32  this is usually about 10 times faster, with a smaller speedup for  x::Float64 . For any other number types, it just calls  tanh . See also  sigmoid_fast . julia> tanh(0.5f0)\n0.46211717f0\n\njulia> tanh_fast(0.5f0)\n0.46211714f0\n\njulia> hard_tanh(0.5f0)\n0.5f0 source"},{"id":724,"pagetitle":"Reference","title":"NNlib.leakyrelu","ref":"/nnlib/stable/reference/#NNlib.leakyrelu","content":" NNlib.leakyrelu  —  Function leakyrelu(x, a=0.01) = max(a*x, x) Leaky  Rectified Linear Unit  activation function. You can also specify the coefficient explicitly, e.g.  leakyrelu(x, 0.01) . julia> lineplot(x -> leakyrelu(x, 0.5), -2, 2, height=7)\n           ┌────────────────────────────────────────┐       \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉│ #42(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠉⠀⠀⠀⠀│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│       \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⣤⡤⡧⠶⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│       \n           │⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⠤⠤⠒⠒⠋⠉⠁⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n        -1 │⣀⣀⠤⠤⠒⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│       \n           └────────────────────────────────────────┘       \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀       \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀       \n\njulia> leakyrelu(-10f0, 0.2)\n-2.0f0\n\njulia> leakyrelu(-10f0, 0.02)\n-0.5f0 source"},{"id":725,"pagetitle":"Reference","title":"NNlib.lisht","ref":"/nnlib/stable/reference/#NNlib.lisht","content":" NNlib.lisht  —  Function lisht(x) = x * tanh(x) Activation function from   \"LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent ...\" julia> lineplot(lisht, -2, 2, height=7)\n          ┌────────────────────────────────────────┐         \n        2 │⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔│ lisht(x)\n          │⠀⠈⠑⢦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀│         \n          │⠀⠀⠀⠀⠈⠣⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠁⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⠔⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⢄⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡠⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⠦⣄⣀⣀⣇⣀⣀⠤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> lineplot!(ans, logcosh)\n          ┌────────────────────────────────────────┐           \n        2 │⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔│ lisht(x)  \n          │⠀⠈⠑⢦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀│ logcosh(x)\n          │⠢⣄⠀⠀⠈⠣⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⠀⠀⣀⠔│           \n   f(x)   │⠀⠈⠑⠢⣀⠀⠀⠑⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠁⠀⣀⠔⠊⠁⠀│           \n          │⠀⠀⠀⠀⠀⠉⠢⢄⡀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⠔⠋⠀⡠⠔⠋⠁⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠦⣌⡓⢄⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡠⠖⣁⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀│           \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠪⠷⣦⣄⣀⣀⣇⣀⣀⣤⠶⠕⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n          └────────────────────────────────────────┘           \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀           \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            source"},{"id":726,"pagetitle":"Reference","title":"NNlib.logcosh","ref":"/nnlib/stable/reference/#NNlib.logcosh","content":" NNlib.logcosh  —  Function logcosh(x) Return  log(cosh(x))  which is computed in a numerically stable way. julia> lineplot(logcosh, -5, 5, height=7)\n          ┌────────────────────────────────────────┐           \n        5 │⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ logcosh(x)\n          │⠉⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋│           \n          │⠀⠀⠀⠑⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠊⠁⠀⠀│           \n   f(x)   │⠀⠀⠀⠀⠀⠀⠑⠦⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠊⠁⠀⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⠦⡀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀│           \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⠦⡀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⢀⡤⠒⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠢⢄⣀⣀⣇⣀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│           \n          └────────────────────────────────────────┘           \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀           \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            source"},{"id":727,"pagetitle":"Reference","title":"NNlib.logsigmoid","ref":"/nnlib/stable/reference/#NNlib.logsigmoid","content":" NNlib.logsigmoid  —  Function logσ(x) Return  log(σ(x))  which is computed in a numerically stable way. julia> lineplot(logsigmoid, -5, 5, height=7)\n           ┌────────────────────────────────────────┐        \n         0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡧⠤⠔⠒⠒⠒⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ logσ(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠊⠉⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠒⠉⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⢀⡤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⣀⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⡤⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -6 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":728,"pagetitle":"Reference","title":"NNlib.mish","ref":"/nnlib/stable/reference/#NNlib.mish","content":" NNlib.mish  —  Function mish(x) = x * tanh(softplus(x)) Activation function from  \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\" . julia> lineplot(mish, -5, 5, height=7)\n           ┌────────────────────────────────────────┐        \n         5 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋│ mish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠒⠁⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠔⠋⠁⠀⠀⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣧⣔⣊⣁⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀│        \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":729,"pagetitle":"Reference","title":"NNlib.relu","ref":"/nnlib/stable/reference/#NNlib.relu","content":" NNlib.relu  —  Function relu(x) = max(0, x) Rectified Linear Unit  activation function. julia> lineplot(relu, -2, 2, height=7)\n          ┌────────────────────────────────────────┐        \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠋│ relu(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠊⠁⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠊⠁⠀⠀⠀⠀⠀│        \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⡠⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⡠⠖⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⠔⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n          └────────────────────────────────────────┘        \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         source"},{"id":730,"pagetitle":"Reference","title":"NNlib.relu6","ref":"/nnlib/stable/reference/#NNlib.relu6","content":" NNlib.relu6  —  Function relu6(x) = min(max(0, x), 6) Rectified Linear Unit  activation function capped at 6. See  \"Convolutional Deep Belief Networks\"  from CIFAR-10. julia> lineplot(relu6, -10, 10, height=7)\n          ┌────────────────────────────────────────┐         \n        6 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠎⠉⠉⠉⠉⠉⠉⠉⠉│ relu6(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⡔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⡤⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⡠⠎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⠖⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡔⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡧⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-10⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀10⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":731,"pagetitle":"Reference","title":"NNlib.rrelu","ref":"/nnlib/stable/reference/#NNlib.rrelu","content":" NNlib.rrelu  —  Function rrelu(x, lo=1/8, hi=1/3) = max(a*x, x)\n# where `a` is randomly sampled from uniform distribution `U(lo, hi)` Randomized Leaky Rectified Linear Unit activation function. See  \"Empirical Evaluation of Rectified Activations\"  You can also specify the bound explicitly, e.g.  rrelu(x, 0.0, 1.0) . julia> lineplot(rrelu, -20, 10, height=7)\n            ┌────────────────────────────────────────┐         \n         10 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋│ rrelu(x)\n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀│         \n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)     │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡤⠤⣤⣤⢤⣤⣤⠤⠤⠤⢼⠮⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│         \n            │⣰⢀⣆⡄⣄⡄⡠⡰⠦⠷⡜⢢⠷⠳⠢⠊⠉⠉⠀⠀⠁⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n            │⠃⠉⠙⠘⠃⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        -10 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n            └────────────────────────────────────────┘         \n            ⠀-20⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀10⠀         \n            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀         \n\njulia> extrema(rrelu.(fill(-10f0, 1000)))\n(-3.3316886f0, -1.2548422f0) source"},{"id":732,"pagetitle":"Reference","title":"NNlib.selu","ref":"/nnlib/stable/reference/#NNlib.selu","content":" NNlib.selu  —  Function selu(x) = λ * (x ≥ 0 ? x : α * (exp(x) - 1))\n\nλ ≈ 1.05070...\nα ≈ 1.67326... Scaled exponential linear units. See  \"Self-Normalizing Neural Networks\" . julia> lineplot(selu, -3, 2, height=7)\n           ┌────────────────────────────────────────┐        \n         3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ selu(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⠒│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⣀⠤⠖⠊⠉⠀⠀⠀⠀│        \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⡠⠤⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⣉⠭⠛⡏⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│        \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⡤⠤⠒⠊⠉⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n        -2 │⠤⠤⠖⠒⠒⠒⠒⠒⠒⠒⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        \n           └────────────────────────────────────────┘        \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀        \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        \n\njulia> selu(-10f0)\n-1.7580194f0 source"},{"id":733,"pagetitle":"Reference","title":"NNlib.sigmoid","ref":"/nnlib/stable/reference/#NNlib.sigmoid","content":" NNlib.sigmoid  —  Function σ(x) = 1 / (1 + exp(-x)) Classic  sigmoid  activation function. Unicode  σ  can be entered as  \\sigma  then tab, in many editors. The ascii name  sigmoid  is also exported. See also  sigmoid_fast . julia> using UnicodePlots\n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          ┌────────────────────────────────────────┐     \n        1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠒⠒⠋⠉⠉⠉⠉⠉⠉│ σ(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⣀⠔⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⡏⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡔⠋⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠊⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n        0 │⣀⣀⣀⣀⣀⣀⣀⠤⠤⠤⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│     \n          └────────────────────────────────────────┘     \n          ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀     \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀     \n\njulia> sigmoid === σ\ntrue source"},{"id":734,"pagetitle":"Reference","title":"NNlib.softplus","ref":"/nnlib/stable/reference/#NNlib.softplus","content":" NNlib.softplus  —  Function softplus(x) = log(exp(x) + 1) See  \"Deep Sparse Rectifier Neural Networks\" , JMLR 2011. julia> lineplot(softplus, -3, 3, height=7)\n          ┌────────────────────────────────────────┐            \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ softplus(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀│            \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠔⠊⠁⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡠⠤⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠤⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        0 │⣀⣀⣀⣀⣀⣀⣀⡠⠤⠤⠤⠤⠔⠒⠒⠚⠉⠉⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          └────────────────────────────────────────┘            \n          ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀            \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> lineplot!(ans, relu)\n          ┌────────────────────────────────────────┐            \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ softplus(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠│ relu(x)    \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⡴⠞⠋⠁│            \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⡴⠞⠋⠁⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⣀⡠⢤⡲⠝⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀│            \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠤⠒⠊⣉⠥⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        0 │⣀⣀⣀⣀⣀⣀⣀⣠⣤⣤⣤⣤⣔⣒⣒⣚⣉⣉⣁⣀⣇⠴⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n          └────────────────────────────────────────┘            \n          ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀            \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> softplus(16f0)\n16.0f0 source"},{"id":735,"pagetitle":"Reference","title":"NNlib.softshrink","ref":"/nnlib/stable/reference/#NNlib.softshrink","content":" NNlib.softshrink  —  Function softshrink(x, λ=0.5) =\n    (x ≥ λ ? x - λ : (-λ ≥ x ? x + λ : 0)) See  \"Softshrink Activation Function\" . julia> lineplot(softshrink, -2, 2, height=7)\n           ┌────────────────────────────────────────┐              \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀│ softshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡤⠔⠒⠉⠁│              \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⠒⠋⠁⠀⠀⠀⠀⠀⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⡤⠤⠤⠤⠤⠤⠤⡧⠤⠤⠤⠤⠶⠮⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⠀⠀⠀⠀⠀⢀⣀⠤⠖⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⠀⣀⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -2 │⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀              \n\njulia> lineplot!(ans, tanhshrink)\n           ┌────────────────────────────────────────┐              \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀│ softshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⡤⠔⠒⣉⡡│ tanhshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣀⡤⠤⣒⣋⠥⠤⠒⠊⠉⠁⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⣤⣤⣤⣤⡤⠤⠤⠤⠤⠤⠤⡷⠶⠶⠶⠶⠶⠾⠿⠯⠭⠭⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⢀⣀⡠⠤⠖⢒⣋⠭⠗⠒⠉⠁⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⠊⣉⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -2 │⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀\n\njulia> softshrink.((-10f0, 10f0))\n(-9.5f0, 9.5f0) source"},{"id":736,"pagetitle":"Reference","title":"NNlib.softsign","ref":"/nnlib/stable/reference/#NNlib.softsign","content":" NNlib.softsign  —  Function softsign(x) = x / (1 + |x|) See  \"Quadratic Polynomials Learn Better Image Features\"  (2009). julia> lineplot(softsign, -5, 5, height=7)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⠤⠤⠤⠤⠤│ softsign(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⣀⡤⠖⠒⠋⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⡔⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠁⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⠤⠤⠒⠋⠁⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⠒⠒⠒⠒⠒⠊⠉⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> lineplot!(ans, tanh)\n           ┌────────────────────────────────────────┐            \n         1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⢀⡤⠖⠊⠉⠉⠉⣉⣉⣉⣉⣉⠭⠭⠭⠭⠭│ softsign(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⡔⣃⡤⠖⠒⠋⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│ tanh(x)    \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣧⡞⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⡯⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡴⠃⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⠤⠤⠒⢋⠕⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n        -1 │⣒⣒⣒⣒⣒⣊⣉⣉⣉⣉⣁⣀⣀⡠⠤⠒⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│            \n           └────────────────────────────────────────┘            \n           ⠀-5⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀            \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀            \n\njulia> softsign(1f0)\n0.5f0\n\njulia> softsign(100f0)\n0.990099f0 source"},{"id":737,"pagetitle":"Reference","title":"NNlib.swish","ref":"/nnlib/stable/reference/#NNlib.swish","content":" NNlib.swish  —  Function swish(x) = x * σ(x) Self-gated activation function. See  \"Swish: a Self-Gated Activation Function\" . julia> lineplot(swish, -2, 2, height=7)\n           ┌────────────────────────────────────────┐         \n         2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤│ swish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋⠁⠀│         \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀│         \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⢀⣀⡤⠔⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n           │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⣤⣤⡤⡧⠴⠶⠯⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│         \n           │⠉⠑⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠉⠉⠉⠉⠁⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n           └────────────────────────────────────────┘         \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2⠀         \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":738,"pagetitle":"Reference","title":"NNlib.hardswish","ref":"/nnlib/stable/reference/#NNlib.hardswish","content":" NNlib.hardswish  —  Function hardswish(x) = x * hardσ(x) Hard-Swish activation function. See  \"Searching for MobileNetV3\" . julia> lineplot(hardswish, -2, 5, height = 7)\n           ┌────────────────────────────────────────┐             \n         5 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠔⠒⠉│ hardswish(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠔⠒⠉⠁⠀⠀⠀⠀│             \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠖⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n   f(x)    │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⢀⣀⠤⠖⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⣤⣤⣖⣚⣉⣁⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀│             \n        -1 │⠉⠒⠒⠒⠒⠉⠉⠉⠉⠁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n           └────────────────────────────────────────┘             \n           ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀5⠀             \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀             \n\njulia> lineplot(hardswish, -4, 0, height = 7);\n\njulia> lineplot!(ans, swish)\n             ┌────────────────────────────────────────┐             \n           0 │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⢣⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡜│ hardswish(x)\n             │⠒⠒⠢⠤⢄⣀⡀⠀⠀⠀⠀⠱⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠎⠀│ swish(x)    \n             │⠀⠀⠀⠀⠀⠀⠈⠉⠑⠒⠦⢄⣘⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡴⠃⠀⠀│             \n   f(x)      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⡖⠦⢄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⢔⠏⠁⠀⠀⠀│             \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠣⣄⠀⠉⠑⠒⠦⠤⢄⣀⣀⣀⣀⡠⠤⠖⣊⠕⠁⠀⠀⠀⠀⠀│             \n             │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠓⠤⡀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡤⠖⠁⠀⠀⠀⠀⠀⠀⠀│             \n        -0.4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠒⠢⠤⠤⠔⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│             \n             └────────────────────────────────────────┘             \n             ⠀-4⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0⠀             \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀             \n\njulia> hardswish.(-5:5)'\n1×11 adjoint(::Vector{Float64}) with eltype Float64:\n -0.0  -0.0  -0.0  -0.333333  -0.333333  0.0  0.666667  1.66667  3.0  4.0  5.0 source"},{"id":739,"pagetitle":"Reference","title":"NNlib.tanhshrink","ref":"/nnlib/stable/reference/#NNlib.tanhshrink","content":" NNlib.tanhshrink  —  Function tanhshrink(x) = x - tanh(x) See  \"Tanhshrink Activation Function\" . julia> lineplot(tanhshrink, -3, 3, height=7)\n           ┌────────────────────────────────────────┐              \n         3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ tanhshrink(x)\n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡠⠤⠖⠊│              \n           │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⢀⣀⡠⠤⠒⠊⠉⠁⠀⠀⠀⠀│              \n   f(x)    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⣤⡤⠤⠤⠤⠤⠤⠤⡷⠶⠶⠶⠶⠶⠮⠭⠥⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│              \n           │⠀⠀⠀⠀⠀⣀⡠⠴⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           │⡠⠴⠒⠊⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n        -3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│              \n           └────────────────────────────────────────┘              \n           ⠀-3⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀3⠀              \n           ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀              \n\njulia> tanhshrink.((-10f0, 10f0))\n(-9.0f0, 9.0f0) source"},{"id":740,"pagetitle":"Reference","title":"NNlib.trelu","ref":"/nnlib/stable/reference/#NNlib.trelu","content":" NNlib.trelu  —  Function trelu(x, theta=1) = x > theta ? x : 0 Threshold gated rectified linear activation function. See  \"Zero-bias autoencoders and the benefits of co-adapting features\" julia> lineplot(trelu, -2, 4, height=7)\n          ┌────────────────────────────────────────┐         \n        4 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡤⠖⠋│ trelu(x)\n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠖⠋⠁⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠔⠊⠁⠀⠀⠀⠀⠀⠀⠀│         \n   f(x)   │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠴⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⣠⠤⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n        0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣇⣀⣀⣀⣀⣀⣀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│         \n          └────────────────────────────────────────┘         \n          ⠀-2⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀4⠀         \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀x⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀          source"},{"id":741,"pagetitle":"Reference","title":"Attention","ref":"/nnlib/stable/reference/#Attention","content":" Attention"},{"id":742,"pagetitle":"Reference","title":"NNlib.dot_product_attention","ref":"/nnlib/stable/reference/#NNlib.dot_product_attention","content":" NNlib.dot_product_attention  —  Function dot_product_attention(query, key, value, [bias]; [fdrop, mask, nheads]) Multihead dot product attention used in transformer architectures. The input arrays must have the first two dimensions given by the number of features and the sequence length, then an arbitrary number of batch dimensions or none. Returns the attention output array of size  (v_dim, q_len, batch_size...)  and the attention scores of size  (kv_len, q_len, nheads, batch_size...) . See also  dot_product_attention_scores  if you only need the attention scores. Arguments query : Query array of size  (qk_dim, q_len, batch_size...) . key : Key array of size  (qk_dim, kv_len, batch_size...) . value : Value array of size  (v_dim, kv_len, batch_size...) . bias : Either  nothing  or an array broadcastable to size  (kv_len, q_len, nheads, batch_size) .         It will be added to the attention scores before applying the softmax. Default  nothing . fdrop : A dropout function or layer to be applied on the attention scores right after the softmax.          Default  identity  (no dropout). mask : Either  nothing  or a boolean array broadcastable to size  (kv_len, q_len, nheads, batch_size) .         The mask is applied to the attention scores just before the softmax.         See  make_causal_mask  fore creating causal masks. Default  nothing . nheads : Number of heads to split the input arrays into. Default  1 . Examples q, k, v = rand(10, 20, 2), rand(10, 30, 2), rand(20, 30, 2)\ny, α = dot_product_attention(q, k, v) source"},{"id":743,"pagetitle":"Reference","title":"NNlib.dot_product_attention_scores","ref":"/nnlib/stable/reference/#NNlib.dot_product_attention_scores","content":" NNlib.dot_product_attention_scores  —  Function dot_product_attention_scores(query, key, [bias]; [fdrop, mask]) Return the attention scores for the  dot_product_attention . Input arrays must have dimensions  (num_features ÷ nheads, nheads, sequence_length, batch_size) . See  dot_product_attention  for more details. source"},{"id":744,"pagetitle":"Reference","title":"NNlib.make_causal_mask","ref":"/nnlib/stable/reference/#NNlib.make_causal_mask","content":" NNlib.make_causal_mask  —  Function make_causal_mask(x, dims=2) Return a boolean square matrix  m  of the same type as  x  and of side  size(x, dims) . Its elements are set such that  m[i, j] == i ≤ j . Can be used to mask the attention scores in  dot_product_attention . source"},{"id":745,"pagetitle":"Reference","title":"Softmax","ref":"/nnlib/stable/reference/#Softmax","content":" Softmax Flux 's  logitcrossentropy  uses  NNlib.softmax  internally."},{"id":746,"pagetitle":"Reference","title":"NNlib.softmax","ref":"/nnlib/stable/reference/#NNlib.softmax","content":" NNlib.softmax  —  Function softmax(x; dims = 1) Softmax  turns input array  x  into probability distributions that sum to 1 along the dimensions specified by  dims . It is semantically equivalent to the following: softmax(x; dims = 1) = exp.(x) ./ sum(exp.(x), dims = dims) with additional manipulations enhancing numerical stability. For a matrix input  x  it will by default ( dims = 1 ) treat it as a batch of vectors, with each column independent. Keyword  dims = 2  will instead treat rows independently, and so on. See also  logsoftmax . Examples julia> softmax([1, 2, 3])\n3-element Vector{Float64}:\n 0.09003057317038046\n 0.24472847105479764\n 0.6652409557748218\n\njulia> softmax([1 2 3; 2 2 2])  # dims=1\n2×3 Matrix{Float64}:\n 0.268941  0.5  0.731059\n 0.731059  0.5  0.268941\n\njulia> softmax([1 2 3; 2 2 2]; dims=2)\n2×3 Matrix{Float64}:\n 0.0900306  0.244728  0.665241\n 0.333333   0.333333  0.333333 Note that, when used with Flux.jl,  softmax  must not be passed to layers like  Dense  which accept an activation function. The activation is broadcasted over the result, thus applies to individual numbers. But  softmax  always needs to see the whole column. julia> using Flux\n\njulia> x = randn(Float32, 4, 4, 3, 13);\n\njulia> model = Chain(Conv((4, 4), 3 => 8, tanh), Flux.flatten, Dense(8 => 7), softmax);\n\njulia> model(x) |> size\n(7, 13)\n\njulia> Dense(4 => 7, softmax)(x)\nERROR: `softmax(x)` called with a number, but it expects an array.  source"},{"id":747,"pagetitle":"Reference","title":"NNlib.logsoftmax","ref":"/nnlib/stable/reference/#NNlib.logsoftmax","content":" NNlib.logsoftmax  —  Function logsoftmax(x; dims = 1) Computes the log of softmax in a more numerically stable way than directly taking  log.(softmax(xs)) . Commonly used in computing cross entropy loss. It is semantically equivalent to the following: logsoftmax(x; dims = 1) = x .- log.(sum(exp.(x), dims = dims)) See also  softmax . source"},{"id":748,"pagetitle":"Reference","title":"Pooling","ref":"/nnlib/stable/reference/#Pooling","content":" Pooling Flux 's  AdaptiveMaxPool ,  AdaptiveMeanPool ,  GlobalMaxPool ,  GlobalMeanPool ,  MaxPool ,  MeanPool  and  lpnormpool  use  NNlib.PoolDims ,  NNlib.maxpool ,  NNlib.meanpool  and  NNlib.lpnormpool  as their backend."},{"id":749,"pagetitle":"Reference","title":"NNlib.PoolDims","ref":"/nnlib/stable/reference/#NNlib.PoolDims","content":" NNlib.PoolDims  —  Type PoolDims(x_size::NTuple{M}, k::Union{NTuple{L, Int}, Int};\n        stride=k, padding=0, dilation=1)  where {M, L} Dimensions for a \"pooling\" operation that can have an arbitrary input size, kernel size, stride, dilation, and channel count.  Used to dispatch onto efficient implementations at compile-time. source"},{"id":750,"pagetitle":"Reference","title":"NNlib.maxpool","ref":"/nnlib/stable/reference/#NNlib.maxpool","content":" NNlib.maxpool  —  Function maxpool(x, k::NTuple{N, Integer}; pad=0, stride=k) Perform max pool operation with window size  k  on input tensor  x . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always  length(k) == ndim(x) - 2 pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k . source"},{"id":751,"pagetitle":"Reference","title":"NNlib.meanpool","ref":"/nnlib/stable/reference/#NNlib.meanpool","content":" NNlib.meanpool  —  Function meanpool(x, k::NTuple{N, Integer}; pad=0, stride=k) Perform mean pool operation with window size  k  on input tensor  x . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always length(k) == ndim(x) - 2` pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k . source"},{"id":752,"pagetitle":"Reference","title":"NNlib.lpnormpool","ref":"/nnlib/stable/reference/#NNlib.lpnormpool","content":" NNlib.lpnormpool  —  Function lpnormpool(x, p::Real, k::NTuple{N, Integer}; pad=0, stride=k) Perform Lp pool operation with value of the Lp norm  p  and window size  k  on input tensor  x , also known as LPPool in pytorch. This pooling operator from  Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks . Arguments: x  and  k : Expects  ndim(x) ∈ 3:5 , and always length(k) == ndim(x) - 2` p  is restricted to  0 < p < Inf . pad : See  pad_zeros  for details. stride : Either a tuple with the same length as  k , or one integer for all directions. Default is  k . For all elements  x  in a size  k  window, lpnormpool computes  (∑ᵢ xᵢ^p)^(1 / p)  as an element of the output. Thus  lpnormpool(x, 1, k) ./ prod(k) ≈ meanpool(x, k)  and  lpnormpool(x, 2, k).^2 ./ prod(k) ≈ meanpool(x.^2, k) . source"},{"id":753,"pagetitle":"Reference","title":"Padding","ref":"/nnlib/stable/reference/#Padding","content":" Padding"},{"id":754,"pagetitle":"Reference","title":"NNlib.pad_reflect","ref":"/nnlib/stable/reference/#NNlib.pad_reflect","content":" NNlib.pad_reflect  —  Function pad_reflect(x, pad::Tuple; [dims])\npad_reflect(x, pad::Int; [dims]) Pad the array  x  reflecting its values across the border. pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_repeat ,  pad_symmetric ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_reflect(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 5  2  5  8  5  2\n 4  1  4  7  4  1\n 5  2  5  8  5  2\n 6  3  6  9  6  3\n 5  2  5  8  5  2\n 4  1  4  7  4  1 source"},{"id":755,"pagetitle":"Reference","title":"NNlib.pad_symmetric","ref":"/nnlib/stable/reference/#NNlib.pad_symmetric","content":" NNlib.pad_symmetric  —  Function pad_symmetric(x, pad::Tuple; [dims])\npad_symmetric(x, pad::Int; [dims]) Pad the array  x  reflecting its values symmetrically across the border, i.e. the border values of  x  are present in the padding values, in contrast to  pad_reflect . pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_repeat ,  pad_reflect ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_symmetric(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 1  1  4  7  7  4\n 1  1  4  7  7  4\n 2  2  5  8  8  5\n 3  3  6  9  9  6\n 3  3  6  9  9  6\n 2  2  5  8  8  5 source"},{"id":756,"pagetitle":"Reference","title":"NNlib.pad_circular","ref":"/nnlib/stable/reference/#NNlib.pad_circular","content":" NNlib.pad_circular  —  Function pad_circular(x, pad::Tuple; [dims])\npad_circular(x, pad::Int; [dims]) Pad the array  x  \"circularly\" across the border by wrapping around values from the opposite side of  x .  pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  The pad length on either side in any dimension must not exceed the size of  x  in that dimension, i.e.  pad_circular  is not able to create abitrary sized tilings of  x . See also  pad_repeat ,  pad_reflect ,  pad_symmetric , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_circular(r, (1,2,1,2))\n6×6 Matrix{Int64}:\n 9  3  6  9  3  6\n 7  1  4  7  1  4\n 8  2  5  8  2  5\n 9  3  6  9  3  6\n 7  1  4  7  1  4\n 8  2  5  8  2  5 source"},{"id":757,"pagetitle":"Reference","title":"NNlib.pad_repeat","ref":"/nnlib/stable/reference/#NNlib.pad_repeat","content":" NNlib.pad_repeat  —  Function pad_repeat(x, pad::Tuple; [dims])\npad_repeat(x, pad::Int; [dims]) Pad the array  x  repeating the values on the border. pad  can a tuple of integers  (l1, r1, ..., ln, rn)  of some length  2n  that specifies the left and right padding size for each of the dimensions in  dims . If  dims  is not given,  it defaults to the first  n  dimensions. If  pad  is an integer, it is applied on both sides on every dimension in  dims . In this case,  dims   defaults to the first  ndims(x)-2  dimensions  (i.e. excludes the channel and batch dimension).  See also  pad_reflect ,  pad_symmetric ,  pad_circular , and  pad_constant . julia> r = reshape(1:9, 3, 3)\n3×3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_repeat(r, (1,2,3,4))\n6×10 Matrix{Int64}:\n 1  1  1  1  4  7  7  7  7  7\n 1  1  1  1  4  7  7  7  7  7\n 2  2  2  2  5  8  8  8  8  8\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9 source"},{"id":758,"pagetitle":"Reference","title":"NNlib.pad_constant","ref":"/nnlib/stable/reference/#NNlib.pad_constant","content":" NNlib.pad_constant  —  Function pad_constant(x, pad::Tuple, val = 0; [dims = :])\npad_constant(x, pad::Int, val = 0; [dims = :]) Pad the array  x  with the constant value  val . pad  can be a tuple of integers. If it is of some length  2 * length(dims)  that specifies the left and right padding size for each of the dimensions in  dims  as  (l1, r1, ..., ln, rn) .  If supplied with a tuple of length  length(dims)  instead, it applies symmetric padding. If  dims  is not given, it defaults to all dimensions. For integer  pad  input, it is applied on both sides on every dimension in  dims . See also  pad_zeros ,  pad_repeat ,  pad_reflect ,  pad_symmetric , and  pad_circular . julia> r = reshape(1:4, 2, 2)\n2×2 reshape(::UnitRange{Int64}, 2, 2) with eltype Int64:\n 1  3\n 2  4\n\njulia> pad_constant(r, (1, 2, 3, 4), 8)\n5×9 Matrix{Int64}:\n 8  8  8  8  8  8  8  8  8\n 8  8  8  1  3  8  8  8  8\n 8  8  8  2  4  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n\njulia> pad_constant(r, 1, 8)\n4×4 Matrix{Int64}:\n 8  8  8  8\n 8  1  3  8\n 8  2  4  8\n 8  8  8  8\n\njulia> r = reshape(1:27, 3, 3, 3)\n3×3×3 reshape(::UnitRange{Int64}, 3, 3, 3) with eltype Int64:\n[:, :, 1] =\n 1  4  7\n 2  5  8\n 3  6  9\n\n[:, :, 2] =\n 10  13  16\n 11  14  17\n 12  15  18\n\n[:, :, 3] =\n 19  22  25\n 20  23  26\n 21  24  27\n\njulia> pad_constant(r, (2,1), dims = 1) # assymetric padding\n6×3×3 Array{Int64, 3}:\n[:, :, 1] =\n 0  0  0\n 0  0  0\n 1  4  7\n 2  5  8\n 3  6  9\n 0  0  0\n\n[:, :, 2] =\n  0   0   0\n  0   0   0\n 10  13  16\n 11  14  17\n 12  15  18\n  0   0   0\n\n[:, :, 3] =\n  0   0   0\n  0   0   0\n 19  22  25\n 20  23  26\n 21  24  27\n  0   0   0\n\njulia> pad_constant(r, (2,1, 3), dims = (1,2)) # padding must always be either the same length as dims, or double it\nERROR: ArgumentError: Could not parse padding (2, 1, 3) and dims (1, 2)\nStacktrace:\n[...] source"},{"id":759,"pagetitle":"Reference","title":"NNlib.pad_zeros","ref":"/nnlib/stable/reference/#NNlib.pad_zeros","content":" NNlib.pad_zeros  —  Function pad_zeros(x, pad::Tuple; [dims])\npad_zeros(x, pad::Int; [dims]) Pad the array  x  with zeros. Equivalent to  pad_constant  with the constant equal to 0.  source"},{"id":760,"pagetitle":"Reference","title":"Convolution","ref":"/nnlib/stable/reference/#Convolution","content":" Convolution Flux 's  Conv  and  CrossCor  layers use  NNlib.DenseConvDims  and  NNlib.conv  internally. NNlib.conv  supports complex datatypes on CPU and CUDA devices. !!! AMDGPU MIOpen supports only cross-correlation (flipkernel=true).     Therefore for every regular convolution (flipkernel=false)     kernel is flipped before calculation.     For better performance, use cross-correlation (flipkernel=true)     and manually flip the kernel before  NNlib.conv  call.      Flux  handles this automatically, this is only required for direct calls."},{"id":761,"pagetitle":"Reference","title":"NNlib.conv","ref":"/nnlib/stable/reference/#NNlib.conv","content":" NNlib.conv  —  Function conv(x, w; stride = 1, pad = 0, dilation = 1, flipped = false, groups = 1) Apply convolution filter  w  to input  x .  x  and  w  are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively.  x  and  w  may have real or complex element types. source"},{"id":762,"pagetitle":"Reference","title":"NNlib.ConvDims","ref":"/nnlib/stable/reference/#NNlib.ConvDims","content":" NNlib.ConvDims  —  Type ConvDims Type system-level information about convolution dimensions. Critical for things like  im2col!()  to generate efficient code, and helpful to reduce the number of kwargs getting passed around. source"},{"id":763,"pagetitle":"Reference","title":"NNlib.depthwiseconv","ref":"/nnlib/stable/reference/#NNlib.depthwiseconv","content":" NNlib.depthwiseconv  —  Function depthwiseconv(x, w; stride=1, pad=0, dilation=1, flipped=false) Depthwise convolution operation with filter  w  on input  x .  x  and  w  are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively. source"},{"id":764,"pagetitle":"Reference","title":"NNlib.DepthwiseConvDims","ref":"/nnlib/stable/reference/#NNlib.DepthwiseConvDims","content":" NNlib.DepthwiseConvDims  —  Type DepthwiseConvDims Concrete subclass of  ConvDims  for a depthwise convolution.  Differs primarily due to characterization by C in, C mult, rather than C in, C out.  Useful to be separate from DenseConvDims primarily for channel calculation differences. source"},{"id":765,"pagetitle":"Reference","title":"NNlib.DenseConvDims","ref":"/nnlib/stable/reference/#NNlib.DenseConvDims","content":" NNlib.DenseConvDims  —  Type DenseConvDims Concrete subclass of  ConvDims  for a normal, dense, conv2d/conv3d. source"},{"id":766,"pagetitle":"Reference","title":"NNlib.unfold","ref":"/nnlib/stable/reference/#NNlib.unfold","content":" NNlib.unfold  —  Function unfold(x, kernel_size; stride = 1, pad = 0, dilation = 0, flipped = true) Places sliding windows of x into a container tensor of size  (num_windows, window_size, batchsize) . The window size is determined by the  prod(spatial dims of kernel)*input_channels . The number of sliding windows will match those of convolution ( conv ) with the same kernel_size and arguments. Note that by default  conv  flips the spatial dimensions of its kernel (default  flipped=false ), whereas  unfold  does not (default  flipped=true ).  Uses  NNlib.im2col!  as backend.  See also  fold , the adjoint/transpose operator  and a potential inverse of  unfold . Example The below example demonstrates that  unfold  uses the same sliding windows as  conv . In general  batched_mul  +  unfold  should not be used to achieve convolution. julia> x = reshape([100 2 3 40 5 6 700], 7, 1, 1);  # 1D data, 1 channel, batch of 1\n\njulia> w = reshape([1 0 -1], 3, 1, 1);  # 1D conv kernel of length 3\n\njulia> kws = (pad=1, stride=2, flipped=true);  # use same args for conv and unfold\n\njulia> z = NNlib.unfold(x, size(w); kws...) \n4×3×1 Array{Int64, 3}:\n[:, :, 1] =\n  0  100   2\n  2    3  40\n 40    5   6\n  6  700   0\n\njulia> y1 = conv(x, w; kws...)\n4×1×1 Array{Int64, 3}:\n[:, :, 1] =\n  -2\n -38\n  34\n   6\n\njulia> y2 = z ⊠ w  # ⊠ (\\boxtimes) is NNlib.batched_mul\n4×1×1 Array{Int64, 3}:\n[:, :, 1] =\n  -2\n -38\n  34\n   6 source"},{"id":767,"pagetitle":"Reference","title":"NNlib.fold","ref":"/nnlib/stable/reference/#NNlib.fold","content":" NNlib.fold  —  Function fold(y, output_size, kernel_size; stride = 1, pad = 0, dilation = 0, flipped = true) The adjoint/transpose operator of  unfold . It accumulates sliding windows from the output of  unfold  into a container tensor of size  output_size . An inverse to  unfold  may be obtained (in some cases) by using  fold  and accounting for scaling issues  with a divisor (see example). Uses  NNlib.col2im!  as backend.  See also  unfold . Example julia> x = reshape([100 2 3 40 5 6 700], 7, 1, 1);  # 1D data, 1 channel, batch of 1\n\njulia> y = NNlib.unfold(x, (3,1,1))  # sliding window of size 3\n5×3×1 Array{Int64, 3}:\n[:, :, 1] =\n 100   2    3\n   2   3   40\n   3  40    5\n  40   5    6\n   5   6  700\n\njulia> z = NNlib.fold(y, size(x), (3,1,1))  # sum of contributions in y. 100 appears once, 40 three times\n7×1×1 Array{Int64, 3}:\n[:, :, 1] =\n 100\n   4\n   9\n 120\n  15\n  12\n 700\n\njulia> divisor = NNlib.fold(NNlib.unfold(ones(size(x)...), (3,1,1)), size(x), (3,1,1))\n7×1×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.0\n 2.0\n 3.0\n 3.0\n 3.0\n 2.0\n 1.0\n\njulia> z ./ divisor \n7×1×1 Array{Float64, 3}:\n[:, :, 1] =\n 100.0\n   2.0\n   3.0\n  40.0\n   5.0\n   6.0\n 700.0 In general, an inverse to  unfold  does not exist if  divisor  contains zeros. source"},{"id":768,"pagetitle":"Reference","title":"Upsampling","ref":"/nnlib/stable/reference/#Upsampling","content":" Upsampling Flux 's  Upsample  layer uses  NNlib.upsample_nearest ,  NNlib.upsample_bilinear , and  NNlib.upsample_trilinear  as its backend. Additionally,  Flux 's  PixelShuffle  layer uses  NNlib.pixel_shuffle  as its backend."},{"id":769,"pagetitle":"Reference","title":"NNlib.upsample_nearest","ref":"/nnlib/stable/reference/#NNlib.upsample_nearest","content":" NNlib.upsample_nearest  —  Function upsample_nearest(x, scale::NTuple{S,Int})\nupsample_nearest(x; size::NTuple{S,Int}) Upsamples the array  x  by integer multiples along the first  S  dimensions. Subsequent dimensions of  x  are not altered. Either the  scale  factors or the final output  size  can be specified. See also  upsample_bilinear , for two dimensions of an  N=4  array. Example julia> upsample_nearest([1 2 3; 4 5 6], (2, 3))\n4×9 Matrix{Int64}:\n 1  1  1  2  2  2  3  3  3\n 1  1  1  2  2  2  3  3  3\n 4  4  4  5  5  5  6  6  6\n 4  4  4  5  5  5  6  6  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6]; size=(4, 9))  # equivalent\ntrue\n\njulia> upsample_nearest([1 2 3; 4 5 6], (2,))\n4×3 Matrix{Int64}:\n 1  2  3\n 1  2  3\n 4  5  6\n 4  5  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6], size=(4,))\ntrue source Missing docstring. Missing docstring for  ∇upsample_nearest . Check Documenter's build log for details."},{"id":770,"pagetitle":"Reference","title":"NNlib.upsample_linear","ref":"/nnlib/stable/reference/#NNlib.upsample_linear","content":" NNlib.upsample_linear  —  Function upsample_linear(x::AbstractArray{T,3}, scale::Real; align_corners::Bool = true)\nupsample_linear(x::AbstractArray{T,3}; size::Integer, align_corners::Bool = true) Upsamples the first dimension of the array  x  by the upsample provided  scale , using linear interpolation. As an alternative to using  scale , the resulting array  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale*S1, S2, S3) , where  S1, S2, S3 = size(x) . source"},{"id":771,"pagetitle":"Reference","title":"NNlib.∇upsample_linear","ref":"/nnlib/stable/reference/#NNlib.∇upsample_linear","content":" NNlib.∇upsample_linear  —  Function ∇upsample_linear(Δ::AbstractArray{T,3}; size::Integer, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Size of the image upsampled in the first place Outputs dx : Downsampled version of  Δ source"},{"id":772,"pagetitle":"Reference","title":"NNlib.upsample_bilinear","ref":"/nnlib/stable/reference/#NNlib.upsample_bilinear","content":" NNlib.upsample_bilinear  —  Function upsample_bilinear(x::AbstractArray{T,4}, scale::NTuple{2,Real}; align_corners::Bool = true)\nupsample_bilinear(x::AbstractArray{T,4}; size::NTuple{2,Integer}, align_corners::Bool = true) Upsamples the first 2 dimensions of the array  x  by the upsample factors stored in  scale , using bilinear interpolation. As an alternative to using  scale , the resulting image  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale[1]*S1, scale[2]*S2, S3, S4) , where  S1, S2, S3, S4 = size(x) . Examples julia> x = reshape(Float32[1 2 3; 4 5 6], (2,3,1,1))\n2×3×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  2.0  3.0\n 4.0  5.0  6.0\n\njulia> upsample_bilinear(x, (2, 3))\n4×9×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  1.25  1.5  1.75  2.0  2.25  2.5  2.75  3.0\n 2.0  2.25  2.5  2.75  3.0  3.25  3.5  3.75  4.0\n 3.0  3.25  3.5  3.75  4.0  4.25  4.5  4.75  5.0\n 4.0  4.25  4.5  4.75  5.0  5.25  5.5  5.75  6.0\n\njulia> ans == upsample_bilinear(x; size=(4, 9))  # specify ouput size instead\ntrue\n\njulia> upsample_bilinear(x, (2.5, 3.5))  # non-integer scaling factors are allowed\n5×10×1×1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0   1.22222  1.44444  1.66667  1.88889  …  2.33333  2.55556  2.77778  3.0\n 1.75  1.97222  2.19444  2.41667  2.63889     3.08333  3.30556  3.52778  3.75\n 2.5   2.72222  2.94444  3.16667  3.38889     3.83333  4.05556  4.27778  4.5\n 3.25  3.47222  3.69444  3.91667  4.13889     4.58333  4.80556  5.02778  5.25\n 4.0   4.22222  4.44444  4.66667  4.88889     5.33333  5.55556  5.77778  6.0 source"},{"id":773,"pagetitle":"Reference","title":"NNlib.∇upsample_bilinear","ref":"/nnlib/stable/reference/#NNlib.∇upsample_bilinear","content":" NNlib.∇upsample_bilinear  —  Function ∇upsample_bilinear(Δ::AbstractArray{T,4}; size::NTuple{2,Integer}, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Lateral (W,H) size of the image upsampled in the first place Outputs dx : Downsampled version of  Δ source"},{"id":774,"pagetitle":"Reference","title":"NNlib.upsample_trilinear","ref":"/nnlib/stable/reference/#NNlib.upsample_trilinear","content":" NNlib.upsample_trilinear  —  Function upsample_trilinear(x::AbstractArray{T,5}, scale::NTuple{3,Real}; align_corners::Bool = true)\nupsample_trilinear(x::AbstractArray{T,5}; size::NTuple{3,Integer}, align_corners::Bool = true) Upsamples the first 3 dimensions of the array  x  by the upsample factors stored in  scale , using trilinear interpolation. As an alternative to using  scale , the resulting image  size  can be directly specified with a keyword argument. The size of the output is equal to  (scale[1]*S1, scale[2]*S2, scale[3]*S3, S4, S5) , where  S1, S2, S3, S4, S5 = size(x) . Examples upsample_trilinear(x, (2, 3, 4))\nupsample_trilinear(x; size=(4, 9, 11))  # specify ouput size instead\nupsample_trilinear(x, (2.5, 3.5, pi))  # non-integer scaling factors are allowed source"},{"id":775,"pagetitle":"Reference","title":"NNlib.∇upsample_trilinear","ref":"/nnlib/stable/reference/#NNlib.∇upsample_trilinear","content":" NNlib.∇upsample_trilinear  —  Function ∇upsample_trilinear(Δ::AbstractArray{T,5}; size::NTuple{3,Integer}, align_corners::Bool = true) where T Arguments Δ : Incoming gradient array, backpropagated from downstream layers size : Lateral size & depth (W,H,D) of the image upsampled in the first place Outputs dx : Downsampled version of  Δ source"},{"id":776,"pagetitle":"Reference","title":"NNlib.pixel_shuffle","ref":"/nnlib/stable/reference/#NNlib.pixel_shuffle","content":" NNlib.pixel_shuffle  —  Function pixel_shuffle(x, r::Integer) Pixel shuffling operation, upscaling by a factor  r . For 4-arrays representing  N  images, the operation converts input  size(x) == (W, H, r^2*C, N)  to output of size  (r*W, r*H, C, N) . For  D -dimensional data, it expects  ndims(x) == D+2  with channel and batch dimensions, and divides the number of channels by  r^D . Used in super-resolution networks to upsample towards high resolution features. Reference: Shi et. al., \"Real-Time Single Image and Video Super-Resolution ...\", CVPR 2016, https://arxiv.org/abs/1609.05158 Examples julia> x = [10i + j + channel/10 for i in 1:2, j in 1:3, channel in 1:4, batch in 1:1]\n2×3×4×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  12.1  13.1\n 21.1  22.1  23.1\n\n[:, :, 2, 1] =\n 11.2  12.2  13.2\n 21.2  22.2  23.2\n\n[:, :, 3, 1] =\n 11.3  12.3  13.3\n 21.3  22.3  23.3\n\n[:, :, 4, 1] =\n 11.4  12.4  13.4\n 21.4  22.4  23.4\n\njulia> pixel_shuffle(x, 2)  # 4 channels used up as 2x upscaling of image dimensions\n4×6×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  11.3  12.1  12.3  13.1  13.3\n 11.2  11.4  12.2  12.4  13.2  13.4\n 21.1  21.3  22.1  22.3  23.1  23.3\n 21.2  21.4  22.2  22.4  23.2  23.4\n\njulia> y = [i + channel/10 for i in 1:3, channel in 1:6, batch in 1:1]\n3×6×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.2  1.3  1.4  1.5  1.6\n 2.1  2.2  2.3  2.4  2.5  2.6\n 3.1  3.2  3.3  3.4  3.5  3.6\n\njulia> pixel_shuffle(y, 2)  # 1D image, with 6 channels reduced to 3\n6×3×1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.3  1.5\n 1.2  1.4  1.6\n 2.1  2.3  2.5\n 2.2  2.4  2.6\n 3.1  3.3  3.5\n 3.2  3.4  3.6 source"},{"id":777,"pagetitle":"Reference","title":"Batched Operations","ref":"/nnlib/stable/reference/#Batched-Operations","content":" Batched Operations Flux 's  Bilinear  layer uses  NNlib.batched_mul  internally."},{"id":778,"pagetitle":"Reference","title":"NNlib.batched_mul","ref":"/nnlib/stable/reference/#NNlib.batched_mul","content":" NNlib.batched_mul  —  Function batched_mul(A, B) -> C\nA ⊠ B  # \\boxtimes Batched matrix multiplication. Result has  C[:,:,k...] == A[:,:,k...] * B[:,:,k...]  where  k...  represent  any indices in the last dimensions. If  ndims(A) == ndims(B) == 3  and  size(B,3) == 1  then instead  C[:,:,k] == A[:,:,k] * B[:,:,1] , and similarly for  A . To transpose each matrix, apply  batched_transpose  to the array, or  batched_adjoint  for conjugate-transpose: julia> A, B = randn(2,5,17), randn(5,9,17);\n\njulia> A ⊠ B |> size\n(2, 9, 17)\n\njulia> batched_adjoint(A) |> size\n(5, 2, 17)\n\njulia> batched_mul(A, batched_adjoint(randn(9,5,17))) |> size\n(2, 9, 17)\n\njulia> A ⊠ randn(5,9,1) |> size\n(2, 9, 17)\n\njulia> batched_transpose(A) == PermutedDimsArray(A, (2,1,3))\ntrue The equivalent  PermutedDimsArray  may be used in place of  batched_transpose . Other permutations are also handled by BLAS, provided that the batch index  k  is not the first dimension of the underlying array. Thus  PermutedDimsArray(::Array, (1,3,2))  and  PermutedDimsArray(::Array, (3,1,2))  are fine. However,  A = PermutedDimsArray(::Array, (3,2,1))  is not acceptable to BLAS, since the batch dimension is the contiguous one:  stride(A,3) == 1 . This will be copied, as doing so is faster than  batched_mul_generic! . Both this  copy  and  batched_mul_generic!  produce  @debug  messages, and setting for instance  ENV[\"JULIA_DEBUG\"] = NNlib  will display them. source batched_mul(A::Array{T,3}, B::Matrix)\nbatched_mul(A::Matrix, B::Array{T,3})\nA ⊠ B This is always matrix-matrix multiplication, but either  A  or  B  may lack a batch index. When  B  is a matrix, result has  C[:,:,k] == A[:,:,k] * B[:,:]  for all  k . When  A  is a matrix, then  C[:,:,k] == A[:,:] * B[:,:,k] . This can also be done by reshaping and calling  * , for instance  A ⊡ B  using TensorCore.jl, but is implemented here using  batched_gemm  instead of  gemm . julia> randn(16,8,32) ⊠ randn(8,4) |> size\n(16, 4, 32)\n\njulia> randn(16,8,32) ⊠ randn(8,4,1) |> size  # equivalent\n(16, 4, 32)\n\njulia> randn(16,8) ⊠ randn(8,4,32) |> size\n(16, 4, 32) See also  batched_vec  to regard  B  as a batch of vectors,  A[:,:,k] * B[:,k] . source"},{"id":779,"pagetitle":"Reference","title":"NNlib.batched_mul!","ref":"/nnlib/stable/reference/#NNlib.batched_mul!","content":" NNlib.batched_mul!  —  Function batched_mul!(C, A, B) -> C\nbatched_mul!(C, A, B, α=1, β=0) In-place batched matrix multiplication, equivalent to  mul!(C[:,:,k], A[:,:,k], B[:,:,k], α, β)  for all  k . If  size(B,3) == 1  then every batch uses  B[:,:,1]  instead. This will call  batched_gemm!  whenever possible. For real arrays this means that, for  X ∈ [A,B,C] , either  strides(X,1)==1  or  strides(X,2)==1 , the latter may be caused by  batched_transpose  or by for instance  PermutedDimsArray(::Array, (3,1,2)) . Unlike  batched_mul  this will never make a copy. For complex arrays, the wrapper made by  batched_adjoint  must be outermost to be seen. In this case the strided accepted by BLAS are more restricted, if  stride(C,1)==1  then only  stride(AorB::BatchedAdjoint,2) == 1  is accepted. source"},{"id":780,"pagetitle":"Reference","title":"NNlib.batched_adjoint","ref":"/nnlib/stable/reference/#NNlib.batched_adjoint","content":" NNlib.batched_adjoint  —  Function batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A) Equivalent to applying  transpose  or  adjoint  to each matrix  A[:,:,k] . These exist to control how  batched_mul  behaves, as it operates on such matrix slices of an array with  ndims(A)==3 . PermutedDimsArray(A, (2,1,3))  is equivalent to  batched_transpose(A) , and is also understood by  batched_mul  (and more widely supported elsewhere). BatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S} Lazy wrappers analogous to  Transpose  and  Adjoint , returned by  batched_transpose  etc. source"},{"id":781,"pagetitle":"Reference","title":"NNlib.batched_transpose","ref":"/nnlib/stable/reference/#NNlib.batched_transpose","content":" NNlib.batched_transpose  —  Function batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A) Equivalent to applying  transpose  or  adjoint  to each matrix  A[:,:,k] . These exist to control how  batched_mul  behaves, as it operates on such matrix slices of an array with  ndims(A)==3 . PermutedDimsArray(A, (2,1,3))  is equivalent to  batched_transpose(A) , and is also understood by  batched_mul  (and more widely supported elsewhere). BatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S} Lazy wrappers analogous to  Transpose  and  Adjoint , returned by  batched_transpose  etc. source"},{"id":782,"pagetitle":"Reference","title":"NNlib.batched_vec","ref":"/nnlib/stable/reference/#NNlib.batched_vec","content":" NNlib.batched_vec  —  Function batched_vec(A::Array{T,3}, B::Matrix)\nbatched_vec(A::Array{T,3}, b::Vector) Batched matrix-vector multiplication: the result has  C[:,:,k] == A[:,:,k] * B[:,k]  for all  k , or else  C[:,:,k] == A[:,:,k] * b  for  b::Vector . With the same argument types,  batched_mul(A, B)  would regard  B  as a fixed matrix, not a batch of vectors. Both reshape and then call  batched_mul(::Array{T,3}, ::Array{T,3}) . julia> A, B, b = randn(16,8,32), randn(8,32), randn(8);\n\njulia> batched_vec(A,B) |> size\n(16, 32)\n\njulia> batched_vec(A,b) |> size\n(16, 32) source"},{"id":783,"pagetitle":"Reference","title":"Gather and Scatter","ref":"/nnlib/stable/reference/#Gather-and-Scatter","content":" Gather and Scatter Flux 's  Embedding  layer uses  NNlib.gather  as its backend."},{"id":784,"pagetitle":"Reference","title":"NNlib.gather","ref":"/nnlib/stable/reference/#NNlib.gather","content":" NNlib.gather  —  Function NNlib.gather(src, idx) -> dst Reverse operation of  scatter . Gathers data from source  src  and writes it in a destination  dst  according to the index array  idx . For each  k  in  CartesianIndices(idx) , assign values to  dst  according to dst[:, ... , k] .= src[:, ... , idx[k]...] Notice that if  idx  is a vector containing integers and  src  is a matrix, previous expression simplifies to dst[:, k] .= src[:, idx[k]] and  k  will run over  1:length(idx) . The elements of  idx  can be integers or integer tuples and may be repeated. A single  src  column can end up being copied into zero, one, or multiple  dst  columns. See  gather!  for an in-place version. Examples julia> NNlib.gather([1,20,300,4000], [2,4,2])\n3-element Vector{Int64}:\n   20\n 4000\n   20\n\njulia> NNlib.gather([1 2 3; 4 5 6], [1,3,1,3,1])\n2×5 Matrix{Int64}:\n 1  3  1  3  1\n 4  6  4  6  4 source gather(src, IJK...) Convert the tuple of integer vectors  IJK  to a tuple of  CartesianIndex  and call  gather  on it:  gather(src, CartesianIndex.(IJK...)) . Examples julia> src = reshape([1:15;], 3, 5)\n3×5 Matrix{Int64}:\n 1  4  7  10  13\n 2  5  8  11  14\n 3  6  9  12  15\n\njulia> NNlib.gather(src, [1, 2], [2, 4])\n2-element Vector{Int64}:\n  4\n 11 source"},{"id":785,"pagetitle":"Reference","title":"NNlib.gather!","ref":"/nnlib/stable/reference/#NNlib.gather!","content":" NNlib.gather!  —  Function NNlib.gather!(dst, src, idx) Reverse operation of  scatter! . Gathers data from source  src  and writes it in destination  dst  according to the index array  idx . For each  k  in  CartesianIndices(idx) , assign values to  dst  according to dst[:, ... , k] .= src[:, ... , idx[k]...] Notice that if  idx  is a vector containing integers, and both  dst  and  src  are matrices, previous expression simplifies to dst[:, k] .= src[:, idx[k]] and  k  will run over  1:length(idx) . The elements of  idx  can be integers or integer tuples and may be repeated. A single  src  column can end up being copied into zero, one, or multiple  dst  columns. See  gather  for an allocating version. source"},{"id":786,"pagetitle":"Reference","title":"NNlib.scatter","ref":"/nnlib/stable/reference/#NNlib.scatter","content":" NNlib.scatter  —  Function NNlib.scatter(op, src, idx; [init, dstsize]) Scatter operation allocating a destination array  dst  and calling  scatter!(op, dst, src, idx)  on it. If keyword  init  is provided, it is used to initialize the content of  dst . Otherwise, the init values is inferred from the reduction operator  op  for some common operators (e.g.  init = 0  for  op = + ). If  dstsize  is provided, it will be used to define the size of destination array, otherwise it will be inferred by  src  and  idx . See  scatter!  for full details on how  idx  works. Examples julia> NNlib.scatter(+, [10,100,1000], [3,1,2])\n3-element Vector{Int64}:\n  100\n 1000\n   10\n\njulia> NNlib.scatter(+, [1 2 3 4; 5 6 7 8], [2,1,1,5])\n2×5 Matrix{Int64}:\n  5  1  0  0  4\n 13  5  0  0  8\n\njulia> NNlib.scatter(*, [10,200,3000], [1,4,2]; init = 10, dstsize = 6)\n6-element Vector{Int64}:\n   100\n 30000\n    10\n  2000\n    10\n    10 source"},{"id":787,"pagetitle":"Reference","title":"NNlib.scatter!","ref":"/nnlib/stable/reference/#NNlib.scatter!","content":" NNlib.scatter!  —  Function NNlib.scatter!(op, dst, src, idx) Scatter operation, which writes data in  src  into  dst  at locations  idx . A binary reduction operator  op  is applied during the scatter. For each index  k  in  idx , accumulates values in  dst  according to dst[:, ..., idx[k]...] = (op).(dst[:, ..., idx[k]...], src[:, ..., k...]) See also  scatter ,  gather . Arguments op : Operations to be applied on  dst  and  src , e.g.  + ,  - ,  * ,  / ,  max ,  min  and  mean . dst : The destination for  src  to aggregate to. This argument will be mutated. src : The source data for aggregating. idx : The mapping for aggregation from source (index) to destination (value).        The  idx  array can contain either integers or tuples. Examples julia> NNlib.scatter!(+, ones(3), [10,100], [1,3])\n3-element Vector{Float64}:\n  11.0\n   1.0\n 101.0\n\njulia> NNlib.scatter!(*, fill(0.5, 2, 4), [1 10; 100 1000], [3,2])\n2×4 Matrix{Float64}:\n 0.5    5.0   0.5  0.5\n 0.5  500.0  50.0  0.5 source"},{"id":788,"pagetitle":"Reference","title":"Sampling","ref":"/nnlib/stable/reference/#Sampling","content":" Sampling"},{"id":789,"pagetitle":"Reference","title":"NNlib.grid_sample","ref":"/nnlib/stable/reference/#NNlib.grid_sample","content":" NNlib.grid_sample  —  Function grid_sample(input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros) Given  input , compute output by sampling  input  values at pixel locations from  grid . Uses bilinear interpolation to calculate output values. This implementation assumes the extrema ( -1  and  1 ) are considered as referring to the center points of the input’s corner pixels (i.e. align corners is  true ). Arguments input : Input array in  (W_in, H_in, C, N)  shape. grid : Input grid in  (2, W_out, H_out, N)  shape.   Where for each  (W_out, H_out, N)  grid contains  (x, y)    coordinates that specify sampling locations normalized by the  input  shape. Therefore,  x  and  y  should have values in  [-1, 1]  range.   For example,  (x = -1, y = -1)  is the left-top pixel of  input ,   and  (x = 1, y = 1)  is the right-bottom pixel of  input . Out-of-bound values are handled according to the  padding_mode . padding_mode : Out-of-bound padding.    :zeros  to use  0  for out-of-bound grid locations.    :border  to use border values for out-of-bound grid locations.   Default is  :zeros . Returns (W_out, H_out, C, N)  sampled grid from  input . Examples In the example below, grid contains two out-of-bound sampling locations, which are handled differently, depending on the  padding_mode . julia> x = reshape(collect(1.0:4.0), (2, 2, 1, 1))\n2×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 2.0  4.0\n\njulia> grid = Array{Float64}(undef, 2, 3, 2, 1);\n\njulia> grid[:, 1, 1, 1] .= (-3, -1);\n\njulia> grid[:, 2, 1, 1] .= (0, -1);\n\njulia> grid[:, 3, 1, 1] .= (1, -1);\n\njulia> grid[:, 1, 2, 1] .= (-1, 1);\n\njulia> grid[:, 2, 2, 1] .= (0, 1);\n\njulia> grid[:, 3, 2, 1] .= (3, 1);\n\njulia> grid_sample(x, grid; padding_mode=:zeros)\n3×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 0.0  3.0\n 1.5  3.5\n 2.0  0.0\n\njulia> grid_sample(x, grid; padding_mode=:border)\n3×2×1×1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 1.5  3.5\n 2.0  4.0 source"},{"id":790,"pagetitle":"Reference","title":"NNlib.∇grid_sample","ref":"/nnlib/stable/reference/#NNlib.∇grid_sample","content":" NNlib.∇grid_sample  —  Function ∇grid_sample(Δ::AbstractArray{T, 4}, input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros) where T Arguments Δ : Input gradient in  (W_out, H_out, C, N)  shape   (same as output of the primal computation). input : Input from primal computation in  (W_in, H_in, C, N)  shape. grid : Grid from primal computation in  (2, W_out, H_out, N)  shape. padding_mode : Out-of-bound padding.    :zeros  to use  0  for out-of-bound grid locations.    :border  to use border values for out-of-bound grid locations.   Should be the same as in primal computation.   Default is  :zeros . Returns dinput  (same shape as  input ) and  dgrid  (same shape as  grid ) gradients. source"},{"id":791,"pagetitle":"Reference","title":"Losses","ref":"/nnlib/stable/reference/#Losses","content":" Losses"},{"id":792,"pagetitle":"Reference","title":"NNlib.ctc_loss","ref":"/nnlib/stable/reference/#NNlib.ctc_loss","content":" NNlib.ctc_loss  —  Function ctc_loss(ŷ, y) Computes the connectionist temporal classification loss between  ŷ  and  y .  ŷ  must be a classes-by-time matrices, i.e., each row represents a class and each column represents a time step. Additionally, the  logsoftmax  function will be applied to  ŷ , so  ŷ  must be the raw activation values from the neural network and not, for example, the activations after being passed through a  softmax  activation function.  y  must be a 1D array of the labels associated with  ŷ . The blank label is assumed to be the last label category in  ŷ , so it is equivalent to  size(ŷ, 1) . Used for sequence-to-sequence classification problems such as speech recognition and handwriting recognition where the exact time-alignment of the output (e.g., letters) is not needed to solve the problem. See  Graves et al. (2006)  or  Graves (2012)  for mathematical details. source"},{"id":793,"pagetitle":"Reference","title":"Miscellaneous","ref":"/nnlib/stable/reference/#Miscellaneous","content":" Miscellaneous"},{"id":794,"pagetitle":"Reference","title":"NNlib.logsumexp","ref":"/nnlib/stable/reference/#NNlib.logsumexp","content":" NNlib.logsumexp  —  Function logsumexp(x; dims = :) Computes  log.(sum(exp.(x); dims))  in a numerically stable way. Without  dims  keyword this returns a scalar. See also  logsoftmax . source"},{"id":795,"pagetitle":"Reference","title":"NNlib.glu","ref":"/nnlib/stable/reference/#NNlib.glu","content":" NNlib.glu  —  Function glu(x, dim = 1) The gated linear unit from the  \"Language Modeling with Gated Convolutional Networks\"  paper. Calculates  a .* sigmoid(b) , where  x  is split in half along given dimension  dim  to form  a  and  b . source"},{"id":796,"pagetitle":"Reference","title":"NNlib.within_gradient","ref":"/nnlib/stable/reference/#NNlib.within_gradient","content":" NNlib.within_gradient  —  Function within_gradient(x) --> Bool Returns  false  except when used inside a  gradient  call, when it returns  true . Useful for Flux regularisation layers which behave differently during training and inference. This should work with any ChainRules-based differentiation package, in which case  x  is ignored. But Tracker.jl overloads  with_gradient(x::TrackedArray) , thus for widest use you should pass it an array whose gradient is of interest. There is also an overload for ForwardDiff.jl's  Dual  types (and arrays of them). Examples julia> using ForwardDiff, Zygote, NNlib\n\njulia> f_good(x) = if NNlib.within_gradient(x)\n                     @show 10x\n                   else\n                     x\n                   end;\n\njulia> Zygote.withgradient(f_good, 1.0)\n10x = 10.0\n(val = 10.0, grad = (10.0,))\n\njulia> ForwardDiff.derivative(f_good, 1.0)\n10x = Dual{ForwardDiff.Tag{typeof(f_good), Float64}}(10.0,10.0)\n10.0\n\njulia> f_bad(x, y) = if any(NNlib.within_gradient, (x, y))\n                       @show x * y\n                     else\n                       x / y\n                     end;\n\njulia> Zygote.withgradient(f_bad, 2.0, 3.0)\n(val = 0.6666666666666666, grad = (0.3333333333333333, -0.2222222222222222))\n\njulia> ForwardDiff.derivative(x -> f_bad(x, 3.0), 2.0)\nx * y = Dual{ForwardDiff.Tag{var\"#9#10\", Float64}}(6.0,3.0)\n3.0 What goes wrong in  f_bad  is that Zygote knows  any  to be non-differentiable, and thus completely ignores its contents. This is not a perfect mechanism, and the only style recommended is precisely that of  f_good  above. source"},{"id":797,"pagetitle":"Reference","title":"NNlib.bias_act!","ref":"/nnlib/stable/reference/#NNlib.bias_act!","content":" NNlib.bias_act!  —  Function bias_act!(σ, x, b) This is equivalent to  x .= σ.(x .+ b) , also replacing  sigmoid  &  tanh  with  sigmoid_fast  &  tanh_fast . It will only overwrite  x  when  x isa StridedArray{<:AbstractFloat} . When used within a gradient, it will overwrite only when  σ  has a method of  derivatives_given_output  which does not need the input at all. Such methods are defined by e.g.  @scalar_rule relu(x) Ω > 0  where the derivative contains only  Ω  (the output) not  x . Warning This is not safe to use if  x  is still needed for the gradient of some other function. Incorrect use will give silently wrong answers. It is intended mainly for Flux layers, in which the previous operation is known to be safe, e.g.  bias_act!(σ, weight * input, bias)  for a  Dense  layer. source"},{"id":802,"pagetitle":"Home","title":"MLUtils.jl","ref":"/mlutils/stable/#MLUtils.jl","content":" MLUtils.jl MLUtils.jl  defines interfaces and implements common utilities for Machine Learning pipelines."},{"id":803,"pagetitle":"Home","title":"Features","ref":"/mlutils/stable/#Features","content":" Features An extensible dataset interface  ( numobs  and  getobs ). Data iteration and dataloaders ( eachobs  and  DataLoader ). Lazy data views ( obsview ).  Resampling procedures ( undersample  and  oversample ). Train/test splits ( splitobs )  Data partitioning and aggregation tools ( batch ,  unbatch ,  chunk ,  group_counts ,  group_indices ). Folds for cross-validation ( kfolds ,  leavepout ). Datasets lazy tranformations ( mapobs ,  filterobs ,  groupobs ,  joinobs ,  shuffleobs ). Toy datasets for demonstration purpose.  Other data handling utilities ( flatten ,  normalise ,  unsqueeze ,  stack ,  unstack )."},{"id":804,"pagetitle":"Home","title":"Examples","ref":"/mlutils/stable/#Examples","content":" Examples Let us take a look at a hello world example to get a feeling for  how to use this package in a typical ML scenario.  using MLUtils\n\n# X is a matrix of floats\n# Y is a vector of strings\nX, Y = load_iris()\n\n# The iris dataset is ordered according to their labels,\n# which means that we should shuffle the dataset before\n# partitioning it into training- and test-set.\nXs, Ys = shuffleobs((X, Y))\n\n# We leave out 15 % of the data for testing\ncv_data, test_data = splitobs((Xs, Ys); at=0.85)\n\n# Next we partition the data using a 10-fold scheme.\nfor (train_data, val_data) in kfolds(cv_data; k=10)\n\n    # We apply a lazy transform for data augmentation\n    train_data = mapobs(xy -> (xy[1] .+ 0.1 .* randn.(), xy[2]),  train_data)\n\n    for epoch = 1:10\n        # Iterate over the data using mini-batches of 5 observations each\n        for (x, y) in eachobs(train_data, batchsize=5)\n            # ... train supervised model on minibatches here\n        end\n    end\nend In the above code snippet, the inner loop for  eachobs  is the only place where data other than indices is actually being copied. In fact, while  x  and  y  are materialized arrays,  all the rest are data views. "},{"id":805,"pagetitle":"Home","title":"Related Packages","ref":"/mlutils/stable/#Related-Packages","content":" Related Packages MLUtils.jl  brings together functionalities previously found in  LearnBase.jl  ,  MLDataPattern.jl  and  MLLabelUtils.jl . These packages are now discontinued.  Other features were ported from the deep learning library  Flux.jl , as they are of general use.  MLJ.jl  is a more complete package for managing the whole machine learning pipeline if you are looking for a sklearn replacement."},{"id":808,"pagetitle":"API","title":"API Reference","ref":"/mlutils/stable/api/#API-Reference","content":" API Reference"},{"id":809,"pagetitle":"API","title":"Index","ref":"/mlutils/stable/api/#Index","content":" Index MLUtils.BatchView MLUtils.DataLoader MLUtils.ObsView MLUtils.batch MLUtils.batchseq MLUtils.batchsize MLUtils.chunk MLUtils.eachobs MLUtils.fill_like MLUtils.filterobs MLUtils.flatten MLUtils.getobs MLUtils.getobs! MLUtils.group_counts MLUtils.group_indices MLUtils.groupobs MLUtils.joinobs MLUtils.kfolds MLUtils.leavepout MLUtils.mapobs MLUtils.normalise MLUtils.numobs MLUtils.obsview MLUtils.ones_like MLUtils.oversample MLUtils.randobs MLUtils.rpad_constant MLUtils.shuffleobs MLUtils.splitobs MLUtils.unbatch MLUtils.undersample MLUtils.unsqueeze MLUtils.unstack MLUtils.zeros_like"},{"id":810,"pagetitle":"API","title":"Docs","ref":"/mlutils/stable/api/#Docs","content":" Docs"},{"id":811,"pagetitle":"API","title":"MLUtils.batch","ref":"/mlutils/stable/api/#MLUtils.batch","content":" MLUtils.batch  —  Function batch(xs) Batch the arrays in  xs  into a single array with  an extra dimension. If the elements of  xs  are tuples, named tuples, or dicts,  the output will be of the same type.  See also  unbatch . Examples julia> batch([[1,2,3], \n              [4,5,6]])\n3×2 Matrix{Int64}:\n 1  4\n 2  5\n 3  6\n\njulia> batch([(a=[1,2], b=[3,4])\n               (a=[5,6], b=[7,8])]) \n(a = [1 5; 2 6], b = [3 7; 4 8]) source"},{"id":812,"pagetitle":"API","title":"MLUtils.batchsize","ref":"/mlutils/stable/api/#MLUtils.batchsize","content":" MLUtils.batchsize  —  Function batchsize(data) -> Int Return the fixed size of each batch in  data . source"},{"id":813,"pagetitle":"API","title":"MLUtils.batchseq","ref":"/mlutils/stable/api/#MLUtils.batchseq","content":" MLUtils.batchseq  —  Function batchseq(seqs, val = 0) Take a list of  N  sequences, and turn them into a single sequence where each item is a batch of  N . Short sequences will be padded by  val . Examples julia> batchseq([[1, 2, 3], [4, 5]], 0)\n3-element Vector{Vector{Int64}}:\n [1, 4]\n [2, 5]\n [3, 0] source"},{"id":814,"pagetitle":"API","title":"MLUtils.BatchView","ref":"/mlutils/stable/api/#MLUtils.BatchView","content":" MLUtils.BatchView  —  Type BatchView(data, batchsize; partial=true, collate=nothing)\nBatchView(data; batchsize=1, partial=true, collate=nothing) Create a view of the given  data  that represents it as a vector of batches. Each batch will contain an equal amount of observations in them. The batch-size can be specified using the  parameter  batchsize . In the case that the size of the dataset is not dividable by the specified  batchsize , the remaining observations will be ignored if  partial=false . If   partial=true  instead the last batch-size can be slightly smaller. Note that any data access is delayed until  getindex  is called. If used as an iterator, the object will iterate over the dataset once, effectively denoting an epoch. For  BatchView  to work on some data structure, the type of the given variable  data  must implement the data container interface. See  ObsView  for more info. Arguments data  : The object describing the dataset. Can be of any   type as long as it implements  getobs  and    numobs  (see Details for more information). batchsize  : The batch-size of each batch.   It is the number of observations that each batch must contain   (except possibly for the last one). partial  : If  partial=false  and the number of observations is   not divisible by the batch-size, then the last mini-batch is dropped. collate : Batching behavior. If  nothing  (default), a batch   is  getobs(data, indices) . If  false , each batch is    [getobs(data, i) for i in indices] . When  true , applies  batch    to the vector of observations in a batch, recursively collating   arrays in the last dimensions. See  batch  for more information   and examples. Examples using MLUtils\nX, Y = MLUtils.load_iris()\n\nA = BatchView(X, batchsize=30)\n@assert typeof(A) <: BatchView <: AbstractVector\n@assert eltype(A) <: SubArray{Float64,2}\n@assert length(A) == 5 # Iris has 150 observations\n@assert size(A[1]) == (4,30) # Iris has 4 features\n\n# 5 batches of size 30 observations\nfor x in BatchView(X, batchsize=30)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert numobs(x) === 30\nend\n\n# 7 batches of size 20 observations\n# Note that the iris dataset has 150 observations,\n# which means that with a batchsize of 20, the last\n# 10 observations will be ignored\nfor (x, y) in BatchView((X, Y), batchsize=20, partial=false)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\n    @assert numobs(x) == numobs(y) == 20\nend\n\n# collate tuple observations\nfor (x, y) in BatchView((rand(10, 3), [\"a\", \"b\", \"c\"]), batchsize=2, collate=true, partial=false)\n    @assert size(x) == (10, 2)\n    @assert size(y) == (2,)\nend\n\n\n# randomly assign observations to one and only one batch.\nfor (x, y) in BatchView(shuffleobs((X, Y)), batchsize=20)\n    @assert typeof(x) <: SubArray{Float64,2}\n    @assert typeof(y) <: SubArray{String,1}\nend source"},{"id":815,"pagetitle":"API","title":"MLUtils.chunk","ref":"/mlutils/stable/api/#MLUtils.chunk","content":" MLUtils.chunk  —  Function chunk(x, n; [dims])\nchunk(x; [size, dims]) Split  x  into  n  parts or alternatively, if  size  is an integer, into equal chunks of size  size .  The parts contain the same number of elements except possibly for the last one that can be smaller. In case  size  is a collection of integers instead, the elements of  x  are split into chunks of the given sizes. If  x  is an array,  dims  can be used to specify along which dimension to  split (defaults to the last dimension). Examples julia> chunk(1:10, 3)\n3-element Vector{UnitRange{Int64}}:\n 1:4\n 5:8\n 9:10\n\njulia> chunk(1:10; size = 2)\n5-element Vector{UnitRange{Int64}}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10\n\njulia> x = reshape(collect(1:20), (5, 4))\n5×4 Matrix{Int64}:\n 1   6  11  16\n 2   7  12  17\n 3   8  13  18\n 4   9  14  19\n 5  10  15  20\n\njulia> xs = chunk(x, 2, dims=1)\n2-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}}, false}}:\n [1 6 11 16; 2 7 12 17; 3 8 13 18]\n [4 9 14 19; 5 10 15 20]\n\njulia> xs[1]\n3×4 view(::Matrix{Int64}, 1:3, :) with eltype Int64:\n 1  6  11  16\n 2  7  12  17\n 3  8  13  18\n\njulia> xes = chunk(x; size = 2, dims = 2)\n2-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, UnitRange{Int64}}, true}}:\n [1 6; 2 7; … ; 4 9; 5 10]\n [11 16; 12 17; … ; 14 19; 15 20]\n\njulia> xes[2]\n5×2 view(::Matrix{Int64}, :, 3:4) with eltype Int64:\n 11  16\n 12  17\n 13  18\n 14  19\n 15  20\n\njulia> chunk(1:6; size = [2, 4])\n2-element Vector{UnitRange{Int64}}:\n 1:2\n 3:6 source chunk(x, partition_idxs; [npartitions, dims]) Partition the array  x  along the dimension  dims  according to the indexes  in  partition_idxs . partition_idxs  must be sorted and contain only positive integers  between 1 and the number of partitions.  If the number of partition  npartitions  is not provided,  it is inferred from  partition_idxs . If  dims  is not provided, it defaults to the last dimension. See also  unbatch . Examples julia> x = reshape([1:10;], 2, 5)\n2×5 Matrix{Int64}:\n 1  3  5  7   9\n 2  4  6  8  10\n\njulia> chunk(x, [1, 2, 2, 3, 3])\n3-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, UnitRange{Int64}}, true}}:\n [1; 2;;]\n [3 5; 4 6]\n [7 9; 8 10] source"},{"id":816,"pagetitle":"API","title":"MLUtils.DataLoader","ref":"/mlutils/stable/api/#MLUtils.DataLoader","content":" MLUtils.DataLoader  —  Type DataLoader(data; [batchsize, buffer, collate, parallel, partial, rng, shuffle]) An object that iterates over mini-batches of  data , each mini-batch containing  batchsize  observations (except possibly the last one). Takes as input a single data array, a tuple (or a named tuple) of arrays, or in general any  data  object that implements the  numobs  and  getobs  methods. The last dimension in each array is the observation dimension, i.e. the one divided into mini-batches. The original data is preserved in the  data  field of the DataLoader. Arguments data : The data to be iterated over. The data type has to be supported by  numobs  and  getobs . batchsize : If less than 0, iterates over individual observations. Otherwise, each iteration (except possibly the last) yields a mini-batch containing  batchsize  observations. Default  1 . buffer : If  buffer=true  and supported by the type of  data , a buffer will be allocated and reused for memory efficiency. You can also pass a preallocated object to  buffer . Default  false . collate : Batching behavior. If  nothing  (default), a batch is  getobs(data, indices) . If  false , each batch is   [getobs(data, i) for i in indices] . When  true , applies  batch  to the vector of observations in a batch,   recursively collating arrays in the last dimensions. See  batch  for more information and examples. parallel : Whether to use load data in parallel using worker threads. Greatly   speeds up data loading by factor of available threads. Requires starting   Julia with multiple threads. Check  Threads.nthreads()  to see the number of   available threads.  Passing  parallel = true  breaks ordering guarantees .   Default  false . partial : This argument is used only when  batchsize > 0 . If  partial=false  and the number of observations is not divisible by the batchsize, then the last mini-batch is dropped. Default  true . rng : A random number generator. Default  Random.GLOBAL_RNG . shuffle : Whether to shuffle the observations before iterating. Unlike   wrapping the data container with  shuffleobs(data) ,  shuffle=true  ensures   that the observations are shuffled anew every time you start iterating over    eachobs . Default  false . Examples julia> Xtrain = rand(10, 100);\n\njulia> array_loader = DataLoader(Xtrain, batchsize=2);\n\njulia> for x in array_loader\n         @assert size(x) == (10, 2)\n         # do something with x, 50 times\n       end\n\njulia> array_loader.data === Xtrain\ntrue\n\njulia> tuple_loader = DataLoader((Xtrain,), batchsize=2);  # similar, but yielding 1-element tuples\n\njulia> for x in tuple_loader\n         @assert x isa Tuple{Matrix}\n         @assert size(x[1]) == (10, 2)\n       end\n\njulia> Ytrain = rand('a':'z', 100);  # now make a DataLoader yielding 2-element named tuples\n\njulia> train_loader = DataLoader((data=Xtrain, label=Ytrain), batchsize=5, shuffle=true);\n\njulia> for epoch in 1:100\n         for (x, y) in train_loader  # access via tuple destructuring\n           @assert size(x) == (10, 5)\n           @assert size(y) == (5,)\n           # loss += f(x, y) # etc, runs 100 * 20 times\n         end\n       end\n\njulia> first(train_loader).label isa Vector{Char}  # access via property name\ntrue\n\njulia> first(train_loader).label == Ytrain[1:5]  # because of shuffle=true\nfalse\n\njulia> foreach(println∘summary, DataLoader(rand(Int8, 10, 64), batchsize=30))  # partial=false would omit last\n10×30 Matrix{Int8}\n10×30 Matrix{Int8}\n10×4 Matrix{Int8} source"},{"id":817,"pagetitle":"API","title":"MLUtils.eachobs","ref":"/mlutils/stable/api/#MLUtils.eachobs","content":" MLUtils.eachobs  —  Function eachobs(data; kws...) Return an iterator over  data . Supports the same arguments as  DataLoader . The  batchsize  default is  -1  here while it is  1  for  DataLoader . Examples X = rand(4,100)\n\nfor x in eachobs(X)\n    # loop entered 100 times\n    @assert typeof(x) <: Vector{Float64}\n    @assert size(x) == (4,)\nend\n\n# mini-batch iterations\nfor x in eachobs(X, batchsize=10)\n    # loop entered 10 times\n    @assert typeof(x) <: Matrix{Float64}\n    @assert size(x) == (4,10)\nend\n\n# support for tuples, named tuples, dicts\nfor (x, y) in eachobs((X, Y))\n    # ...\nend source"},{"id":818,"pagetitle":"API","title":"MLUtils.fill_like","ref":"/mlutils/stable/api/#MLUtils.fill_like","content":" MLUtils.fill_like  —  Function fill_like(x, val, [element_type=eltype(x)], [dims=size(x)])) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to  val .  The third and fourth arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. See also  zeros_like  and  ones_like . Examples julia> x = rand(Float32, 2)\n2-element Vector{Float32}:\n 0.16087806\n 0.89916044\n\njulia> fill_like(x, 1.7, (3, 3))\n3×3 Matrix{Float32}:\n 1.7  1.7  1.7\n 1.7  1.7  1.7\n 1.7  1.7  1.7\n\njulia> using CUDA\n\njulia> x = CUDA.rand(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 0.803167  0.476101\n 0.303041  0.317581\n\njulia> fill_like(x, 1.7, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n 1.7  1.7\n 1.7  1.7 source"},{"id":819,"pagetitle":"API","title":"MLUtils.filterobs","ref":"/mlutils/stable/api/#MLUtils.filterobs","content":" MLUtils.filterobs  —  Function filterobs(f, data) Return a subset of data container  data  including all indices  i  for which  f(getobs(data, i)) === true . data = 1:10\nnumobs(data) == 10\nfdata = filterobs(>(5), data)\nnumobs(fdata) == 5 source"},{"id":820,"pagetitle":"API","title":"MLUtils.flatten","ref":"/mlutils/stable/api/#MLUtils.flatten","content":" MLUtils.flatten  —  Function flatten(x::AbstractArray) Reshape arbitrarly-shaped input into a matrix-shaped output, preserving the size of the last dimension. See also  unsqueeze . Examples julia> rand(3,4,5) |> flatten |> size\n(12, 5) source"},{"id":821,"pagetitle":"API","title":"MLUtils.getobs","ref":"/mlutils/stable/api/#MLUtils.getobs","content":" MLUtils.getobs  —  Function getobs(data, [idx]) Return the observations corresponding to the observation index  idx . Note that  idx  can be any type as long as  data  has defined  getobs  for that type. If  idx  is not provided, then materialize all observations in  data . If  data  does not have  getobs  defined, then in the case of  Tables.table(data) == true  returns the row(s) in position  idx , otherwise returns  data[idx] . Authors of custom data containers should implement  Base.getindex  for their type instead of  getobs .  getobs  should only be implemented for types where there is a difference between  getobs  and  Base.getindex  (such as multi-dimensional arrays). The returned observation(s) should be in the form intended to be passed as-is to some learning algorithm. There is no strict interface requirement on how this \"actual data\" must look like. Every author behind some custom data container can make this decision themselves. The output should be consistent when  idx  is a scalar vs vector. getobs  supports by default nested combinations of array, tuple, named tuples, and dictionaries.  See also  getobs!  and  numobs . Examples # named tuples \nx = (a = [1, 2, 3], b = rand(6, 3))\n\ngetobs(x, 2) == (a = 2, b = x.b[:, 2])\ngetobs(x, [1, 3]) == (a = [1, 3], b = x.b[:, [1, 3]])\n\n\n# dictionaries\nx = Dict(:a => [1, 2, 3], :b => rand(6, 3))\n\ngetobs(x, 2) == Dict(:a => 2, :b => x[:b][:, 2])\ngetobs(x, [1, 3]) == Dict(:a => [1, 3], :b => x[:b][:, [1, 3]]) source"},{"id":822,"pagetitle":"API","title":"MLUtils.getobs!","ref":"/mlutils/stable/api/#MLUtils.getobs!","content":" MLUtils.getobs!  —  Function getobs!(buffer, data, idx) Inplace version of  getobs(data, idx) . If this method is defined for the type of  data , then  buffer  should be used to store the result, instead of allocating a dedicated object. Implementing this function is optional. In the case no such method is provided for the type of  data , then  buffer  will be  ignored  and the result of  getobs  returned. This could be because the type of  data  may not lend itself to the concept of  copy! . Thus, supporting a custom  getobs!  is optional and not required. See also  getobs  and  numobs .  source"},{"id":823,"pagetitle":"API","title":"MLUtils.joinobs","ref":"/mlutils/stable/api/#MLUtils.joinobs","content":" MLUtils.joinobs  —  Function joinobs(datas...) Concatenate data containers  datas . data1, data2 = 1:10, 11:20\njdata = joinumobs(data1, data2)\ngetobs(jdata, 15) == 15 source"},{"id":824,"pagetitle":"API","title":"MLUtils.group_counts","ref":"/mlutils/stable/api/#MLUtils.group_counts","content":" MLUtils.group_counts  —  Function group_counts(x) Count the number of times that each element of  x  appears. See also  group_indices Examples julia> group_counts(['a', 'b', 'b'])\nDict{Char, Int64} with 2 entries:\n  'a' => 1\n  'b' => 2 source"},{"id":825,"pagetitle":"API","title":"MLUtils.group_indices","ref":"/mlutils/stable/api/#MLUtils.group_indices","content":" MLUtils.group_indices  —  Function group_indices(x) -> Dict Computes the indices of elements in the vector  x  for each distinct value contained.  This information is useful for resampling strategies, such as stratified sampling. See also  group_counts . Examples julia> x = [:yes, :no, :maybe, :yes];\n\njulia> group_indices(x)\nDict{Symbol, Vector{Int64}} with 3 entries:\n  :yes   => [1, 4]\n  :maybe => [3]\n  :no    => [2] source"},{"id":826,"pagetitle":"API","title":"MLUtils.groupobs","ref":"/mlutils/stable/api/#MLUtils.groupobs","content":" MLUtils.groupobs  —  Function groupobs(f, data) Split data container data  data  into different data containers, grouping observations by  f(obs) . data = -10:10\ndatas = groupobs(>(0), data)\nlength(datas) == 2 source"},{"id":827,"pagetitle":"API","title":"MLUtils.kfolds","ref":"/mlutils/stable/api/#MLUtils.kfolds","content":" MLUtils.kfolds  —  Function kfolds(n::Integer, k = 5) -> Tuple Compute the train/validation assignments for  k  repartitions of  n  observations, and return them in the form of two vectors. The first vector contains the index-vectors for the training subsets, and the second vector the index-vectors for the validation subsets respectively. A general rule of thumb is to use either  k = 5  or  k = 10 . The following code snippet generates the indices assignments for  k = 5 julia> train_idx, val_idx = kfolds(10, 5); Each observation is assigned to the validation subset once (and only once). Thus, a union over all validation index-vectors reproduces the full range  1:n . Note that there is no random assignment of observations to subsets, which means that adjacent observations are likely to be part of the same validation subset. julia> train_idx\n5-element Array{Array{Int64,1},1}:\n [3,4,5,6,7,8,9,10]\n [1,2,5,6,7,8,9,10]\n [1,2,3,4,7,8,9,10]\n [1,2,3,4,5,6,9,10]\n [1,2,3,4,5,6,7,8]\n\njulia> val_idx\n5-element Array{UnitRange{Int64},1}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10 source kfolds(data, [k = 5]) Repartition a  data  container  k  times using a  k  folds strategy and return the sequence of folds as a lazy iterator.  Only data subsets are created, which means that no actual data is copied until  getobs  is invoked. Conceptually, a k-folds repartitioning strategy divides the given  data  into  k  roughly equal-sized parts. Each part will serve as validation set once, while the remaining parts are used for training. This results in  k  different partitions of  data . In the case that the size of the dataset is not dividable by the specified  k , the remaining observations will be evenly distributed among the parts. for (x_train, x_val) in kfolds(X, k=10)\n    # code called 10 times\n    # nobs(x_val) may differ up to ±1 over iterations\nend Multiple variables are supported (e.g. for labeled data) for ((x_train, y_train), val) in kfolds((X, Y), k=10)\n    # ...\nend By default the folds are created using static splits. Use  shuffleobs  to randomly assign observations to the folds. for (x_train, x_val) in kfolds(shuffleobs(X), k = 10)\n    # ...\nend See  leavepout  for a related function. source"},{"id":828,"pagetitle":"API","title":"MLUtils.leavepout","ref":"/mlutils/stable/api/#MLUtils.leavepout","content":" MLUtils.leavepout  —  Function leavepout(n::Integer, [size = 1]) -> Tuple Compute the train/validation assignments for  k ≈ n/size  repartitions of  n  observations, and return them in the form of two vectors. The first vector contains the index-vectors for the training subsets, and the second vector the index-vectors for the validation subsets respectively. Each validation subset will have either  size  or  size+1  observations assigned to it. The following code snippet generates the index-vectors for  size = 2 . julia> train_idx, val_idx = leavepout(10, 2); Each observation is assigned to the validation subset once (and only once). Thus, a union over all validation index-vectors reproduces the full range  1:n . Note that there is no random assignment of observations to subsets, which means that adjacent observations are likely to be part of the same validation subset. julia> train_idx\n5-element Array{Array{Int64,1},1}:\n [3,4,5,6,7,8,9,10]\n [1,2,5,6,7,8,9,10]\n [1,2,3,4,7,8,9,10]\n [1,2,3,4,5,6,9,10]\n [1,2,3,4,5,6,7,8]\n\njulia> val_idx\n5-element Array{UnitRange{Int64},1}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10 source leavepout(data, p = 1) Repartition a  data  container using a k-fold strategy, where  k  is chosen in such a way, that each validation subset of the resulting folds contains roughly  p  observations. Defaults to  p = 1 , which is also known as \"leave-one-out\" partitioning. The resulting sequence of folds is returned as a lazy iterator. Only data subsets are created. That means no actual data is copied until  getobs  is invoked. for (train, val) in leavepout(X, p=2)\n    # if nobs(X) is dividable by 2,\n    # then numobs(val) will be 2 for each iteraton,\n    # otherwise it may be 3 for the first few iterations.\nend See kfolds  for a related function. source"},{"id":829,"pagetitle":"API","title":"MLUtils.mapobs","ref":"/mlutils/stable/api/#MLUtils.mapobs","content":" MLUtils.mapobs  —  Function mapobs(f, data; batched=:auto) Lazily map  f  over the observations in a data container  data . Returns a new data container  mdata  that can be indexed and has a length. Indexing triggers the transformation  f . The batched keyword argument controls the behavior of  mdata[idx]  and  mdata[idxs]   where  idx  is an integer and  idxs  is a vector of integers: batched=:auto  (default). Let  f  handle the two cases.   Calls  f(getobs(data, idx))  and  f(getobs(data, idxs)) . batched=:never . The function  f  is always called on a single observation.   Calls  f(getobs(data, idx))  and  [f(getobs(data, idx)) for idx in idxs] . batched=:always . The function  f  is always called on a batch of observations.   Calls  getobs(f(getobs(data, [idx])), 1)  and  f(getobs(data, idxs)) . Examples julia> data = (a=[1,2,3], b=[1,2,3]);\n\njulia> mdata = mapobs(data) do x\n         (c = x.a .+ x.b,  d = x.a .- x.b)\n       end\nmapobs(#25, (a = [1, 2, 3], b = [1, 2, 3]); batched=:auto))\n\njulia> mdata[1]\n(c = 2, d = 0)\n\njulia> mdata[1:2]\n(c = [2, 4], d = [0, 0]) source mapobs(fs, data) Lazily map each function in tuple  fs  over the observations in data container  data . Returns a tuple of transformed data containers. source mapobs(namedfs::NamedTuple, data) Map a  NamedTuple  of functions over  data , turning it into a data container of  NamedTuple s. Field syntax can be used to select a column of the resulting data container. data = 1:10\nnameddata = mapobs((x = sqrt, y = log), data)\ngetobs(nameddata, 10) == (x = sqrt(10), y = log(10))\ngetobs(nameddata.x, 10) == sqrt(10) source"},{"id":830,"pagetitle":"API","title":"MLUtils.numobs","ref":"/mlutils/stable/api/#MLUtils.numobs","content":" MLUtils.numobs  —  Function numobs(data) Return the total number of observations contained in  data . If  data  does not have  numobs  defined,  then in the case of  Tables.table(data) == true  returns the number of rows, otherwise returns  length(data) . Authors of custom data containers should implement  Base.length  for their type instead of  numobs .  numobs  should only be implemented for types where there is a difference between  numobs  and  Base.length  (such as multi-dimensional arrays). getobs  supports by default nested combinations of array, tuple, named tuples, and dictionaries.  See also  getobs . Examples \n# named tuples \nx = (a = [1, 2, 3], b = rand(6, 3))\nnumobs(x) == 3\n\n# dictionaries\nx = Dict(:a => [1, 2, 3], :b => rand(6, 3))\nnumobs(x) == 3 All internal containers must have the same number of observations: julia> x = (a = [1, 2, 3, 4], b = rand(6, 3));\n\njulia> numobs(x)\nERROR: DimensionMismatch: All data containers must have the same number of observations.\nStacktrace:\n [1] _check_numobs_error()\n   @ MLUtils ~/.julia/dev/MLUtils/src/observation.jl:163\n [2] _check_numobs\n   @ ~/.julia/dev/MLUtils/src/observation.jl:130 [inlined]\n [3] numobs(data::NamedTuple{(:a, :b), Tuple{Vector{Int64}, Matrix{Float64}}})\n   @ MLUtils ~/.julia/dev/MLUtils/src/observation.jl:177\n [4] top-level scope\n   @ REPL[35]:1 source"},{"id":831,"pagetitle":"API","title":"MLUtils.normalise","ref":"/mlutils/stable/api/#MLUtils.normalise","content":" MLUtils.normalise  —  Function normalise(x; dims=ndims(x), ϵ=1e-5) Normalise the array  x  to mean 0 and standard deviation 1 across the dimension(s) given by  dims . Per default,  dims  is the last dimension.  ϵ  is a small additive factor added to the denominator for numerical stability. source"},{"id":832,"pagetitle":"API","title":"MLUtils.obsview","ref":"/mlutils/stable/api/#MLUtils.obsview","content":" MLUtils.obsview  —  Function obsview(data, [indices]) Returns a lazy view of the observations in  data  that correspond to the given  indices . No data will be copied except of the indices. It is similar to constructing an  ObsView ,  but returns a  SubArray  if the type of  data  is  Array  or  SubArray . Furthermore, this function may be extended for custom types of  data  that also want to provide their own subset-type. In case  data  is a tuple, the constructor will be mapped over its elements. That means that the constructor returns a tuple of  ObsView  instead of a  ObsView  of tuples. If instead you want to get the subset of observations corresponding to the given  indices  in their native type, use  getobs . See  ObsView  for more information. source"},{"id":833,"pagetitle":"API","title":"MLUtils.ObsView","ref":"/mlutils/stable/api/#MLUtils.ObsView","content":" MLUtils.ObsView  —  Type ObsView(data, [indices]) Used to represent a subset of some  data  of arbitrary type by storing which observation-indices the subset spans. Furthermore, subsequent subsettings are accumulated without needing to access actual data. The main purpose for the existence of  ObsView  is to delay data access and movement until an actual batch of data (or single observation) is needed for some computation. This is particularily useful when the data is not located in memory, but on the hard drive or some remote location. In such a scenario one wants to load the required data only when needed. Any data access is delayed until  getindex  is called,  and even  getindex  returns the result of  obsview  which in general avoids data movement until  getobs  is called. If used as an iterator, the view will iterate over the dataset once, effectively denoting an epoch. Each iteration will return a lazy subset to the current observation. Arguments data  : The object describing the dataset. Can be of any   type as long as it implements  getobs  and    numobs  (see Details for more information). indices  : Optional. The index or indices of the   observation(s) in  data  that the subset should represent.   Can be of type  Int  or some subtype of  AbstractVector . Methods getindex  : Returns the observation(s) of the given   index/indices. No data is copied aside   from the required indices. numobs  : Returns the total number observations in the subset. getobs  : Returns the underlying data that the    ObsView  represents at the given relative indices. Note   that these indices are in \"subset space\", and in general will   not directly correspond to the same indices in the underlying   data set. Details For  ObsView  to work on some data structure, the desired type  MyType  must implement the following interface: getobs(data::MyType, idx)  :   Should return the observation(s) indexed by  idx .   In what form is up to the user.   Note that  idx  can be of type  Int  or  AbstractVector . numobs(data::MyType)  :   Should return the total number of observations in  data The following methods can also be provided and are optional: getobs(data::MyType)  :   By default this function is the identity function.   If that is not the behaviour that you want for your type,   you need to provide this method as well. obsview(data::MyType, idx)  :   If your custom type has its own kind of subset type, you can   return it here. An example for such a case are  SubArray  for   representing a subset of some  AbstractArray . getobs!(buffer, data::MyType, [idx])  :   Inplace version of  getobs(data, idx) . If this method   is provided for  MyType , then  eachobs  can preallocate a buffer that is then reused   every iteration. Note:  buffer  should be equivalent to the   return value of  getobs(::MyType, ...) , since this is how    buffer  is preallocated by default. Examples X, Y = MLUtils.load_iris()\n\n# The iris set has 150 observations and 4 features\n@assert size(X) == (4,150)\n\n# Represents the 80 observations as a ObsView\nv = ObsView(X, 21:100)\n@assert numobs(v) == 80\n@assert typeof(v) <: ObsView\n# getobs indexes into v\n@assert getobs(v, 1:10) == X[:, 21:30]\n\n# Use `obsview` to avoid boxing into ObsView\n# for types that provide a custom \"subset\", such as arrays.\n# Here it instead creates a native SubArray.\nv = obsview(X, 1:100)\n@assert numobs(v) == 100\n@assert typeof(v) <: SubArray\n\n# Also works for tuples of arbitrary length\nsubset = obsview((X, Y), 1:100)\n@assert numobs(subset) == 100\n@assert typeof(subset) <: Tuple # tuple of SubArray\n\n# Use as iterator\nfor x in ObsView(X)\n    @assert typeof(x) <: SubArray{Float64,1}\nend\n\n# iterate over each individual labeled observation\nfor (x, y) in ObsView((X, Y))\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert typeof(y) <: String\nend\n\n# same but in random order\nfor (x, y) in ObsView(shuffleobs((X, Y)))\n    @assert typeof(x) <: SubArray{Float64,1}\n    @assert typeof(y) <: String\nend\n\n# Indexing: take first 10 observations\nx, y = ObsView((X, Y))[1:10] See also obsview ,   getobs ,  numobs ,  splitobs ,  shuffleobs ,  kfolds . source"},{"id":834,"pagetitle":"API","title":"MLUtils.ones_like","ref":"/mlutils/stable/api/#MLUtils.ones_like","content":" MLUtils.ones_like  —  Function ones_like(x, [element_type=eltype(x)], [dims=size(x)])) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to 1.  The second and third arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. See also  zeros_like  and  fill_like . Examples julia> x = rand(Float32, 2)\n2-element Vector{Float32}:\n 0.8621633\n 0.5158395\n\njulia> ones_like(x, (3, 3))\n3×3 Matrix{Float32}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia> using CUDA\n\njulia> x = CUDA.rand(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 0.82297   0.656143\n 0.701828  0.391335\n\njulia> ones_like(x, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n 1.0  1.0\n 1.0  1.0 source"},{"id":835,"pagetitle":"API","title":"MLUtils.oversample","ref":"/mlutils/stable/api/#MLUtils.oversample","content":" MLUtils.oversample  —  Function oversample(data, classes; fraction=1, shuffle=true)\noversample(data::Tuple; fraction=1, shuffle=true) Generate a re-balanced version of  data  by repeatedly sampling existing observations in such a way that every class will have at least  fraction  times the number observations of the largest class in  classes . This way, all classes will have a minimum number of observations in the resulting data set relative to what largest class has in the given (original)  data . As an example, by default (i.e. with  fraction = 1 ) the resulting dataset will be near perfectly balanced. On the other hand, with  fraction = 0.5  every class in the resulting data with have at least 50% as many observations as the largest class. The  classes  input is an array with the same length as  numobs(data) .   The convenience parameter  shuffle  determines if the resulting data will be shuffled after its creation; if it is not shuffled then all the repeated samples will be together at the end, sorted by class. Defaults to  true . The output will contain both the resampled data and classes. # 6 observations with 3 features each\nX = rand(3, 6)\n# 2 classes, severely imbalanced\nY = [\"a\", \"b\", \"b\", \"b\", \"b\", \"a\"]\n\n# oversample the class \"a\" to match \"b\"\nX_bal, Y_bal = oversample(X, Y)\n\n# this results in a bigger dataset with repeated data\n@assert size(X_bal) == (3,8)\n@assert length(Y_bal) == 8\n\n# now both \"a\", and \"b\" have 4 observations each\n@assert sum(Y_bal .== \"a\") == 4\n@assert sum(Y_bal .== \"b\") == 4 For this function to work, the type of  data  must implement  numobs  and  getobs .  Note that if  data  is a tuple and  classes  is not given,  then it will be assumed that the last element of the tuple contains the classes. julia> data = DataFrame(X1=rand(6), X2=rand(6), Y=[:a,:b,:b,:b,:b,:a])\n6×3 DataFrames.DataFrame\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.933372  │ 0.812814    │ b │\n│ 4   │ 0.522172  │ 0.245457    │ b │\n│ 5   │ 0.505208  │ 0.11202     │ b │\n│ 6   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> getobs(oversample(data, data.Y))\n8×3 DataFrame\n Row │ X1        X2         Y      \n     │ Float64   Float64    Symbol \n─────┼─────────────────────────────\n   1 │ 0.376304  0.100022   a\n   2 │ 0.467095  0.185437   b\n   3 │ 0.481957  0.319906   b\n   4 │ 0.336762  0.390811   b\n   5 │ 0.376304  0.100022   a\n   6 │ 0.427064  0.0648339  a\n   7 │ 0.427064  0.0648339  a\n   8 │ 0.457043  0.490688   b See  ObsView  for more information on data subsets. See also  undersample . source"},{"id":836,"pagetitle":"API","title":"MLUtils.randobs","ref":"/mlutils/stable/api/#MLUtils.randobs","content":" MLUtils.randobs  —  Function randobs(data, [n]) Pick a random observation or a batch of  n  random observations from  data . For this function to work, the type of  data  must implement  numobs  and  getobs . source"},{"id":837,"pagetitle":"API","title":"MLUtils.rpad_constant","ref":"/mlutils/stable/api/#MLUtils.rpad_constant","content":" MLUtils.rpad_constant  —  Function rpad_constant(v::AbstractArray, n::Union{Integer, Tuple}, val = 0; dims=:) Return the given sequence padded with  val  along the dimensions  dims  up to a maximum length in each direction specified by  n . Examples julia> rpad_constant([1, 2], 4, -1) # passing with -1 up to size 4\n4-element Vector{Int64}:\n 1\n 2\n -1\n -1\n\njulia> rpad_constant([1, 2, 3], 2) # no padding if length is already greater than n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> rpad_constant([1 2; 3 4], 4; dims=1) # padding along the first dimension\n4×2 Matrix{Int64}:\n 1  2\n 3  4\n 0  0\n 0  0 \n\njulia> rpad_constant([1 2; 3 4], 4) # padding along all dimensions by default\n4×2 Matrix{Int64}:\n 1  2\n 3  4\n 0  0\n 0  0  source"},{"id":838,"pagetitle":"API","title":"MLUtils.shuffleobs","ref":"/mlutils/stable/api/#MLUtils.shuffleobs","content":" MLUtils.shuffleobs  —  Function shuffleobs([rng], data) Return a \"subset\" of  data  that spans all observations, but has the order of the observations shuffled. The values of  data  itself are not copied. Instead only the indices are shuffled. This function calls  obsview  to accomplish that, which means that the return value is likely of a different type than  data . # For Arrays the subset will be of type SubArray\n@assert typeof(shuffleobs(rand(4,10))) <: SubArray\n\n# Iterate through all observations in random order\nfor x in eachobs(shuffleobs(X))\n    ...\nend The optional parameter  rng  allows one to specify the random number generator used for shuffling. This is useful when reproducible results are desired. By default, uses the global RNG. See  Random  in Julia's standard library for more info. For this function to work, the type of  data  must implement  numobs  and  getobs . See  ObsView  for more information. source"},{"id":839,"pagetitle":"API","title":"MLUtils.splitobs","ref":"/mlutils/stable/api/#MLUtils.splitobs","content":" MLUtils.splitobs  —  Function splitobs(n::Int; at) -> Tuple Compute the indices for two or more disjoint subsets of the range  1:n  with splits given by  at . Examples julia> splitobs(100, at=0.7)\n(1:70, 71:100)\n\njulia> splitobs(100, at=(0.1, 0.4))\n(1:10, 11:50, 51:100) source splitobs(data; at, shuffle=false) -> Tuple Split the  data  into multiple subsets proportional to the value(s) of  at .  If  shuffle=true , randomly permute the observations before splitting. Supports any datatype implementing the  numobs  and  getobs  interfaces. Examples # A 70%-30% split\ntrain, test = splitobs(X, at=0.7)\n\n# A 50%-30%-20% split\ntrain, val, test = splitobs(X, at=(0.5, 0.3))\n\n# A 70%-30% split with multiple arrays and shuffling\ntrain, test = splitobs((X, y), at=0.7, shuffle=true)\nXtrain, Ytrain = train source Missing docstring. Missing docstring for  stack . Check Documenter's build log for details."},{"id":840,"pagetitle":"API","title":"MLUtils.unbatch","ref":"/mlutils/stable/api/#MLUtils.unbatch","content":" MLUtils.unbatch  —  Function unbatch(x) Reverse of the  batch  operation, unstacking the last dimension of the array  x . See also  unstack  and  chunk . Examples julia> unbatch([1 3 5 7;\n                2 4 6 8])\n4-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n [7, 8] source"},{"id":841,"pagetitle":"API","title":"MLUtils.undersample","ref":"/mlutils/stable/api/#MLUtils.undersample","content":" MLUtils.undersample  —  Function undersample(data, classes; shuffle=true) Generate a class-balanced version of  data  by subsampling its observations in such a way that the resulting number of observations will be the same number for every class. This way, all classes will have as many observations in the resulting data set as the smallest class has in the given (original)  data . The convenience parameter  shuffle  determines if the resulting data will be shuffled after its creation; if it is not shuffled then all the observations will be in their original order. Defaults to  false . The output will contain both the resampled data and classes. # 6 observations with 3 features each\nX = rand(3, 6)\n# 2 classes, severely imbalanced\nY = [\"a\", \"b\", \"b\", \"b\", \"b\", \"a\"]\n\n# subsample the class \"b\" to match \"a\"\nX_bal, Y_bal = undersample(X, Y)\n\n# this results in a smaller dataset\n@assert size(X_bal) == (3,4)\n@assert length(Y_bal) == 4\n\n# now both \"a\", and \"b\" have 2 observations each\n@assert sum(Y_bal .== \"a\") == 2\n@assert sum(Y_bal .== \"b\") == 2 For this function to work, the type of  data  must implement  numobs  and  getobs .  Note that if  data  is a tuple, then it will be assumed that the last element of the tuple contains the targets. julia> data = DataFrame(X1=rand(6), X2=rand(6), Y=[:a,:b,:b,:b,:b,:a])\n6×3 DataFrames.DataFrame\n│ Row │ X1        │ X2          │ Y │\n├─────┼───────────┼─────────────┼───┤\n│ 1   │ 0.226582  │ 0.0443222   │ a │\n│ 2   │ 0.504629  │ 0.722906    │ b │\n│ 3   │ 0.933372  │ 0.812814    │ b │\n│ 4   │ 0.522172  │ 0.245457    │ b │\n│ 5   │ 0.505208  │ 0.11202     │ b │\n│ 6   │ 0.0997825 │ 0.000341996 │ a │\n\njulia> getobs(undersample(data, data.Y))\n4×3 DataFrame\n Row │ X1        X2         Y      \n     │ Float64   Float64    Symbol \n─────┼─────────────────────────────\n   1 │ 0.427064  0.0648339  a\n   2 │ 0.376304  0.100022   a\n   3 │ 0.467095  0.185437   b\n   4 │ 0.457043  0.490688   b See  ObsView  for more information on data subsets. See also  oversample . source"},{"id":842,"pagetitle":"API","title":"MLUtils.unsqueeze","ref":"/mlutils/stable/api/#MLUtils.unsqueeze","content":" MLUtils.unsqueeze  —  Function unsqueeze(x; dims) Return  x  reshaped into an array one dimensionality higher than  x , where  dims  indicates in which dimension  x  is extended.  dims  can be an integer between 1 and  ndims(x)+1 . See also  flatten ,  stack . Examples julia> unsqueeze([1 2; 3 4], dims=2)\n2×1×2 Array{Int64, 3}:\n[:, :, 1] =\n 1\n 3\n\n[:, :, 2] =\n 2\n 4\n\n\njulia> xs = [[1, 2], [3, 4], [5, 6]]\n3-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n\njulia> unsqueeze(xs, dims=1)\n1×3 Matrix{Vector{Int64}}:\n [1, 2]  [3, 4]  [5, 6] source unsqueeze(; dims) Returns a function which, acting on an array, inserts a dimension of size 1 at  dims . Examples julia> rand(21, 22, 23) |> unsqueeze(dims=2) |> size\n(21, 1, 22, 23) source"},{"id":843,"pagetitle":"API","title":"MLUtils.unstack","ref":"/mlutils/stable/api/#MLUtils.unstack","content":" MLUtils.unstack  —  Function unstack(xs; dims) Unroll the given  xs  into an array of arrays along the given dimension  dims . See also  stack ,  unbatch , and  chunk . Examples julia> unstack([1 3 5 7; 2 4 6 8], dims=2)\n4-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n [7, 8] source"},{"id":844,"pagetitle":"API","title":"MLUtils.zeros_like","ref":"/mlutils/stable/api/#MLUtils.zeros_like","content":" MLUtils.zeros_like  —  Function zeros_like(x, [element_type=eltype(x)], [dims=size(x)])) Create an array with the given element type and size, based upon the given source array  x . All element of the new array will be set to 0.  The second and third arguments are both optional, defaulting to the given array's eltype and size. The dimensions may be specified as an integer or as a tuple argument. See also  ones_like  and  fill_like . Examples julia> x = rand(Float32, 2)\n2-element Vector{Float32}:\n 0.4005432\n 0.36934233\n\njulia> zeros_like(x, (3, 3))\n3×3 Matrix{Float32}:\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n\njulia> using CUDA\n\njulia> x = CUDA.rand(2, 2)\n2×2 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 0.0695155  0.667979\n 0.558468   0.59903\n\njulia> zeros_like(x, Float64)\n2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n 0.0  0.0\n 0.0  0.0 source"},{"id":845,"pagetitle":"API","title":"Datasets Docs","ref":"/mlutils/stable/api/#Datasets-Docs","content":" Datasets Docs"},{"id":846,"pagetitle":"API","title":"MLUtils.Datasets.load_iris","ref":"/mlutils/stable/api/#MLUtils.Datasets.load_iris","content":" MLUtils.Datasets.load_iris  —  Function load_iris() -> X, y, names Loads the first 150 observations from the Iris flower data set introduced by Ronald Fisher (1936). The 4 by 150 matrix  X  contains the numeric measurements, in which each individual column denotes an observation. The vector  y  contains the class labels as strings. The vector  names  contains the names of the features (i.e. rows of  X ) [1] Fisher, Ronald A. \"The use of multiple measurements in taxonomic problems.\" Annals of eugenics 7.2 (1936): 179-188. source"},{"id":847,"pagetitle":"API","title":"MLUtils.Datasets.make_sin","ref":"/mlutils/stable/api/#MLUtils.Datasets.make_sin","content":" MLUtils.Datasets.make_sin  —  Function make_sin(n, start, stop; noise = 0.3, f_rand = randn) -> x, y Generates  n  noisy equally spaces samples of a sinus from  start  to  stop  by adding  noise .* f_rand(length(x))  to the result of  fun(x) . source"},{"id":848,"pagetitle":"API","title":"MLUtils.Datasets.make_spiral","ref":"/mlutils/stable/api/#MLUtils.Datasets.make_spiral","content":" MLUtils.Datasets.make_spiral  —  Function make_spiral(n, a, theta, b; noise = 0.01, f_rand = randn) -> x, y Generates  n  noisy responses for a spiral with two labels. Uses the radius, angle and scaling arguments to space the points in 2D space and adding  noise .* f_randn(n)  to the response. source"},{"id":849,"pagetitle":"API","title":"MLUtils.Datasets.make_poly","ref":"/mlutils/stable/api/#MLUtils.Datasets.make_poly","content":" MLUtils.Datasets.make_poly  —  Function make_poly(coef, x; noise = 0.01, f_rand = randn) -> x, y Generates a noisy response for a polynomial of degree  length(coef)  using the vector  x  as input and adding  noise .* f_randn(length(x))  to the result. The vector  coef  contains the coefficients for the terms of the polynome. The first element of  coef  denotes the coefficient for the term with the highest degree, while the last element of  coef  denotes the intercept. source"},{"id":854,"pagetitle":"Overview","title":"OneHotArrays.jl","ref":"/onehotarrays/stable/#OneHotArrays.jl","content":" OneHotArrays.jl Memory efficient one-hot array encodings (primarily for use in machine-learning contexts like Flux.jl)."},{"id":855,"pagetitle":"Overview","title":"Usage","ref":"/onehotarrays/stable/#Usage","content":" Usage One-hot arrays are boolean arrays where only a single element in the first dimension is  true  (i.e. \"hot\"). OneHotArrays.jl stores such arrays efficiently by encoding a N-dimensional array of booleans as a (N - 1)-dimensional array of integers. For example, the one-hot vector below only uses a single  UInt32  for storage. julia> β = onehot(:b, (:a, :b, :c))\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n 1\n ⋅ As seen above, the one-hot encoding can be useful for representing labeled data. The label  :b  is encoded into a 3-element vector where the \"hot\" element indicates the label from the set  (:a, :b, :c) . We can also encode a batch of one-hot vectors or reverse the encoding. julia> oh = onehotbatch(\"abracadabra\", 'a':'e', 'e')\n5×11 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  1\n ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n\njulia> Flux.onecold(β, (:a, :b, :c))\n:b\n\njulia> Flux.onecold([0.3, 0.2, 0.5], (:a, :b, :c))\n:c In addition to functions for encoding and decoding data as one-hot, this package provides numerous \"fast-paths\" for linear algebraic operations with one-hot arrays. For example, multiplying by a matrix by a one-hot vector triggers an indexing operation instead of a matrix multiplication."},{"id":858,"pagetitle":"Reference","title":"Reference","ref":"/onehotarrays/stable/reference/#Reference","content":" Reference"},{"id":859,"pagetitle":"Reference","title":"OneHotArrays.onecold","ref":"/onehotarrays/stable/reference/#OneHotArrays.onecold","content":" OneHotArrays.onecold  —  Function onecold(y::AbstractArray, labels = 1:size(y,1)) Roughly the inverse operation of  onehot  or  onehotbatch :  This finds the index of the largest element of  y , or each column of  y ,  and looks them up in  labels . If  labels  are not specified, the default is integers  1:size(y,1)  – the same operation as  argmax(y, dims=1)  but sometimes a different return type. Examples julia> onecold([false, true, false])\n2\n\njulia> onecold([0.3, 0.2, 0.5], (:a, :b, :c))\n:c\n\njulia> onecold([ 1  0  0  1  0  1  0  1  0  0  1\n                 0  1  0  0  0  0  0  0  1  0  0\n                 0  0  0  0  1  0  0  0  0  0  0\n                 0  0  0  0  0  0  1  0  0  0  0\n                 0  0  1  0  0  0  0  0  0  1  0 ], 'a':'e') |> String\n\"abeacadabea\" source"},{"id":860,"pagetitle":"Reference","title":"OneHotArrays.onehot","ref":"/onehotarrays/stable/reference/#OneHotArrays.onehot-Tuple{Any, Any}","content":" OneHotArrays.onehot  —  Method onehot(x, labels, [default]) Returns a  OneHotVector  which is roughly a sparse representation of  x .== labels . Instead of storing say  Vector{Bool} , it stores the index of the first occurrence  of  x  in  labels . If  x  is not found in labels, then it either returns  onehot(default, labels) , or gives an error if no default is given. See also  onehotbatch  to apply this to many  x s,  and  onecold  to reverse either of these, as well as to generalise  argmax . Examples julia> β = onehot(:b, (:a, :b, :c))\n3-element OneHotVector(::UInt32) with eltype Bool:\n ⋅\n 1\n ⋅\n\njulia> αβγ = (onehot(0, 0:2), β, onehot(:z, [:a, :b, :c], :c))  # uses default\n(Bool[1, 0, 0], Bool[0, 1, 0], Bool[0, 0, 1])\n\njulia> hcat(αβγ...)  # preserves sparsity\n3×3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅\n ⋅  1  ⋅\n ⋅  ⋅  1 source"},{"id":861,"pagetitle":"Reference","title":"OneHotArrays.onehotbatch","ref":"/onehotarrays/stable/reference/#OneHotArrays.onehotbatch-Tuple{Any, Any, Vararg{Any, N} where N}","content":" OneHotArrays.onehotbatch  —  Method onehotbatch(xs, labels, [default]) Returns a  OneHotMatrix  where  k th column of the matrix is  onehot(xs[k], labels) . This is a sparse matrix, which stores just a  Vector{UInt32}  containing the indices of the nonzero elements. If one of the inputs in  xs  is not found in  labels , that column is  onehot(default, labels)  if  default  is given, else an error. If  xs  has more dimensions,  N = ndims(xs) > 1 , then the result is an   AbstractArray{Bool, N+1}  which is one-hot along the first dimension,  i.e.  result[:, k...] == onehot(xs[k...], labels) . Note that  xs  can be any iterable, such as a string. And that using a tuple for  labels  will often speed up construction, certainly for less than 32 classes. Examples julia> oh = onehotbatch(\"abracadabra\", 'a':'e', 'e')\n5×11 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  1\n ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n\njulia> reshape(1:15, 3, 5) * oh  # this matrix multiplication is done efficiently\n3×11 Matrix{Int64}:\n 1  4  13  1  7  1  10  1  4  13  1\n 2  5  14  2  8  2  11  2  5  14  2\n 3  6  15  3  9  3  12  3  6  15  3 source"},{"id":862,"pagetitle":"Reference","title":"OneHotArrays.OneHotArray","ref":"/onehotarrays/stable/reference/#OneHotArrays.OneHotArray","content":" OneHotArrays.OneHotArray  —  Type OneHotArray{T, N, M, I} <: AbstractArray{Bool, M}\nOneHotArray(indices, L) A one-hot  M -dimensional array with  L  labels (i.e.  size(A, 1) == L  and  sum(A, dims=1) == 1 ) stored as a compact  N == M-1 -dimensional array of indices. Typically constructed by  onehot  and  onehotbatch . Parameter  I  is the type of the underlying storage, and  T  its eltype. source"},{"id":863,"pagetitle":"Reference","title":"OneHotArrays.OneHotMatrix","ref":"/onehotarrays/stable/reference/#OneHotArrays.OneHotMatrix","content":" OneHotArrays.OneHotMatrix  —  Type OneHotMatrix{T, I} = OneHotArray{T, 1, 2, I}\nOneHotMatrix(indices, L) A one-hot matrix (with  L  labels) typically constructed using  onehotbatch . Stored efficiently as a vector of indices with type  I  and eltype  T . source"},{"id":864,"pagetitle":"Reference","title":"OneHotArrays.OneHotVector","ref":"/onehotarrays/stable/reference/#OneHotArrays.OneHotVector","content":" OneHotArrays.OneHotVector  —  Type OneHotVector{T} = OneHotArray{T, 0, 1, T}\nOneHotVector(indices, L) A one-hot vector with  L  labels (i.e.  length(A) == L  and  count(A) == 1 ) typically constructed by  onehot . Stored efficiently as a single index of type  T , usually  UInt32 . source"},{"id":869,"pagetitle":"Home","title":"MLDatasets.jl's Documentation","ref":"/mldatasets/stable/#MLDatasets.jl's-Documentation","content":" MLDatasets.jl's Documentation This package represents a community effort to provide a common interface for accessing common Machine Learning (ML) datasets. In contrast to other data-related Julia packages, the focus of  MLDatasets.jl  is specifically on downloading, unpacking, and accessing benchmark dataset. Functionality for the purpose of data processing or visualization is only provided to a degree that is special to some dataset. This package is a part of the  JuliaML  ecosystem."},{"id":870,"pagetitle":"Home","title":"Installation","ref":"/mldatasets/stable/#Installation","content":" Installation To install  MLDatasets.jl , start up Julia and type the following code snippet into the REPL. It makes use of the native Julia package manager. Pkg.add(\"MLDatasets\")"},{"id":871,"pagetitle":"Home","title":"Available Datasets","ref":"/mldatasets/stable/#Available-Datasets","content":" Available Datasets Datasets are grouped into different categories. Click on the links below for a full list of datasets available in each category. Graph Datasets  - datasets with an underlying graph structure: Cora, PubMed, CiteSeer, ... Miscellaneuous Datasets  - datasets that do not fall into any of the other categories: Iris, BostonHousing, ... Text Datasets  - datasets for language models.  Vision Datasets  - vision related datasets such as MNIST, CIFAR10, CIFAR100, ... "},{"id":872,"pagetitle":"Home","title":"Basic Usage","ref":"/mldatasets/stable/#Basic-Usage","content":" Basic Usage The way  MLDatasets.jl  is organized is that each dataset is its own type.  Where possible, those types share a common interface (fields and methods).  Once a dataset has been instantiated, e.g. by  dataset = MNIST() ,   an observation  i  can be retrieved using the indexing syntax  dataset[i] . By indexing with no arguments,  dataset[:] , the whole set of observations is collected. The total number of observations is given by  length(dataset) . For example you can load the training set of the  MNIST  database of handwritten digits using the following commands: julia> using MLDatasets\n\njulia> trainset = MNIST(:train)\ndataset MNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> length(trainset)\n60000\n\njulia> trainset[1]  # return first observation as a NamedTuple\n(features = Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \n targets = 5)\n\njulia> X_train, y_train = trainset[:] # return all observations\n(features = [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], \n targets = [5, 0, 4, 1, 9, 2, 1, 3, 1, 4  …  9, 2, 9, 5, 1, 8, 3, 5, 6, 8])\n\njulia> summary(X_train)\n\"28×28×60000 Array{Float32, 3}\" Input features are commonly denoted by  features , while classification labels and regression targets are denoted by  targets . julia> using MLDatasets, DataFrames\n\njulia> iris = Iris()\ndataset Iris:\n  metadata    =>    Dict{String, Any} with 4 entries\n  features    =>    150×4 DataFrame\n  targets     =>    150×1 DataFrame\n  dataframe   =>    150×5 DataFrame\n\njulia> iris.features\n150×4 DataFrame\n Row │ sepallength  sepalwidth  petallength  petalwidth \n     │ Float64      Float64     Float64      Float64    \n─────┼──────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2\n   2 │         4.9         3.0          1.4         0.2\n   3 │         4.7         3.2          1.3         0.2\n   4 │         4.6         3.1          1.5         0.2\n   5 │         5.0         3.6          1.4         0.2\n   6 │         5.4         3.9          1.7         0.4\n   7 │         4.6         3.4          1.4         0.3\n   8 │         5.0         3.4          1.5         0.2\n   9 │         4.4         2.9          1.4         0.2\n  ⋮  │      ⋮           ⋮            ⋮           ⋮\n 142 │         6.9         3.1          5.1         2.3\n 143 │         5.8         2.7          5.1         1.9\n 144 │         6.8         3.2          5.9         2.3\n 145 │         6.7         3.3          5.7         2.5\n 146 │         6.7         3.0          5.2         2.3\n 147 │         6.3         2.5          5.0         1.9\n 148 │         6.5         3.0          5.2         2.0\n 149 │         6.2         3.4          5.4         2.3\n 150 │         5.9         3.0          5.1         1.8\n                                        132 rows omitted\n\njulia> iris.targets\n150×1 DataFrame\n Row │ class          \n     │ String15       \n─────┼────────────────\n   1 │ Iris-setosa\n   2 │ Iris-setosa\n   3 │ Iris-setosa\n   4 │ Iris-setosa\n   5 │ Iris-setosa\n   6 │ Iris-setosa\n   7 │ Iris-setosa\n   8 │ Iris-setosa\n   9 │ Iris-setosa\n  ⋮  │       ⋮\n 142 │ Iris-virginica\n 143 │ Iris-virginica\n 144 │ Iris-virginica\n 145 │ Iris-virginica\n 146 │ Iris-virginica\n 147 │ Iris-virginica\n 148 │ Iris-virginica\n 149 │ Iris-virginica\n 150 │ Iris-virginica\n      132 rows omitted"},{"id":873,"pagetitle":"Home","title":"MLUtils compatibility","ref":"/mldatasets/stable/#MLUtils-compatibility","content":" MLUtils compatibility MLDatasets.jl guarantees compatibility with the  getobs  and  numobs  interface defined in  MLUtils.jl . In practice, applying  getobs  and  numobs  on datasets is equivalent to applying indexing and  length ."},{"id":874,"pagetitle":"Home","title":"Conditional module loading","ref":"/mldatasets/stable/#Conditional-module-loading","content":" Conditional module loading MLDatasets.jl relies on many different packages in order to load and process the diverse type of datasets it supports. Most likely, any single user of the library will use a limited subset of these functionalities. In order to reduce the time taken by  using MLDatasets  in users' code, we use a  lazy import system  that defers the import of packages inside MLDatasets.jl as much as possible.   For some of the packages, some manual intervention is needed from the user.  As an example, the following code will produce an error: julia> using MLDataset\n\njulia> MNIST(); # fine, MNIST doesn't require DataFrames\n\njulia> Iris() # ERROR: Add `import DataFrames` or `using DataFrames` to your code to unlock this functionality. We can easily fix the error with an additional import as recommended by the error message: julia> using MLDataset, DataFrames\n\njulia> Iris()\ndataset Iris:\n  metadata    =>    Dict{String, Any} with 4 entries\n  features    =>    150×4 DataFrame\n  targets     =>    150×1 DataFrame\n  dataframe   =>    150×5 DataFrame"},{"id":875,"pagetitle":"Home","title":"Download location","ref":"/mldatasets/stable/#Download-location","content":" Download location MLDatasets.jl is built on top of the package  DataDeps.jl . To load the data, the package looks for the necessary files in various locations (see  DataDeps.jl  for more information on how to configure such defaults). If the data can't be found in any of those locations, then the package will trigger a download dialog to  ~/.julia/datadeps/<DATASETNAME> . To overwrite this on a case by case basis, it is possible to specify a data directory directly in the dataset constructor (e.g.  MNIST(dir = <directory>) ). In order to download datasets without having to manually confirm the download,  you can set to true the following environmental variable: ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true"},{"id":878,"pagetitle":"LICENSE","title":"LICENSE","ref":"/mldatasets/stable/LICENSE/#LICENSE","content":" LICENSE The MIT License (MIT) Copyright (c) 2015 Hiroyuki Shindo Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."},{"id":881,"pagetitle":"Dataset Containers","title":"Dataset Containers","ref":"/mldatasets/stable/containers/overview/#Dataset-Containers","content":" Dataset Containers MLDatasets.jl contains several reusable data containers for accessing datasets in common storage formats. This feature is a work-in-progress and subject to change."},{"id":882,"pagetitle":"Dataset Containers","title":"MLDatasets.FileDataset","ref":"/mldatasets/stable/containers/overview/#MLDatasets.FileDataset","content":" MLDatasets.FileDataset  —  Type FileDataset([loadfn = FileIO.load,] paths)\nFileDataset([loadfn = FileIO.load,] dir, pattern = \"*\", depth = 4) Wrap a set of file  paths  as a dataset (traversed in the same order as  paths ). Alternatively, specify a  dir  and collect all paths that match a glob  pattern  (recursively globbing by  depth ). The glob order determines the traversal order. source"},{"id":883,"pagetitle":"Dataset Containers","title":"MLDatasets.CachedDataset","ref":"/mldatasets/stable/containers/overview/#MLDatasets.CachedDataset","content":" MLDatasets.CachedDataset  —  Type CachedDataset(source, cachesize = numbobs(source))\nCachedDataset(source, cacheidx = 1:numbobs(source))\nCachedDataset(source, cacheidx, cache) Wrap a  source  data container and cache  cachesize  samples in memory. This can be useful for improving read speeds when  source  is a lazy data container, but your system memory is large enough to store a sizeable chunk of it. By default the observation indices  1:cachesize  are cached. You can manually pass in a set of  cacheidx  as well. See also  make_cache  for customizing the default cache creation for  source . source"},{"id":884,"pagetitle":"Dataset Containers","title":"MLDatasets.make_cache","ref":"/mldatasets/stable/containers/overview/#MLDatasets.make_cache","content":" MLDatasets.make_cache  —  Function make_cache(source, cacheidx) Return a in-memory copy of  source  at observation indices  cacheidx . Defaults to  getobs(source, cacheidx) . source"},{"id":887,"pagetitle":"Graphs","title":"Graph Datasets","ref":"/mldatasets/stable/datasets/graphs/#Graph-Datasets","content":" Graph Datasets A collection of datasets with an underlying graph structure. Some of these datasets contain a single graph, that can be accessed with  dataset[:]  or  dataset[1] . Others contain many graphs,  accessed through  dataset[i] . Graphs are represented by the  MLDatasets.Graph   and  MLDatasets.HeteroGraph  type."},{"id":888,"pagetitle":"Graphs","title":"Index","ref":"/mldatasets/stable/datasets/graphs/#Index","content":" Index MLDatasets.CiteSeer MLDatasets.Cora MLDatasets.Graph MLDatasets.HeteroGraph MLDatasets.KarateClub MLDatasets.METRLA MLDatasets.MovieLens MLDatasets.OGBDataset MLDatasets.OrganicMaterialsDB MLDatasets.PEMSBAY MLDatasets.PolBlogs MLDatasets.PubMed MLDatasets.Reddit MLDatasets.TUDataset MLDatasets.TemporalBrains"},{"id":889,"pagetitle":"Graphs","title":"Documentation","ref":"/mldatasets/stable/datasets/graphs/#Documentation","content":" Documentation"},{"id":890,"pagetitle":"Graphs","title":"MLDatasets.Graph","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.Graph","content":" MLDatasets.Graph  —  Type Graph(; kws...) A type that represents a graph and that can also store node and edge data. It doesn't distinguish between directed or undirected graph, therefore for undirected graphs will store edges in both directions. Nodes are indexed in  1:num_nodes . Graph datasets in MLDatasets.jl contain one or more  Graph  or  HeteroGraph  objects. Keyword Arguments num_nodes : the number of nodes. If omitted, is inferred from  edge_index . edge_index : a tuple containing two vectors with length equal to the number of edges.   The first vector contains the list of the source nodes of each edge, the second the target nodes.   Defaults to  (Int[], Int[]) . node_data : node-related data. Can be  nothing , a named tuple of arrays or a dictionary of arrays.           The arrays last dimension size should be equal to the number of nodes.           Default  nothing . edge_data : edge-related data. Can be  nothing , a named tuple of arrays or a dictionary of arrays.            The arrays' last dimension size should be equal to the number of edges.            Default  nothing . Examples All graph datasets in MLDatasets.jl contain  Graph  or  HeteroGraph  objects: julia> using MLDatasets: Cora\n\njulia> d = Cora() # the Cora dataset\ndataset Cora:\n  metadata    =>    Dict{String, Any} with 3 entries\n  graphs      =>    1-element Vector{Graph}\n\njulia> d[1]\nGraph:\n  num_nodes   =>    2708\n  num_edges   =>    10556\n  edge_index  =>    (\"10556-element Vector{Int64}\", \"10556-element Vector{Int64}\")\n  node_data   =>    (features = \"1433×2708 Matrix{Float32}\", targets = \"2708-element Vector{Int64}\", train_mask = \"2708-element BitVector with 140 trues\", val_mask = \"2708-element BitVector with 500 trues\", test_mask = \"2708-element BitVector with 1000 trues\")\n  edge_data   =>    nothing Let's se how to convert a Graphs.jl's graph to a  MLDatasets.Graph  and viceversa: import Graphs, MLDatasets\n\n## From Graphs.jl to MLDatasets.Graphs\n\n# From a directed graph\ng = Graphs.erdos_renyi(10, 20, is_directed=true)\ns = [e.src for e in Graphs.edges(g)]\nt = [e.dst for e in Graphs.edges(g)]\nmlg = MLDatasets.Graph(num_nodes=10, edge_index=(s, t))\n\n# From an undirected graph\ng = Graphs.erdos_renyi(10, 20, is_directed=false)\ns = [e.src for e in Graphs.edges(g)]\nt = [e.dst for e in Graphs.edges(g)]\ns, t = [s; t], [t; s] # adding reverse edges\nmlg = MLDatasets.Graph(num_nodes=10, edge_index=(s, t))\n\n# From MLDatasets.Graphs to Graphs.jl\ns, t = mlg.edge_index\ng = Graphs.DiGraph(mlg.num_nodes)\nfor (i, j) in zip(s, t)\n    Graphs.add_edge!(g, i, j)\nend source"},{"id":891,"pagetitle":"Graphs","title":"MLDatasets.HeteroGraph","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.HeteroGraph","content":" MLDatasets.HeteroGraph  —  Type HeteroGraph(; kws...) HeteroGraph is used for HeteroGeneous Graphs. HeteroGraph  unlike  Graph  can have different types of nodes. Each node pertains to different types of information.  Edges in  HeteroGraph  is defined by relations. A relation is a tuple of  ( src_node_type ,  edge_type ,  target_node_type ) where  edge_type  represents the relation between the src and target nodes. Edges between same node types are possible.  A  HeteroGraph  can be directed or undirected. It doesn't distinguish between directed  or undirected graphs. Therefore, for undirected graphs, it will store edges in both directions. Nodes are indexed in  1:num_nodes . Keyword Arguments num_nodes : Dictionary containing the number of nodes for each node type. If omitted, is inferred from  edge_index . num_edges : Dictionary containing the number of edges for each relation. edge_indices : Dictionary containing the  edge_index  for each edge relation. An  edge_index  is a tuple containing two vectors with length equal to the number of edges for the relation.   The first vector contains the list of the source nodes of each edge, the second contains the target nodes. node_data : node-related data. Can be  nothing , Dictionary of a dictionary of arrays. Data of a specific type of node can be accessed            using node data[node type].The array's last dimension size should be equal to the number of nodes.           Default  nothing . edge_data : Can be  nothing , Dictionary of a dictionary of arrays. Data of a specific type of edge can be accessed            using edge data[edge type].The array's last dimension size should be equal to the number of nodes.           Default  nothing . source"},{"id":892,"pagetitle":"Graphs","title":"MLDatasets.CiteSeer","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.CiteSeer","content":" MLDatasets.CiteSeer  —  Type CiteSeer(; dir=nothing) The CiteSeer citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2]. References [1]:  Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking [2]:  Planetoid source"},{"id":893,"pagetitle":"Graphs","title":"MLDatasets.Cora","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.Cora","content":" MLDatasets.Cora  —  Type Cora() The Cora citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. Each node has a predefined feature with 1433 dimensions.  The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2]. Statistics Nodes: 2708 Edges: 10556 Number of Classes: 7 Label split: Train:  140 Val:    500 Test:  1000 The split is the one used in the original paper [1] and  doesn't consider all nodes. References [1]:  Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking [2]:  Planetoid source"},{"id":894,"pagetitle":"Graphs","title":"MLDatasets.KarateClub","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.KarateClub","content":" MLDatasets.KarateClub  —  Type KarateClub() The Zachary's karate club dataset originally appeared in Ref [1]. The network contains 34 nodes (members of the karate club). The nodes are connected by 78 undirected and unweighted edges. The edges indicate if the two members interacted outside the club. The node labels indicate which community or the karate club the member belongs to. The club based labels are as per the original dataset in Ref [1]. The community labels are obtained by modularity-based clustering following Ref [2]. The data is retrieved from Ref [3] and [4]. One node per unique label is used as training data. References [1]:  An Information Flow Model for Conflict and Fission in Small Groups [2]:  Semi-supervised Classification with Graph Convolutional Networks [3]:  PyTorch Geometric Karate Club Dataset [4]:  NetworkX Zachary's Karate Club Dataset source"},{"id":895,"pagetitle":"Graphs","title":"MLDatasets.MovieLens","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.MovieLens","content":" MLDatasets.MovieLens  —  Type MovieLens(name; dir=nothing) Datasets from the  MovieLens website  collected and maintained by  GroupLens .  The MovieLens datasets are presented in a Graph format.  For license and usage restrictions please refer to the Readme.md of the datasets. There are 6 versions of MovieLens datasets currently supported: \"100k\",  \"1m\",  \"10m\", \"20m\", \"25m\", \"latest-small\".  The 100k and 1k datasets contain movie data and rating data along with demographic data. Starting from the 10m dataset, MovieLens datasets no longer contain the demographic data.  These datasets contain movie data, rating data, and tag information.  The 20m and 25m datasets additionally contain  genome tag scores .  Each movie in these datasets contains tag relevance scores for every tag. Each dataset contains an heterogeneous graph, with two kinds of nodes,   movie  and  user . The rating is represented by an edge between them:  (user, rating, movie) .  20m, 25m, and latest-small datasets also contain  tag  nodes and edges of type  (user, tag, movie)  and  optionally  (movie, score, tag) . Examples MovieLens 100K dataset julia> data = MovieLens(\"100k\")\nMovieLens 100k:\n  metadata    =>    Dict{String, Any} with 2 entries\n  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}\n\njulia> metadata = data.metadata\nDict{String, Any} with 2 entries:\n  \"genre_labels\"      => [\"Unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fa…\n  \"movie_id_to_title\" => Dict(1144=>\"Quiet Room, The (1996)\", 1175=>\"Hugo Pool (1997)\", 719=>\"Canadian Bacon (1994)\", 1546=>\"Shadow…\n\njulia> g = data[:]\n  Heterogeneous Graph:\n    node_types    =>    2-element Vector{String}\n    edge_types    =>    1-element Vector{Tuple{String, String, String}}\n    num_nodes     =>    Dict{String, Int64} with 2 entries\n    num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 1 entry\n    edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 1 entry\n    node_data     =>    Dict{String, Dict} with 2 entries\n    edge_data     =>    Dict{Tuple{String, String, String}, Dict} with 1 entry\n\n# Access the user information\njulia> user_data = g.node_data[\"user\"]\nDict{Symbol, AbstractVector} with 4 entries:\n  :age        => [24, 53, 23, 24, 33, 42, 57, 36, 29, 53  …  61, 42, 24, 48, 38, 26, 32, 20, 48, 22]\n  :occupation => [\"technician\", \"other\", \"writer\", \"technician\", \"other\", \"executive\", \"administrator\", \"administrator\", \"student\",…\n  :zipcode    => [\"85711\", \"94043\", \"32067\", \"43537\", \"15213\", \"98101\", \"91344\", \"05201\", \"01002\", \"90703\"  …  \"22902\", \"66221\", \"3…\n  :gender     => Bool[1, 0, 1, 1, 0, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n\n# Access rating information\njulia> g.edge_data[(\"user\", \"rating\", \"movie\")]\nDict{Symbol, Vector} with 2 entries:\n  :timestamp => [881250949, 891717742, 878887116, 880606923, 886397596, 884182806, 881171488, 891628467, 886324817, 883603013  …  8…\n  :rating    => Float16[3.0, 3.0, 1.0, 2.0, 1.0, 4.0, 2.0, 5.0, 3.0, 3.0  …  4.0, 4.0, 3.0, 2.0, 3.0, 3.0, 5.0, 1.0, 2.0, 3.0] MovieLens 20m dataset julia> data = MovieLens(\"20m\")\nMovieLens 20m:\n  metadata    =>    Dict{String, Any} with 4 entries\n  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}\n\n# There is only 1 graph in MovieLens dataset\njulia> g = data[1]\nHeterogeneous Graph:\n  node_types    =>    3-element Vector{String}\n  edge_types    =>    3-element Vector{Tuple{String, String, String}}\n  num_nodes     =>    Dict{String, Int64} with 3 entries\n  num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 3 entries\n  edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 3 entries\n  node_data     =>    Dict{String, Dict} with 0 entries\n  edge_data     =>    Dict{Tuple{String, String, String}, Dict} with 3 entries\n\n# Apart from user rating a movie, a user assigns tag to movies and there are genome-scores for movie-tag pairs \njulia> g.edge_indices\n  Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 3 entries:\n    (\"movie\", \"score\", \"tag\")   => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  131170, 131170, 131170, 131170, 131170, 131170, 131170, 131170,…\n    (\"user\", \"tag\", \"movie\")    => ([18, 65, 65, 65, 65, 65, 65, 65, 65, 65  …  3489, 7045, 7045, 7164, 7164, 55999, 55999, 55999, 55…\n    (\"user\", \"rating\", \"movie\") => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  60816, 61160, 65682, 66762, 68319, 68954, 69526, 69644, 70286, …\n\n# Access the rating\njulia> g.edge_data[(\"user\", \"rating\", \"movie\")]\nDict{Symbol, Vector} with 2 entries:\n  :timestamp => [1112486027, 1112484676, 1112484819, 1112484727, 1112484580, 1094785740, 1094785734, 1112485573, 1112484940, 111248…\n  :rating    => Float16[3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 4.0, 4.0, 4.0, 4.0  …  4.5, 4.0, 4.5, 4.5, 4.5, 4.5, 4.5, 3.0, 5.0, 2.5]\n\n# Access the movie-tag scores\nscore = g.edge_data[(\"movie\", \"score\", \"tag\")][:score]\n23419536-element Vector{Float64}:\n 0.025000000000000022\n 0.025000000000000022\n 0.057750000000000024\n ⋮ References [1]  GroupLens Website [2]  TensorFlow MovieLens Implementation [3] Jesse Vig, Shilad Sen, and John Riedl. 2012. The Tag Genome: Encoding Community Knowledge to Support Novel Interaction. ACM Trans. Interact. Intell. Syst. 2, 3, Article 13 (September 2012), 44 pages. https://doi.org/10.1145/2362394.2362395.    [4] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (January 2016), 19 pages. https://doi.org/10.1145/2827872   source"},{"id":896,"pagetitle":"Graphs","title":"MLDatasets.OGBDataset","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.OGBDataset","content":" MLDatasets.OGBDataset  —  Type OGBDataset(name; dir=nothing) The collection of datasets from the  Open Graph Benchmark: Datasets for Machine Learning on Graphs  paper. name  is the name  of one of the datasets (listed  here ) available for node prediction, edge prediction, or graph prediction tasks. Examples Node prediction tasks julia> data = OGBDataset(\"ogbn-arxiv\")\nOGBDataset ogbn-arxiv:\n  metadata    =>    Dict{String, Any} with 17 entries\n  graphs      =>    1-element Vector{MLDatasets.Graph}\n  graph_data  =>    nothing\n\njulia> data[:]\nGraph:\n  num_nodes   =>    169343\n  num_edges   =>    1166243\n  edge_index  =>    (\"1166243-element Vector{Int64}\", \"1166243-element Vector{Int64}\")\n  node_data   =>    (val_mask = \"29799-trues BitVector\", test_mask = \"48603-trues BitVector\", year = \"169343-element Vector{Int64}\", features = \"128×169343 Matrix{Float32}\", label = \"169343-element Vector{Int64}\", train_mask = \"90941-trues BitVector\")\n  edge_data   =>    nothing\n\njulia> data.metadata\nDict{String, Any} with 17 entries:\n  \"download_name\"         => \"arxiv\"\n  \"num classes\"           => 40\n  \"num tasks\"             => 1\n  \"binary\"                => false\n  \"url\"                   => \"http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\"\n  \"additional node files\" => [\"node_year\"]\n  \"is hetero\"             => false\n  \"task level\"            => \"node\"\n  ⋮                       => ⋮\n\njulia> data = OGBDataset(\"ogbn-mag\")\nOGBDataset ogbn-mag:\n  metadata    =>    Dict{String, Any} with 17 entries\n  graphs      =>    1-element Vector{MLDatasets.HeteroGraph}\n  graph_data  =>    nothing\n\njulia> data[:]\nHeterogeneous Graph:\n  num_nodes     =>    Dict{String, Int64} with 4 entries\n  num_edges     =>    Dict{Tuple{String, String, String}, Int64} with 4 entries\n  edge_indices  =>    Dict{Tuple{String, String, String}, Tuple{Vector{Int64}, Vector{Int64}}} with 4 entries\n  node_data     =>    (year = \"Dict{String, Vector{Float32}} with 1 entry\", features = \"Dict{String, Matrix{Float32}} with 1 entry\", label = \"Dict{String, Vector{Int64}} with 1 entry\")\n  edge_data     =>    (reltype = \"Dict{Tuple{String, String, String}, Vector{Float32}} with 4 entries\",) Edge prediction task julia> data = OGBDataset(\"ogbl-collab\")\nOGBDataset ogbl-collab:\n  metadata    =>    Dict{String, Any} with 15 entries\n  graphs      =>    1-element Vector{MLDatasets.Graph}\n  graph_data  =>    nothing\n\njulia> data[:]\nGraph:\n  num_nodes   =>    235868\n  num_edges   =>    2358104\n  edge_index  =>    (\"2358104-element Vector{Int64}\", \"2358104-element Vector{Int64}\")\n  node_data   =>    (features = \"128×235868 Matrix{Float32}\",)\n  edge_data   =>    (year = \"2×1179052 Matrix{Int64}\", weight = \"2×1179052 Matrix{Int64}\") Graph prediction task julia> data = OGBDataset(\"ogbg-molhiv\")\nOGBDataset ogbg-molhiv:\n  metadata    =>    Dict{String, Any} with 17 entries\n  graphs      =>    41127-element Vector{MLDatasets.Graph}\n  graph_data  =>    (labels = \"41127-element Vector{Int64}\", train_mask = \"32901-trues BitVector\", val_mask = \"4113-trues BitVector\", test_mask = \"4113-trues BitVector\")\n\njulia> data[1]\n(graphs = Graph(19, 40), labels = 0) source"},{"id":897,"pagetitle":"Graphs","title":"MLDatasets.OrganicMaterialsDB","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.OrganicMaterialsDB","content":" MLDatasets.OrganicMaterialsDB  —  Type OrganicMaterialsDB(; split=:train, dir=nothing) The OMDB-GAP1 v1.1 dataset from the Organic Materials Database (OMDB) of bulk organic crystals. The dataset has to be manually downloaded from https://omdb.mathub.io/dataset,  then unzipped and  its file content placed in the  OrganicMaterialsDB  folder. The dataset contains the following files: Filename Description structures.xyz 12500 crystal structures. Use the first 10000 as training examples and the remaining 2500 as test set. bandgaps.csv 12500 DFT band gaps corresponding to structures.xyz CODids.csv 12500 COD ids cross referencing the Crystallographic Open Database (in the same order as structures.xyz) Please cite the paper introducing this dataset: https://arxiv.org/abs/1810.12814 source"},{"id":898,"pagetitle":"Graphs","title":"MLDatasets.PolBlogs","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.PolBlogs","content":" MLDatasets.PolBlogs  —  Type PolBlogs(; dir=nothing) The Political Blogs dataset from the  The Political Blogosphere and the 2004 US Election: Divided they Blog  paper. PolBlogs  is a graph with 1,490 vertices (representing political blogs) and 19,025 edges (links between blogs). The links are automatically extracted from a crawl of the front page of the blog.  Each vertex receives a label indicating the political leaning of the blog: liberal or conservative. source"},{"id":899,"pagetitle":"Graphs","title":"MLDatasets.PubMed","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.PubMed","content":" MLDatasets.PubMed  —  Type PubMed(; dir=nothing, reverse_edges=true) The PubMed citation network dataset from Ref. [1]. Nodes represent documents and edges represent citation links. The dataset is designed for the node classification task.  The task is to predict the category of certain paper. The dataset is retrieved from Ref. [2]. References [1]:  Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking [2]:  Planetoid source"},{"id":900,"pagetitle":"Graphs","title":"MLDatasets.Reddit","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.Reddit","content":" MLDatasets.Reddit  —  Type Reddit(; full=true, dir=nothing) The Reddit dataset was introduced in Ref [1]. It is a graph dataset of Reddit posts made in the month of September, 2014. The dataset contains a single post-to-post graph, connecting posts if the same user comments on both.  The node label in this case is one of the 41 communities, or “subreddit”s, that a post belongs to.   This dataset contains 232,965 posts. The first 20 days are used for training and the remaining days for testing (with 30% used for validation). Each node is represented by a 602 word vector. Use  full=false  to load only a subsample of the dataset. References [1]:  Inductive Representation Learning on Large Graphs [2]:  Benchmarks on the Reddit Dataset source"},{"id":901,"pagetitle":"Graphs","title":"MLDatasets.TUDataset","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.TUDataset","content":" MLDatasets.TUDataset  —  Type TUDataset(name; dir=nothing) A variety of graph benchmark datasets,  .e.g.  \"QM9\", \"IMDB-BINARY\", \"REDDIT-BINARY\" or \"PROTEINS\", collected from the  TU Dortmund University . Retrieve from the TUDataset collection the dataset  name , where  name  is any of the datasets available  here .  A  TUDataset  object can be indexed to retrieve a specific graph or a subset of graphs. See  here  for an in-depth  description of the format.  Usage Example julia> data = TUDataset(\"PROTEINS\")\ndataset TUDataset:\n  name        =>    PROTEINS\n  metadata    =>    Dict{String, Any} with 1 entry\n  graphs      =>    1113-element Vector{MLDatasets.Graph}\n  graph_data  =>    (targets = \"1113-element Vector{Int64}\",)\n  num_nodes   =>    43471\n  num_edges   =>    162088\n  num_graphs  =>    1113\n\njulia> data[1]\n(graphs = Graph(42, 162), targets = 1) source"},{"id":902,"pagetitle":"Graphs","title":"MLDatasets.METRLA","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.METRLA","content":" MLDatasets.METRLA  —  Type METRLA(; num_timesteps_in::Int = 12, num_timesteps_out::Int=12, dir=nothing, normalize = true) The METR-LA dataset from the  Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting  paper. METRLA  is a graph with 207 nodes representing traffic sensors in Los Angeles.  The edge weights  w  are contained as a feature array in  edge_data  and represent the distance between the sensors.  The node features are the traffic speed and the time of the measurements collected by the sensors, divided into  num_timesteps_in  time steps.  The target values are the traffic speed of the measurements collected by the sensors, divided into  num_timesteps_out  time steps. The  normalize  flag indicates whether the data are normalized using Z-score normalization. source"},{"id":903,"pagetitle":"Graphs","title":"MLDatasets.PEMSBAY","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.PEMSBAY","content":" MLDatasets.PEMSBAY  —  Type PEMSBAY(; num_timesteps_in::Int = 12, num_timesteps_out::Int=12, dir=nothing, normalize = true) The PEMS-BAY dataset described in the  Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting  paper. It is collected by California Transportation Agencies (Cal- Trans) Performance Measurement System (PeMS). PEMSBAY  is a graph with 325 nodes representing traffic sensors in the Bay Area.  The edge weights  w  are contained as a feature array in  edge_data  and represent the distance between the sensors.  The node features are the traffic speed and the time of the measurements collected by the sensors, divided into  num_timesteps_in  time steps.  The target values are the traffic speed of the measurements collected by the sensors, divided into  num_timesteps_out  time steps. The  normalize  flag indicates whether the data are normalized using Z-score normalization. source"},{"id":904,"pagetitle":"Graphs","title":"MLDatasets.TemporalBrains","ref":"/mldatasets/stable/datasets/graphs/#MLDatasets.TemporalBrains","content":" MLDatasets.TemporalBrains  —  Type TemporalBrains(; dir = nothing, threshold_value = 0.6) The TemporalBrains dataset contains a collection of temporal brain networks (as  TemporalSnapshotsGraph s) of 1000 subjects obtained from resting-state fMRI data from the  Human Connectome Project (HCP) . The number of nodes is fixed for each of the 27 snapshots at 102, while the edges change over time. For each  Graph  snapshot, the feature of a node represents the average activation of the node during that snapshot and it is contained in  Graphs.node_data . Each  TemporalSnapshotsGraph  has a label representing their gender (\"M\" for male and \"F\" for female) and age range (22-25, 26-30, 31-35 and 36+) contained as a named tuple in  graph_data . The  threshold_value  is used to binarize the edge weights and is set to 0.6 by default. source"},{"id":907,"pagetitle":"Meshes","title":"Mesh Datasets","ref":"/mldatasets/stable/datasets/meshes/#Mesh-Datasets","content":" Mesh Datasets Mesh datasets contains data in the form of  Meshes.Mesh . See  Meshes.jl  for a better understanding of Meshes."},{"id":908,"pagetitle":"Meshes","title":"Index","ref":"/mldatasets/stable/datasets/meshes/#Index","content":" Index MLDatasets.FAUST"},{"id":909,"pagetitle":"Meshes","title":"Documentation","ref":"/mldatasets/stable/datasets/meshes/#Documentation","content":" Documentation"},{"id":910,"pagetitle":"Meshes","title":"MLDatasets.FAUST","ref":"/mldatasets/stable/datasets/meshes/#MLDatasets.FAUST","content":" MLDatasets.FAUST  —  Type FAUST(split=:train; dir=nothing) The MPI FAUST dataset (2014). FAUST contains 300 real, high-resolution human scans of 10 different subjects in 30 different poses, with automatically computed ground-truth correspondences. Each scan is a high-resolution, triangulated, non-watertight mesh acquired with a 3D multi-stereo system. FAUST is subdivided into a training and a test set. The training set includes 100 scans (10 per subject) with their corresponding ground-truth alignments. The test set includes 200 scans. The FAUST benchmark defines 100 preselected scan pairs, partitioned into two classes – 60 requiring intra-subject matching, 40 requiring inter-subject matching. The dataset required to be downloaded manually from the  website  and extracted in the correct location. For information about where to place the dataset, refer to the example section. Dataset Variables scans : Vector of non-watertight scans in the form of  Mesh . registrations : Vector of registrations corresponding to each scan in  scans .  registrations  like  scans  are also in the form of  Mesh . labels : For each scan in the training set, we provide the boolean Vector of length equal to the number of vertices in the corresponding scan. It represents which vertices were reliably registered by the corresponding registration. metadata : A dictionary containing additional information on the dataset. Currently only  :test  split has metadata containing information about the registrations required for the inter and intra challenge proposed by the author. Examples Loading the dataset julia> using MLDatasets\n\njulia> dataset = FAUST()\n[ Info: This program requested access to the data dependency MPI-FAUST\n[ Info: It could not be found on your system. It requires manual installation.\n┌ Info: Please install it to one of the directories in the DataDeps load path: /home/user/.julia/packages/DataDeps/EDWdQ/deps/data/MPI-FAUST,\n│ /home/user/.julia/datadeps/MPI-FAUST,\n│ /home/user/.julia/juliaup/julia-1.7.3+0.x86/local/share/julia/datadeps/MPI-FAUST,\n│ /home/user/.julia/juliaup/julia-1.7.3+0.x86/share/julia/datadeps/MPI-FAUST,\n│ /home/user/datadeps/MPI-FAUST,\n│ /scratch/datadeps/MPI-FAUST,\n│ /staging/datadeps/MPI-FAUST,\n│ /usr/share/datadeps/MPI-FAUST,\n└ or /usr/local/share/datadeps/MPI-FAUST\n[ Info: by following the instructions:\n┌ Info: Dataset: MPI-FAUST.\n└ Website: http://faust.is.tue.mpg.de/\nOnce installed please enter 'y' reattempt loading, or 'a' to abort\n[y/a] Now download and extract the dataset into one of the given locations. For unix link systems, an example command can be unzip -q <path-to-filename</filename.zip ~/.julia/datadeps The corresponding folder tree should look like ├── test\n│   ├── challenge_pairs\n│   └── scans\n└── training\n    ├── ground_truth_vertices\n    ├── registrations\n    └── scans Press  y  to re-attept loading. dataset FAUST:\n  scans          =>    100-element Vector{Any}\n  registrations  =>    100-element Vector{Any}\n  labels         =>    100-element Vector{Vector{Bool}}\n  metadata       =>    Dict{String, Any} with 0 entries Load train and test split julia> train_faust = FAUST(:train)\ndataset FAUST:\n  scans          =>    100-element Vector{Any}\n  registrations  =>    100-element Vector{Any}\n  labels         =>    100-element Vector{Vector{Bool}}\n  metadata       =>    Dict{String, Any} with 0 entries\n\njulia> test_faust = FAUST(:test)\ndataset FAUST:\n  scans          =>    200-element Vector{Any}\n  registrations  =>    0-element Vector{Any}\n  labels         =>    0-element Vector{Vector{Bool}}\n  metadata       =>    Dict{String, Any} with 2 entries Scan, registrations and ground-truth julia> dataset = FAUST(); # defaults to train split\n\njulia> scan = dataset.scans[1] # pick one scan\nMesh{3, Float32, Triangle}:\n Triangle(Float32[-0.0045452323, 0.08537669, 0.22134435], Float32[-0.0030340434, 0.08542955, 0.22206494],\nFloat32[-0.0042151767, 0.08697654, 0.22171047])\n Triangle(Float32[-0.05358432, 0.08490027, 0.17748278], Float32[-0.05379858, 0.083174236, 0.17670263],\nFloat32[-0.052645437, 0.08346437, 0.17816517])\n.\n.\n.\n Triangle(Float32[-0.07851, -1.0956081, 0.07093428], Float32[-0.06905176, -1.0986279, 0.07775441],\nFloat32[-0.069199145, -1.0928112, 0.06812464])\n\njulia> registration = dataset.registrations[1] # The corresponding registration\nMesh{3, Float32, Triangle}:\n Triangle(Float32[0.12491254, 0.51199615, 0.29041073], Float32[0.11376736, 0.5156298, 0.3007352],\nFloat32[0.119374536, 0.50043654, 0.29687837])\n Triangle(Float32[0.119374536, 0.50043654, 0.29687837], Float32[0.11376736, 0.5156298, 0.3007352],\nFloat32[0.10888693, 0.5008964, 0.30557302])\n.\n.\n.\n Triangle(Float32[0.033744745, 0.030968456, 0.2359996], Float32[0.058017172, 0.044458304, 0.23422624],\nFloat32[0.03615713, 0.04858183, 0.23596591])\n\njulia> label = dataset.labels[1] # The ground-truth/labels for each vertices in scan\n176387-element Vector{Bool}:\n 1\n 1\n 1\n .\n .\n .\n 0\n 0\n 0 References MPI Faust Website Bogo, Federica & Romero, Javier & Loper, Matthew & Black, Michael. (2014). FAUST: Dataset and evaluation for 3D mesh registration. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 10.1109/CVPR.2014.491. source"},{"id":913,"pagetitle":"Miscellaneous","title":"Miscellaneuous Datasets","ref":"/mldatasets/stable/datasets/misc/#Miscellaneuous-Datasets","content":" Miscellaneuous Datasets"},{"id":914,"pagetitle":"Miscellaneous","title":"Index","ref":"/mldatasets/stable/datasets/misc/#Index","content":" Index MLDatasets.BostonHousing MLDatasets.Iris MLDatasets.Mutagenesis MLDatasets.Titanic MLDatasets.Wine"},{"id":915,"pagetitle":"Miscellaneous","title":"Documentation","ref":"/mldatasets/stable/datasets/misc/#Documentation","content":" Documentation"},{"id":916,"pagetitle":"Miscellaneous","title":"MLDatasets.BostonHousing","ref":"/mldatasets/stable/datasets/misc/#MLDatasets.BostonHousing","content":" MLDatasets.BostonHousing  —  Type BostonHousing(; as_df = true, dir = nothing) The classical Boston Housing tabular dataset. Sources:    (a) Origin:  This dataset was taken from the StatLib library which is                 maintained at Carnegie Mellon University.    (b) Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the                   demand for clean air', J. Environ. Economics & Management,                  vol.5, 81-102, 1978.    (c) Date: July 7, 1993 Number of Instances: 506 Number of Attributes: 13 continuous attributes (including target                             attribute \"MEDV\"), 1 binary-valued attribute. Arguments If  as_df = true , load the data as dataframes instead of plain arrays. You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. Fields metadata : A dictionary containing additional information on the dataset. features : The data features. An array if  as_df=false , otherwise a dataframe. targets : The targets for supervised learning. An array if  as_df=false , otherwise a dataframe. dataframe : A dataframe containing both  features  and  targets . It is  nothing  if  as_df=false , otherwise a dataframed. Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. Examples julia> using MLDatasets: BostonHousing\n\njulia> dataset = BostonHousing()\nBostonHousing:\n  metadata => Dict{String, Any} with 5 entries\n  features => 506×13 DataFrame\n  targets => 506×1 DataFrame\n  dataframe => 506×14 DataFrame\n\n\njulia> dataset[1:5][1]\n5×13 DataFrame\n Row │ CRIM     ZN       INDUS    CHAS   NOX      RM       AGE      DIS      RAD    TAX    PTRATIO  B        LSTAT   \n     │ Float64  Float64  Float64  Int64  Float64  Float64  Float64  Float64  Int64  Int64  Float64  Float64  Float64 \n─────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09        1    296     15.3   396.9      4.98\n   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671      2    242     17.8   396.9      9.14\n   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671      2    242     17.8   392.83     4.03\n   4 │ 0.03237      0.0     2.18      0    0.458    6.998     45.8   6.0622      3    222     18.7   394.63     2.94\n   5 │ 0.06905      0.0     2.18      0    0.458    7.147     54.2   6.0622      3    222     18.7   396.9      5.33\n\njulia> dataset[1:5][2]\n5×1 DataFrame\nRow │ MEDV    \n    │ Float64 \n────┼─────────\n  1 │    24.0\n  2 │    21.6\n  3 │    34.7\n  4 │    33.4\n  5 │    36.2  \n\njulia> X, y = BostonHousing(as_df=false)[:]\n([0.00632 0.02731 … 0.10959 0.04741; 18.0 0.0 … 0.0 0.0; … ; 396.9 396.9 … 393.45 396.9; 4.98 9.14 … 6.48 7.88], [24.0 21.6 … 22.0 11.9]) source"},{"id":917,"pagetitle":"Miscellaneous","title":"MLDatasets.Iris","ref":"/mldatasets/stable/datasets/misc/#MLDatasets.Iris","content":" MLDatasets.Iris  —  Type Iris(; as_df = true, dir = nothing) Fisher's classic iris dataset.  Measurements from 3 different species of iris: setosa, versicolor and virginica. There are 50 examples of each species. There are 4 measurements for each example: sepal length, sepal width, petal length and petal width.  The measurements are in centimeters. The module retrieves the data from the  UCI Machine Learning Repository . NOTE: no pre-defined train-test split for this dataset.  Arguments If  as_df = true , load the data as dataframes instead of plain arrays. You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. Fields metadata : A dictionary containing additional information on the dataset. features : The data features. An array if  as_df=false , otherwise a dataframe. targets : The targets for supervised learning. An array if  as_df=false , otherwise a dataframe. dataframe : A dataframe containing both  features  and  targets . It is  nothing  if  as_df=false , otherwise a dataframed. Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. Examples julia> dataset = Iris()\nIris:\n  metadata => Dict{String, Any} with 4 entries\n  features => 150×4 DataFrame\n  targets => 150×1 DataFrame\n  dataframe => 150×5 DataFrame\n\n\njulia> dataset[1:2]\n(2×4 DataFrame\n Row │ sepallength  sepalwidth  petallength  petalwidth \n     │ Float64      Float64     Float64      Float64    \n─────┼──────────────────────────────────────────────────\n   1 │         5.1         3.5          1.4         0.2\n   2 │         4.9         3.0          1.4         0.2, 2×1 DataFrame\n Row │ class       \n     │ String15    \n─────┼─────────────\n   1 │ Iris-setosa\n   2 │ Iris-setosa)\n\njulia> X, y = Iris(as_df=false)[:]\n([5.1 4.9 … 6.2 5.9; 3.5 3.0 … 3.4 3.0; 1.4 1.4 … 5.4 5.1; 0.2 0.2 … 2.3 1.8], InlineStrings.String15[\"Iris-setosa\" \"Iris-setosa\" … \"Iris-virginica\" \"Iris-virginica\"]) source"},{"id":918,"pagetitle":"Miscellaneous","title":"MLDatasets.Mutagenesis","ref":"/mldatasets/stable/datasets/misc/#MLDatasets.Mutagenesis","content":" MLDatasets.Mutagenesis  —  Type Mutagenesis(; split=:train, dir=nothing)\nMutagenesis(split; dir=nothing) The  Mutagenesis  dataset comprises 188 molecules trialed for mutagenicity on Salmonella typhimurium, available from   relational.fit.cvut.cz  and   CTUAvastLab/datasets . Set  split  to  :train ,  :val ,  :test , or  :all , to select the training,  validation, test partition respectively or the whole dataset. The  indexes  field in the result contains the indexes of the partition in the full dataset. Website: https://relational.fit.cvut.cz/dataset/Mutagenesis License: CC0 julia> using MLDatasets: Mutagenesis\n\njulia> dataset = Mutagenesis(:train)\nMutagenesis dataset:\n  split : train\n  indexes : 100-element Vector{Int64}\n  features : 100-element Vector{Dict{Symbol, Any}}\n  targets : 100-element Vector{Int64}\n\njulia> dataset[1].features\nDict{Symbol, Any} with 5 entries:\n  :lumo  => -1.246\n  :inda  => 0\n  :logp  => 4.23\n  :ind1  => 1\n  :atoms => Dict{Symbol, Any}[Dict(:element=>\"c\", :bonds=>Dict{Symbol, Any}[Dict(:element=>\"c\", :bond_type=>7, :charge=>-0.117, :atom_type=>22), Dict(:element=>\"h\", :bond_type=>1, :charge=>0.142, :atom_type=>3)…\n\njulia> dataset[1].targets\n1\n\njulia> dataset = Mutagenesis(:all)\nMutagenesis dataset:\n  split : all\n  indexes : 188-element Vector{Int64}\n  features : 188-element Vector{Dict{Symbol, Any}}\n  targets : 188-element Vector{Int64} source"},{"id":919,"pagetitle":"Miscellaneous","title":"MLDatasets.Titanic","ref":"/mldatasets/stable/datasets/misc/#MLDatasets.Titanic","content":" MLDatasets.Titanic  —  Type Titanic(; as_df = true, dir = nothing) The Titanic dataset, describing the survival of passengers on the Titanic ship. Arguments If  as_df = true , load the data as dataframes instead of plain arrays. You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. Fields metadata : A dictionary containing additional information on the dataset. features : The data features. An array if  as_df=false , otherwise a dataframe. targets : The targets for supervised learning. An array if  as_df=false , otherwise a dataframe. dataframe : A dataframe containing both  features  and  targets . It is  nothing  if  as_df=false , otherwise a dataframed. Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. Examples julia> using MLDatasets: Titanic\n\njulia> using DataFrames\n\njulia> dataset = Titanic()\nTitanic:\n  metadata => Dict{String, Any} with 5 entries\n  features => 891×11 DataFrame\n  targets => 891×1 DataFrame\n  dataframe => 891×12 DataFrame\n\n\njulia> describe(dataset.dataframe)\n12×7 DataFrame\n Row │ variable     mean      min                  median   max                          nmissing  eltype                   \n     │ Symbol       Union…    Any                  Union…   Any                          Int64     Type                     \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ PassengerId  446.0     1                    446.0    891                                 0  Int64\n   2 │ Survived     0.383838  0                    0.0      1                                   0  Int64\n   3 │ Pclass       2.30864   1                    3.0      3                                   0  Int64\n   4 │ Name                   Abbing, Mr. Anthony           van Melkebeke, Mr. Philemon         0  String\n   5 │ Sex                    female                        male                                0  String7\n   6 │ Age          29.6991   0.42                 28.0     80.0                              177  Union{Missing, Float64}\n   7 │ SibSp        0.523008  0                    0.0      8                                   0  Int64\n   8 │ Parch        0.381594  0                    0.0      6                                   0  Int64\n   9 │ Ticket                 110152                        WE/P 5735                           0  String31\n  10 │ Fare         32.2042   0.0                  14.4542  512.329                             0  Float64\n  11 │ Cabin                  A10                           T                                 687  Union{Missing, String15}\n  12 │ Embarked               C                             S                                   2  Union{Missing, String1} source"},{"id":920,"pagetitle":"Miscellaneous","title":"MLDatasets.Wine","ref":"/mldatasets/stable/datasets/misc/#MLDatasets.Wine","content":" MLDatasets.Wine  —  Type Wine(; as_df = true, dir = nothing) The UCI Wine dataset. These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. Data source is the  UCI Machine Learning Repository  where further details can be retrieved. Arguments If  as_df = true , load the data as dataframes instead of plain arrays. You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. Fields metadata : A dictionary containing additional information on the dataset. features : The data features. An array if  as_df=false , otherwise a dataframe. targets : The targets for supervised learning. An array if  as_df=false , otherwise a dataframe. dataframe : A dataframe containing both  features  and  targets . It is  nothing  if  as_df=false , otherwise a dataframed. Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. Examples julia> using MLDatasets: Wine\n\njulia> using DataFrames\n\njulia> dataset = Wine()\ndataset Wine:\n  metadata   =>    Dict{String, Any} with 5 entries\n  features   =>    178×13 DataFrame\n  targets    =>    178×1 DataFrame\n  dataframe  =>    178×14 DataFrame\n\n\njulia> describe(dataset.dataframe)\n14×7 DataFrame\n Row │ variable              mean        min     median   max      nmissing  eltype   \n     │ Symbol                Float64     Real    Float64  Real     Int64     DataType \n─────┼────────────────────────────────────────────────────────────────────────────────\n   1 │ Wine                    1.9382      1       2.0       3            0  Int64\n   2 │ Alcohol                13.0006     11.03   13.05     14.83         0  Float64\n   3 │ Malic.acid              2.33635     0.74    1.865     5.8          0  Float64\n   4 │ Ash                     2.36652     1.36    2.36      3.23         0  Float64\n   5 │ Acl                    19.4949     10.6    19.5      30.0          0  Float64\n   6 │ Mg                     99.7416     70      98.0     162            0  Int64\n   7 │ Phenols                 2.29511     0.98    2.355     3.88         0  Float64\n   8 │ Flavanoids              2.02927     0.34    2.135     5.08         0  Float64\n   9 │ Nonflavanoid.phenols    0.361854    0.13    0.34      0.66         0  Float64\n  10 │ Proanth                 1.5909      0.41    1.555     3.58         0  Float64\n  11 │ Color.int               5.05809     1.28    4.69     13.0          0  Float64\n  12 │ Hue                     0.957449    0.48    0.965     1.71         0  Float64\n  13 │ OD                      2.61169     1.27    2.78      4.0          0  Float64\n  14 │ Proline               746.893     278     673.5    1680            0  Int64 source"},{"id":923,"pagetitle":"Text","title":"Text Datasets","ref":"/mldatasets/stable/datasets/text/#Text-Datasets","content":" Text Datasets"},{"id":924,"pagetitle":"Text","title":"Index","ref":"/mldatasets/stable/datasets/text/#Index","content":" Index MLDatasets.PTBLM MLDatasets.SMSSpamCollection MLDatasets.UD_English"},{"id":925,"pagetitle":"Text","title":"Documentation","ref":"/mldatasets/stable/datasets/text/#Documentation","content":" Documentation"},{"id":926,"pagetitle":"Text","title":"MLDatasets.PTBLM","ref":"/mldatasets/stable/datasets/text/#MLDatasets.PTBLM","content":" MLDatasets.PTBLM  —  Type PTBLM(; split=:train, dir=nothing)\nPTBLM(split; [dir]) The PTBLM dataset consists of Penn Treebank sentences for language modeling, available from https://github.com/tomsercu/lstm. The unknown words are replaced with <unk> so that the total vocabulary size becomes 10000. source"},{"id":927,"pagetitle":"Text","title":"MLDatasets.SMSSpamCollection","ref":"/mldatasets/stable/datasets/text/#MLDatasets.SMSSpamCollection","content":" MLDatasets.SMSSpamCollection  —  Type SMSSpamCollection(; dir=nothing) The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages  that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according being ham (legitimate) or spam. The corpus has a total of 4,827 SMS legitimate messages (86.6%) and a total of 747 (13.4%) spam messages. The corpus has been collected by Tiago Agostinho de Almeida (http://www.dt.fee.unicamp.br/~tiago)  and José María Gómez Hidalgo (http://www.esp.uem.es/jmgomez). ```julia-repl julia> using MLDatasets: SMSSpamCollection julia> targets = SMSSpamCollection.targets(); julia> summary(targets) \"5574-element Vector{Any}\" julia> targets[1] \"ham\" julia> summary(features) \"5574-element Vector{Any}\" source"},{"id":928,"pagetitle":"Text","title":"MLDatasets.UD_English","ref":"/mldatasets/stable/datasets/text/#MLDatasets.UD_English","content":" MLDatasets.UD_English  —  Type UD_English(; split=:train, dir=nothing)\nUD_English(split=; [dir]) A Gold Standard Universal Dependencies Corpus for English, built over the source material of the English Web Treebank LDC2012T13 (https://catalog.ldc.upenn.edu/LDC2012T13). The corpus comprises 254,825 words and 16,621 sentences,  taken from five genres of web media: weblogs, newsgroups, emails, reviews, and Yahoo! answers.  See the LDC2012T13 documentation for more details on the sources of the sentences.  The trees were automatically converted into Stanford Dependencies and then hand-corrected to Universal Dependencies.  All the basic dependency annotations have been single-annotated, a limited portion of them have been double-annotated,  and subsequent correction has been done to improve consistency. Other aspects of the treebank, such as Universal POS,  features and enhanced dependencies, has mainly been done automatically, with very limited hand-correction. Authors: Natalia Silveira and Timothy Dozat and             Marie-Catherine de Marneffe and Samuel             Bowman and Miriam Connor and John Bauer and             Christopher D. Manning Website: https://github.com/UniversalDependencies/UD_English-EWT source"},{"id":931,"pagetitle":"Vision","title":"Vision Datasets","ref":"/mldatasets/stable/datasets/vision/#Vision-Datasets","content":" Vision Datasets A collection of datasets for 2d computer vision.  Numerical arrays can be converted to color images using   convert2image , and displayed in the terminal with the package  ImageInTerminal.jl"},{"id":932,"pagetitle":"Vision","title":"Index","ref":"/mldatasets/stable/datasets/vision/#Index","content":" Index MLDatasets.CIFAR10 MLDatasets.CIFAR100 MLDatasets.EMNIST MLDatasets.FashionMNIST MLDatasets.MNIST MLDatasets.Omniglot MLDatasets.SVHN2 MLDatasets.convert2image"},{"id":933,"pagetitle":"Vision","title":"Documentation","ref":"/mldatasets/stable/datasets/vision/#Documentation","content":" Documentation"},{"id":934,"pagetitle":"Vision","title":"MLDatasets.convert2image","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.convert2image","content":" MLDatasets.convert2image  —  Function convert2image(d, i)\nconvert2image(d, x)\nconvert2image(DType, x) Convert the observation(s)  i  from dataset  d  to image(s). It can also convert a numerical array  x . In order to support a new dataset, e.g.  MyDataset ,  implement  convert2image(::Type{MyDataset}, x::AbstractArray) . Examples julia> using MLDatasets, ImageInTerminal\n\njulia> d = MNIST()\n\njulia> convert2image(d, 1:2) \n# You should see 2 images in the terminal\n\njulia> x = d[1].features;\n\njulia> convert2image(MNIST, x) # or convert2image(d, x) source"},{"id":935,"pagetitle":"Vision","title":"MLDatasets.CIFAR10","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.CIFAR10","content":" MLDatasets.CIFAR10  —  Type CIFAR10(; Tx=Float32, split=:train, dir=nothing)\nCIFAR10([Tx, split]) The CIFAR10 dataset is a labeled subsets of the 80 million tiny images dataset. It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. Arguments You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. split : selects the data partition. Can take the values  :train  or  :test .  Fields metadata : A dictionary containing additional information on the dataset. features : An array storing the data features. targets : An array storing the targets for supervised learning. split . Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. convert2image  converts features to  RGB  images. Examples julia> using MLDatasets: CIFAR10\n\njulia> dataset = CIFAR10()\nCIFAR10:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :train\n  features    =>    32×32×3×50000 Array{Float32, 4}\n  targets     =>    50000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n 6\n 9\n 9\n 4\n 1\n\njulia> X, y = dataset[:];\n\njulia> dataset = CIFAR10(Tx=Float64, split=:test)\nCIFAR10:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :test\n  features    =>    32×32×3×10000 Array{Float64, 4}\n  targets     =>    10000-element Vector{Int64}\n\njulia> dataset.metadata\nDict{String, Any} with 2 entries:\n  \"n_observations\" => 10000\n  \"class_names\"    => [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"] source"},{"id":936,"pagetitle":"Vision","title":"MLDatasets.CIFAR100","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.CIFAR100","content":" MLDatasets.CIFAR100  —  Type CIFAR100(; Tx=Float32, split=:train, dir=nothing)\nCIFAR100([Tx, split]) The CIFAR100 dataset is a labeled subsets of the 80 million tiny images dataset. It consists of 60000 32x32 colour images in 100 classes and 20 superclasses, with 600 images per class. Return the CIFAR-100  trainset  labels (coarse and fine) corresponding to the given  indices  as a tuple of two  Int  or two  Vector{Int} . The variables returned are the coarse label(s) ( Yc ) and the fine label(s) ( Yf ) respectively. Arguments You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. split : selects the data partition. Can take the values  :train  or  :test .  Fields metadata : A dictionary containing additional information on the dataset. features : An array storing the data features. targets : An array storing the targets for supervised learning. split . Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. convert2image  converts features to  RGB  images. Examples julia> dataset = CIFAR100()\nCIFAR100:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    32×32×3×50000 Array{Float32, 4}\n  targets     =>    (coarse = \"50000-element Vector{Int64}\", fine = \"50000-element Vector{Int64}\")\n\njulia> dataset[1:5].targets\n(coarse = [11, 15, 4, 14, 1], fine = [19, 29, 0, 11, 1])\n\njulia> X, y = dataset[:];\n\njulia> dataset.metadata\nDict{String, Any} with 3 entries:\n  \"n_observations\"     => 50000\n  \"class_names_coarse\" => [\"aquatic_mammals\", \"fish\", \"flowers\", \"food_containers\", \"fruit_and_vegetables\", \"household_electrical_devices\", \"household_furniture\", \"insects\", \"large_carnivores\", \"large_man-made_…\n  \"class_names_fine\"   => [\"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\"  …  \"train\", \"trout\", \"tulip\", \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"w… source"},{"id":937,"pagetitle":"Vision","title":"MLDatasets.EMNIST","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.EMNIST","content":" MLDatasets.EMNIST  —  Type EMNIST(name; Tx=Float32, split=:train, dir=nothing)\nEMNIST(name, [Tx, split]) The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 (https://www.nist.gov/srd/nist-special-database-19) and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset (http://yann.lecun.com/exdb/mnist/). Further information on the dataset contents and conversion process can be found in the paper available at https://arxiv.org/abs/1702.05373v1. Arguments name : name of the EMNIST dataset. Possible values are:  :balanced, :byclass, :bymerge, :digits, :letters, :mnist . split : selects the data partition. Can take the values  :train  or  :test .  You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. Fields name . split . metadata : A dictionary containing additional information on the dataset. features : An array storing the data features. targets : An array storing the targets for supervised learning. Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. convert2image  converts features to  Gray  images. Examples The images are loaded as a multi-dimensional array of eltype  Tx . If  Tx <: Integer , then all values will be within  0  and  255 ,  otherwise the values are scaled to be between  0  and  1 .  EMNIST().features  is a 3D array (i.e. a  Array{Tx,3} ), in WHN format (width, height, num_images). Labels are stored as a vector of integers in  EMNIST().targets .  julia> using MLDatasets: EMNIST\n\njulia> dataset = EMNIST(:letters, split=:train)\nEMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n7\n2\n1\n0\n4\n\njulia> X, y = dataset[:];\n\njulia> dataset = EMNIST(:balanced, Tx=UInt8, split=:test)\nEMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    28×28×10000 Array{UInt8, 3}\n  targets     =>    10000-element Vector{Int64} source"},{"id":938,"pagetitle":"Vision","title":"MLDatasets.FashionMNIST","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.FashionMNIST","content":" MLDatasets.FashionMNIST  —  Type FashionMNIST(; Tx=Float32, split=:train, dir=nothing)\nFashionMNIST([Tx, split]) FashionMNIST is a dataset of Zalando's article images consisting of a training set of 60000 examples and a test set of 10000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. It can serve as a drop-in replacement for MNIST. Authors: Han Xiao, Kashif Rasul, Roland Vollgraf Website: https://github.com/zalandoresearch/fashion-mnist See  MNIST  for details of the interface. source"},{"id":939,"pagetitle":"Vision","title":"MLDatasets.MNIST","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.MNIST","content":" MLDatasets.MNIST  —  Type MNIST(; Tx=Float32, split=:train, dir=nothing)\nMNIST([Tx, split]) The MNIST database of handwritten digits. Authors: Yann LeCun, Corinna Cortes, Christopher J.C. Burges Website: http://yann.lecun.com/exdb/mnist/ MNIST is a classic image-classification dataset that is often used in small-scale machine learning experiments. It contains 70,000 images of handwritten digits. Each observation is a 28x28 pixel gray-scale image that depicts a handwritten version of 1 of the 10 possible digits (0-9). Arguments You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. split : selects the data partition. Can take the values  :train  or  :test .  Fields metadata : A dictionary containing additional information on the dataset. features : An array storing the data features. targets : An array storing the targets for supervised learning. split . Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. convert2image  converts features to  Gray  images. Examples The images are loaded as a multi-dimensional array of eltype  Tx . If  Tx <: Integer , then all values will be within  0  and  255 ,  otherwise the values are scaled to be between  0  and  1 .  MNIST().features  is a 3D array (i.e. a  Array{Tx,3} ), in WHN format (width, height, num_images). Labels are stored as a vector of integers in  MNIST().targets .  julia> using MLDatasets: MNIST\n\njulia> dataset = MNIST(:train)\nMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    28×28×60000 Array{Float32, 3}\n  targets     =>    60000-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n7\n2\n1\n0\n4\n\njulia> X, y = dataset[:];\n\njulia> dataset = MNIST(UInt8, :test)\nMNIST:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    28×28×10000 Array{UInt8, 3}\n  targets     =>    10000-element Vector{Int64} source"},{"id":940,"pagetitle":"Vision","title":"MLDatasets.Omniglot","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.Omniglot","content":" MLDatasets.Omniglot  —  Type Omniglot(; Tx=Float32, split=:train, dir=nothing)\nOmniglot([Tx, split]) Omniglot data set for one-shot learning Authors: Brenden M. Lake, Ruslan Salakhutdinov, Joshua B. Tenenbaum Website: https://github.com/brendenlake/omniglot The Omniglot data set is designed for developing more human-like learning algorithms. It contains 1623 different handwritten characters from 50 different alphabets. Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people. Each image is paired with stroke data, a sequences of [x,y,t] coordinates with time (t) in milliseconds. Arguments You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. split : selects the data partition. Can take the values  :train ,  :test ,  :small1 , or  :small2 .  Fields metadata : A dictionary containing additional information on the dataset. features : An array storing the data features. targets : An array storing the targets for supervised learning. split . Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. convert2image  converts features to  Gray  images. Examples The images are loaded as a multi-dimensional array of eltype  Tx . All values will be  0  or  1 .  Omniglot().features  is a 3D array (i.e. a  Array{Tx,3} ), in WHN format (width, height, num_images). Labels are stored as a vector of strings in  Omniglot().targets .  julia> using MLDatasets: Omniglot\n\njulia> dataset = Omniglot(:train)\nOmniglot:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :train\n  features    =>    105×105×19280 Array{Float32, 3}\n  targets     =>    19280-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{String}:\n \"Arcadian\"\n \"Arcadian\"\n \"Arcadian\"\n \"Arcadian\"\n \"Arcadian\"\n\njulia> X, y = dataset[:];\n\njulia> dataset = Omniglot(UInt8, :test)\nOmniglot:\n  metadata    =>    Dict{String, Any} with 3 entries\n  split       =>    :test\n  features    =>    105×105×13180 Array{UInt8, 3}\n  targets     =>    13180-element Vector{Int64} source"},{"id":941,"pagetitle":"Vision","title":"MLDatasets.SVHN2","ref":"/mldatasets/stable/datasets/vision/#MLDatasets.SVHN2","content":" MLDatasets.SVHN2  —  Type SVHN2(; Tx=Float32, split=:train, dir=nothing)\nSVHN2([Tx, split]) The Street View House Numbers (SVHN) Dataset. Authors: Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Website: http://ufldl.stanford.edu/housenumbers SVHN was obtained from house numbers in Google Street View images. As such they are quite diverse in terms of orientation and image background. Similar to MNIST, SVHN has 10 classes (the digits 0-9), but unlike MNIST there is more data and the images are a little bigger (32x32 instead of 28x28) with an additional RGB color channel. The dataset is split up into three subsets: 73257 digits for training, 26032 digits for testing, and 531131 additional to use as extra training data. Arguments You can pass a specific  dir  where to load or download the dataset, otherwise uses the default one. split : selects the data partition. Can take the values  :train ,  :test  or  :extra .  Fields metadata : A dictionary containing additional information on the dataset. features : An array storing the data features. targets : An array storing the targets for supervised learning. split . Methods dataset[i] : Return observation(s)  i  as a named tuple of features and targets. dataset[:] : Return all observations as a named tuple of features and targets. length(dataset) : Number of observations. convert2image  converts features to  RGB  images. Examples julia> using MLDatasets: SVHN2\n\njulia> using MLDatasets: SVHN2\n\njulia> dataset = SVHN2()\nSVHN2:\n  metadata    =>    Dict{String, Any} with 2 entries\n  split       =>    :train\n  features    =>    32×32×3×73257 Array{Float32, 4}\n  targets     =>    73257-element Vector{Int64}\n\njulia> dataset[1:5].targets\n5-element Vector{Int64}:\n 1\n 9\n 2\n 3\n 2\n\njulia> dataset.metadata\nDict{String, Any} with 2 entries:\n  \"n_observations\" => 73257\n  \"class_names\"    => [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\"] source"},{"id":944,"pagetitle":"Home","title":"Metalhead","ref":"/metalhead/stable/#Metalhead","content":" Metalhead Metalhead.jl  provides standard machine learning vision models for use with  Flux.jl . The architectures in this package make use of pure Flux layers, and they represent the best-practices for creating modules like residual blocks, inception blocks, etc. in Flux. Metalhead also provides some building blocks for more complex models in the Layers module."},{"id":945,"pagetitle":"Home","title":"Installation","ref":"/metalhead/stable/#Installation","content":" Installation julia> ]add Metalhead"},{"id":946,"pagetitle":"Home","title":"Getting Started","ref":"/metalhead/stable/#Getting-Started","content":" Getting Started You can find the Metalhead.jl getting started guide  here ."},{"id":947,"pagetitle":"Home","title":"Available models","ref":"/metalhead/stable/#Available-models","content":" Available models To contribute new models, see our  contributing docs ."},{"id":948,"pagetitle":"Home","title":"Image Classification","ref":"/metalhead/stable/#Image-Classification","content":" Image Classification Model Name Constructor Pre-trained? AlexNet AlexNet N ConvMixer ConvMixer N ConvNeXt ConvNeXt N DenseNet DenseNet N EfficientNet EfficientNet N EfficientNetv2 EfficientNetv2 N gMLP gMLP N GoogLeNet GoogLeNet N Inception-v3 Inceptionv3 N Inception-v4 Inceptionv4 N InceptionResNet-v2 InceptionResNetv2 N MLPMixer MLPMixer N MobileNetv1 MobileNetv1 N MobileNetv2 MobileNetv2 N MobileNetv3 MobileNetv3 N MNASNet MNASNet N ResMLP ResMLP N ResNet ResNet Y ResNeXt ResNeXt Y SqueezeNet SqueezeNet Y Xception Xception N WideResNet WideResNet Y VGG VGG Y Vision Transformer ViT Y"},{"id":949,"pagetitle":"Home","title":"Other Models","ref":"/metalhead/stable/#Other-Models","content":" Other Models Model Name Constructor Pre-trained? UNet UNet N"},{"id":952,"pagetitle":"DenseNet","title":"DenseNet","ref":"/metalhead/stable/api/densenet/#DenseNet","content":" DenseNet This is the API reference for the DenseNet model present in Metalhead.jl."},{"id":953,"pagetitle":"DenseNet","title":"The higher level model","ref":"/metalhead/stable/api/densenet/#The-higher-level-model","content":" The higher level model"},{"id":954,"pagetitle":"DenseNet","title":"Metalhead.DenseNet","ref":"/metalhead/stable/api/densenet/#Metalhead.DenseNet","content":" Metalhead.DenseNet  —  Type DenseNet(config::Int; pretrain = false, growth_rate = 32,\n         reduction = 0.5, inchannels = 3, nclasses = 1000) Create a DenseNet model with specified configuration. Currently supported values are (121, 161, 169, 201) ( reference ). Arguments config : the configuration of the model pretrain : whether to load the model with pre-trained weights for ImageNet. growth_rate : the output feature map growth probability of dense blocks (i.e.  k  in the ref) reduction : the factor by which the number of feature maps is scaled across each transition inchannels : the number of input channels nclasses : the number of output classes Warning DenseNet  does not currently support pretrained weights. See also  Metalhead.densenet . source"},{"id":955,"pagetitle":"DenseNet","title":"The core function","ref":"/metalhead/stable/api/densenet/#The-core-function","content":" The core function"},{"id":956,"pagetitle":"DenseNet","title":"Metalhead.densenet","ref":"/metalhead/stable/api/densenet/#Metalhead.densenet","content":" Metalhead.densenet  —  Function densenet(nblocks::AbstractVector{Int}; growth_rate = 32,\n         reduction = 0.5, dropout_prob = nothing, inchannels = 3,\n         nclasses = 1000) Create a DenseNet model ( reference ). Arguments nblocks : number of dense blocks between transitions growth_rate : the output feature map growth probability of dense blocks (i.e.  k  in the ref) reduction : the factor by which the number of feature maps is scaled across each transition dropout_prob : the dropout probability for the classifier head. Set to  nothing  to disable dropout inchannels : the number of input channels nclasses : the number of output classes source"},{"id":959,"pagetitle":"EfficientNet family of models","title":"EfficientNet family of models","ref":"/metalhead/stable/api/efficientnet/#EfficientNet-family-of-models","content":" EfficientNet family of models This is the API reference for the EfficientNet family of models supported by Metalhead.jl."},{"id":960,"pagetitle":"EfficientNet family of models","title":"Metalhead.EfficientNet","ref":"/metalhead/stable/api/efficientnet/#Metalhead.EfficientNet","content":" Metalhead.EfficientNet  —  Type EfficientNet(config::Symbol; pretrain::Bool = false, inchannels::Integer = 3,\n             nclasses::Integer = 1000) Create an EfficientNet model ( reference ). Arguments config : size of the model. Can be one of  [:b0, :b1, :b2, :b3, :b4, :b5, :b6, :b7, :b8] . pretrain : set to  true  to load the pre-trained weights for ImageNet inchannels : number of input channels. nclasses : number of output classes. Warning EfficientNet does not currently support pretrained weights. See also  Metalhead.efficientnet . source"},{"id":961,"pagetitle":"EfficientNet family of models","title":"Metalhead.EfficientNetv2","ref":"/metalhead/stable/api/efficientnet/#Metalhead.EfficientNetv2","content":" Metalhead.EfficientNetv2  —  Type EfficientNetv2(config::Symbol; pretrain::Bool = false, inchannels::Integer = 3,\n               nclasses::Integer = 1000) Create an EfficientNetv2 model ( reference ). Arguments config : size of the network (one of  [:small, :medium, :large, :xlarge] ) pretrain : whether to load the pre-trained weights for ImageNet inchannels : number of input channels nclasses : number of output classes Warning EfficientNetv2  does not currently support pretrained weights. See also  efficientnet . source"},{"id":964,"pagetitle":"Hybrid CNN architectures","title":"Hybrid CNN architectures","ref":"/metalhead/stable/api/hybrid/#Hybrid-CNN-architectures","content":" Hybrid CNN architectures These models are hybrid CNN architectures that borrow certain ideas from vision transformer models."},{"id":965,"pagetitle":"Hybrid CNN architectures","title":"The higher-level model constructors","ref":"/metalhead/stable/api/hybrid/#The-higher-level-model-constructors","content":" The higher-level model constructors"},{"id":966,"pagetitle":"Hybrid CNN architectures","title":"Metalhead.ConvMixer","ref":"/metalhead/stable/api/hybrid/#Metalhead.ConvMixer","content":" Metalhead.ConvMixer  —  Type ConvMixer(config::Symbol; pretrain::Bool = false, inchannels::Integer = 3,\n          nclasses::Integer = 1000) Creates a ConvMixer model. ( reference ) Arguments config : the size of the model, either  :base ,  :small  or  :large pretrain : whether to load the pre-trained weights for ImageNet inchannels : number of input channels nclasses : number of classes in the output Warning ConvMixer  does not currently support pretrained weights. See also  Metalhead.convmixer . source"},{"id":967,"pagetitle":"Hybrid CNN architectures","title":"Metalhead.ConvNeXt","ref":"/metalhead/stable/api/hybrid/#Metalhead.ConvNeXt","content":" Metalhead.ConvNeXt  —  Type ConvNeXt(config::Symbol; pretrain::Bool = true, inchannels::Integer = 3,\n         nclasses::Integer = 1000) Creates a ConvNeXt model. ( reference ) Arguments config : The size of the model, one of  tiny ,  small ,  base ,  large  or  xlarge . pretrain : set to  true  to load pre-trained weights for ImageNet inchannels : number of input channels nclasses : number of output classes Warning ConvNeXt  does not currently support pretrained weights. See also  Metalhead.convnext . source"},{"id":968,"pagetitle":"Hybrid CNN architectures","title":"The mid-level functions","ref":"/metalhead/stable/api/hybrid/#The-mid-level-functions","content":" The mid-level functions"},{"id":969,"pagetitle":"Hybrid CNN architectures","title":"Metalhead.convmixer","ref":"/metalhead/stable/api/hybrid/#Metalhead.convmixer","content":" Metalhead.convmixer  —  Function convmixer(planes::Integer, depth::Integer; kernel_size::Dims{2} = (9, 9),\n          patch_size::Dims{2} = (7, 7), activation = gelu,\n          inchannels::Integer = 3, nclasses::Integer = 1000) Creates a ConvMixer model. ( reference ) Arguments planes : number of planes in the output of each block depth : number of layers kernel_size : kernel size of the convolutional layers patch_size : size of the patches activation : activation function used after the convolutional layers inchannels : number of input channels nclasses : number of classes in the output source"},{"id":970,"pagetitle":"Hybrid CNN architectures","title":"Metalhead.convnext","ref":"/metalhead/stable/api/hybrid/#Metalhead.convnext","content":" Metalhead.convnext  —  Function convnext(config::Symbol; stochastic_depth_prob = 0.0, layerscale_init = 1.0f-6,\n         inchannels::Integer = 3, nclasses::Integer = 1000) Creates a ConvNeXt model. ( reference ) Arguments config : The size of the model, one of  tiny ,  small ,  base ,  large  or  xlarge . stochastic_depth_prob : Stochastic depth probability. layerscale_init : Initial value for  LayerScale  ( reference ) inchannels : number of input channels. nclasses : number of output classes source"},{"id":973,"pagetitle":"Inception family of models","title":"Inception family of models","ref":"/metalhead/stable/api/inception/#Inception-family-of-models","content":" Inception family of models This is the API reference for the Inception family of models supported by Metalhead.jl."},{"id":974,"pagetitle":"Inception family of models","title":"The higher-level model constructors","ref":"/metalhead/stable/api/inception/#The-higher-level-model-constructors","content":" The higher-level model constructors"},{"id":975,"pagetitle":"Inception family of models","title":"Metalhead.GoogLeNet","ref":"/metalhead/stable/api/inception/#Metalhead.GoogLeNet","content":" Metalhead.GoogLeNet  —  Type GoogLeNet(; pretrain::Bool = false, inchannels::Integer = 3, nclasses::Integer = 1000) Create an Inception-v1 model (commonly referred to as  GoogLeNet ) ( reference ). Arguments pretrain : set to  true  to load the model with pre-trained weights for ImageNet nclasses : the number of output classes batchnorm : set to  true  to use batch normalization after each convolution bias : set to  true  to use bias in the convolution layers Warning GoogLeNet  does not currently support pretrained weights. See also  Metalhead.googlenet . source"},{"id":976,"pagetitle":"Inception family of models","title":"Metalhead.Inceptionv3","ref":"/metalhead/stable/api/inception/#Metalhead.Inceptionv3","content":" Metalhead.Inceptionv3  —  Type Inceptionv3(; pretrain::Bool = false, inchannels::Integer = 3, nclasses::Integer = 1000) Create an Inception-v3 model ( reference ). Arguments pretrain : set to  true  to load the pre-trained weights for ImageNet inchannels : number of input channels nclasses : the number of output classes Warning Inceptionv3  does not currently support pretrained weights. See also  Metalhead.inceptionv3 . source"},{"id":977,"pagetitle":"Inception family of models","title":"Metalhead.Inceptionv4","ref":"/metalhead/stable/api/inception/#Metalhead.Inceptionv4","content":" Metalhead.Inceptionv4  —  Type Inceptionv4(; pretrain::Bool = false, inchannels::Integer = 3,\n            nclasses::Integer = 1000) Creates an Inceptionv4 model. ( reference ) Arguments pretrain : set to  true  to load the pre-trained weights for ImageNet inchannels : number of input channels. nclasses : the number of output classes. Warning Inceptionv4  does not currently support pretrained weights. See also  Metalhead.inceptionv4 . source"},{"id":978,"pagetitle":"Inception family of models","title":"Metalhead.InceptionResNetv2","ref":"/metalhead/stable/api/inception/#Metalhead.InceptionResNetv2","content":" Metalhead.InceptionResNetv2  —  Type InceptionResNetv2(; pretrain::Bool = false, inchannels::Integer = 3, \n                  nclasses::Integer = 1000) Creates an InceptionResNetv2 model. ( reference ) Arguments pretrain : set to  true  to load the pre-trained weights for ImageNet inchannels : number of input channels. nclasses : the number of output classes. Warning InceptionResNetv2  does not currently support pretrained weights. See also  Metalhead.inceptionresnetv2 . source"},{"id":979,"pagetitle":"Inception family of models","title":"Metalhead.Xception","ref":"/metalhead/stable/api/inception/#Metalhead.Xception","content":" Metalhead.Xception  —  Type Xception(; pretrain::Bool = false, inchannels::Integer = 3, nclasses::Integer = 1000) Creates an Xception model. ( reference ) Arguments pretrain : set to  true  to load the pre-trained weights for ImageNet. inchannels : number of input channels. nclasses : the number of output classes. Warning Xception  does not currently support pretrained weights. See also  Metalhead.xception . source"},{"id":980,"pagetitle":"Inception family of models","title":"The mid-level functions","ref":"/metalhead/stable/api/inception/#The-mid-level-functions","content":" The mid-level functions"},{"id":981,"pagetitle":"Inception family of models","title":"Metalhead.googlenet","ref":"/metalhead/stable/api/inception/#Metalhead.googlenet","content":" Metalhead.googlenet  —  Function googlenet(; dropout_prob = 0.4, inchannels::Integer = 3, nclasses::Integer = 1000) Create an Inception-v1 model (commonly referred to as GoogLeNet) ( reference ). Arguments dropout_prob : the dropout probability in the classifier head. Set to  nothing  to disable dropout. inchannels : the number of input channels nclasses : the number of output classes batchnorm : set to  true  to include batch normalization after each convolution bias : set to  true  to use bias in the convolution layers source"},{"id":982,"pagetitle":"Inception family of models","title":"Metalhead.inceptionv3","ref":"/metalhead/stable/api/inception/#Metalhead.inceptionv3","content":" Metalhead.inceptionv3  —  Function inceptionv3(; dropout_prob = 0.2, inchannels::Integer = 3, nclasses::Integer = 1000) Create an Inception-v3 model ( reference ). Arguments dropout_prob : the dropout probability in the classifier head. Set to  nothing  to disable dropout. inchannels : number of input feature maps nclasses : the number of output classes source"},{"id":983,"pagetitle":"Inception family of models","title":"Metalhead.inceptionv4","ref":"/metalhead/stable/api/inception/#Metalhead.inceptionv4","content":" Metalhead.inceptionv4  —  Function inceptionv4(; dropout_prob = nothing, inchannels::Integer = 3, nclasses::Integer = 1000) Create an Inceptionv4 model. ( reference ) Arguments dropout_prob : probability of dropout in classifier head. Set to  nothing  to disable dropout. inchannels : number of input channels. nclasses : the number of output classes. source"},{"id":984,"pagetitle":"Inception family of models","title":"Metalhead.inceptionresnetv2","ref":"/metalhead/stable/api/inception/#Metalhead.inceptionresnetv2","content":" Metalhead.inceptionresnetv2  —  Function inceptionresnetv2(; inchannels::Integer = 3, dropout_prob = nothing, nclasses::Integer = 1000) Creates an InceptionResNetv2 model. ( reference ) Arguments dropout_prob : probability of dropout in classifier head. Set to  nothing  to disable dropout. inchannels : number of input channels. nclasses : the number of output classes. source"},{"id":985,"pagetitle":"Inception family of models","title":"Metalhead.xception","ref":"/metalhead/stable/api/inception/#Metalhead.xception","content":" Metalhead.xception  —  Function xception(; dropout_prob = nothing, inchannels::Integer = 3, nclasses::Integer = 1000) Creates an Xception model. ( reference ) Arguments dropout_prob : probability of dropout in classifier head. Set to  nothing  to disable dropout. inchannels : number of input channels. nclasses : the number of output classes. source"},{"id":988,"pagetitle":"Layers","title":"Layers","ref":"/metalhead/stable/api/layers/#Layers","content":" Layers Metalhead also defines a module called  Layers  which contains some custom layers that are used to configure the models in Metalhead. These layers are not available in Flux at present. To use the functions defined in the  Layers  module, you need to import it. using Metalhead: Layers This page contains the API reference for the  Layers  module. Warning The  Layers  module is still a work in progress. While we will endeavour to keep the API stable, we cannot guarantee that it will not change in the future. If you find any of the functions in this module do not work as expected, please open an issue on GitHub."},{"id":989,"pagetitle":"Layers","title":"Convolution + BatchNorm layers","ref":"/metalhead/stable/api/layers/#Convolution-BatchNorm-layers","content":" Convolution + BatchNorm layers"},{"id":990,"pagetitle":"Layers","title":"Metalhead.Layers.conv_norm","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.conv_norm","content":" Metalhead.Layers.conv_norm  —  Function conv_norm(kernel_size::Dims{2}, inplanes::Integer, outplanes::Integer,\n          activation = relu; norm_layer = BatchNorm, revnorm::Bool = false,\n          preact::Bool = false, stride::Integer = 1, pad::Integer = 0,\n          dilation::Integer = 1, groups::Integer = 1, [bias, weight, init]) Create a convolution + normalisation layer pair with activation. Arguments kernel_size : size of the convolution kernel (tuple) inplanes : number of input feature maps outplanes : number of output feature maps activation : the activation function for the final layer norm_layer : the normalisation layer used. Note that using  identity  as the normalisation layer will result in no normalisation being applied. (This is only compatible with  preact  and  revnorm  both set to  false .) revnorm : set to  true  to place the normalisation layer before the convolution preact : set to  true  to place the activation function before the normalisation layer (only compatible with  revnorm = false ) bias : bias for the convolution kernel. This is set to  false  by default if  norm_layer  is not  identity  and  true  otherwise. stride : stride of the convolution kernel pad : padding of the convolution kernel dilation : dilation of the convolution kernel groups : groups for the convolution kernel weight ,  init : initialization for the convolution kernel (see  Flux.Conv ) source"},{"id":991,"pagetitle":"Layers","title":"Metalhead.Layers.basic_conv_bn","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.basic_conv_bn","content":" Metalhead.Layers.basic_conv_bn  —  Function basic_conv_bn(kernel_size::Dims{2}, inplanes, outplanes, activation = relu;\n              kwargs...) Returns a convolution + batch normalisation pair with activation as used by the Inception family of models with default values matching those used in the official TensorFlow implementation. Arguments kernel_size : size of the convolution kernel (tuple) inplanes : number of input feature maps outplanes : number of output feature maps activation : the activation function for the final layer batchnorm : set to  true  to include batch normalization after each convolution kwargs : keyword arguments passed to  conv_norm source"},{"id":992,"pagetitle":"Layers","title":"Convolution-related custom blocks","ref":"/metalhead/stable/api/layers/#Convolution-related-custom-blocks","content":" Convolution-related custom blocks These blocks are designed to be used in convolutional neural networks. Most of these are used in the MobileNet and EfficientNet family of models, but they also feature in \"fancier\" versions of well known-models like ResNet (SE-ResNet)."},{"id":993,"pagetitle":"Layers","title":"Metalhead.Layers.dwsep_conv_norm","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.dwsep_conv_norm","content":" Metalhead.Layers.dwsep_conv_norm  —  Function dwsep_conv_norm(kernel_size::Dims{2}, inplanes::Integer, outplanes::Integer,\n                activation = relu; norm_layer = BatchNorm, stride::Integer = 1,\n                bias::Bool = !(norm_layer !== identity), pad::Integer = 0, [bias, weight, init]) Create a depthwise separable convolution chain as used in MobileNetv1. This is sequence of layers: a  kernel_size  depthwise convolution from  inplanes => inplanes a (batch) normalisation layer +  activation  (if  norm_layer !== identity ; otherwise  activation  is applied to the convolution output) a  kernel_size  convolution from  inplanes => outplanes a (batch) normalisation layer +  activation  (if  norm_layer !== identity ; otherwise  activation  is applied to the convolution output) See Fig. 3 in  reference . Arguments kernel_size : size of the convolution kernel (tuple) inplanes : number of input feature maps outplanes : number of output feature maps activation : the activation function for the final layer norm_layer : the normalisation layer used. Note that using  identity  as the normalisation layer will result in no normalisation being applied. bias : whether to use bias in the convolution layers. stride : stride of the first convolution kernel pad : padding of the first convolution kernel weight ,  init : initialization for the convolution kernel (see  Flux.Conv ) source"},{"id":994,"pagetitle":"Layers","title":"Metalhead.Layers.mbconv","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.mbconv","content":" Metalhead.Layers.mbconv  —  Function mbconv(kernel_size::Dims{2}, inplanes::Integer, explanes::Integer,\n       outplanes::Integer, activation = relu; stride::Integer,\n       reduction::Union{Nothing, Real} = nothing,\n       se_round_fn = x -> round(Int, x), norm_layer = BatchNorm, kwargs...) Create a basic inverted residual block for MobileNet and Efficient variants. This is a sequence of layers: a 1x1 convolution from  inplanes => explanes  followed by a (batch) normalisation layer activation  if  inplanes != explanes a  kernel_size  depthwise separable convolution from  explanes => explanes a (batch) normalisation layer a squeeze-and-excitation block (if  reduction != nothing ) from  explanes => se_round_fn(explanes / reduction)  and back to  explanes a 1x1 convolution from  explanes => outplanes a (batch) normalisation layer +  activation Warning This function does not handle the residual connection by default. The user must add this manually to use this block as a standalone. To construct a model, check out the builders, which handle the residual connection and other details. First introduced in the MobileNetv2 paper. (See Fig. 3 in  reference .) Arguments kernel_size : kernel size of the convolutional layers inplanes : number of input feature maps explanes : The number of expanded feature maps. This is the number of feature maps after the first 1x1 convolution. outplanes : The number of output feature maps activation : The activation function for the first two convolution layer stride : The stride of the convolutional kernel, has to be either 1 or 2 reduction : The reduction factor for the number of hidden feature maps in a squeeze and excite layer (see  squeeze_excite ) se_round_fn : The function to round the number of reduced feature maps in the squeeze and excite layer norm_layer : The normalization layer to use source"},{"id":995,"pagetitle":"Layers","title":"Metalhead.Layers.fused_mbconv","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.fused_mbconv","content":" Metalhead.Layers.fused_mbconv  —  Function fused_mbconv(kernel_size::Dims{2}, inplanes::Integer, explanes::Integer,\n             outplanes::Integer, activation = relu;\n             stride::Integer, norm_layer = BatchNorm) Create a fused inverted residual block. This is a sequence of layers: a  kernel_size  depthwise separable convolution from  explanes => explanes a (batch) normalisation layer a 1x1 convolution from  explanes => outplanes  followed by a (batch) normalisation layer +  activation  if  inplanes != explanes Warning This function does not handle the residual connection by default. The user must add this manually to use this block as a standalone. To construct a model, check out the builders, which handle the residual connection and other details. Originally introduced by Google in  EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML . Later used in the EfficientNetv2 paper. Arguments kernel_size : kernel size of the convolutional layers inplanes : number of input feature maps explanes : The number of expanded feature maps outplanes : The number of output feature maps activation : The activation function for the first two convolution layer stride : The stride of the convolutional kernel, has to be either 1 or 2 norm_layer : The normalization layer to use source"},{"id":996,"pagetitle":"Layers","title":"Metalhead.Layers.squeeze_excite","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.squeeze_excite","content":" Metalhead.Layers.squeeze_excite  —  Function squeeze_excite(inplanes::Integer; reduction::Real = 16, round_fn = _round_channels, \n               norm_layer = identity, activation = relu, gate_activation = sigmoid) Creates a squeeze-and-excitation layer used in MobileNets, EfficientNets and SE-ResNets. Arguments inplanes : The number of input feature maps reduction : The reduction factor for the number of hidden feature maps in the squeeze and excite layer. The number of hidden feature maps is calculated as  round_fn(inplanes / reduction) . round_fn : The function to round the number of reduced feature maps. activation : The activation function for the first convolution layer gate_activation : The activation function for the gate layer norm_layer : The normalization layer to be used after the convolution layers rd_planes : The number of hidden feature maps in a squeeze and excite layer source"},{"id":997,"pagetitle":"Layers","title":"Metalhead.Layers.effective_squeeze_excite","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.effective_squeeze_excite","content":" Metalhead.Layers.effective_squeeze_excite  —  Function effective_squeeze_excite(inplanes, gate_activation = sigmoid) Effective squeeze-and-excitation layer. (reference:  CenterMask : Real-Time Anchor-Free Instance Segmentation ) Arguments inplanes : The number of input feature maps gate_activation : The activation function for the gate layer source"},{"id":998,"pagetitle":"Layers","title":"Normalisation, Dropout and Pooling layers","ref":"/metalhead/stable/api/layers/#Normalisation,-Dropout-and-Pooling-layers","content":" Normalisation, Dropout and Pooling layers Metalhead provides various custom layers for normalisation, dropout and pooling which have been used to additionally customise various models."},{"id":999,"pagetitle":"Layers","title":"Normalisation layers","ref":"/metalhead/stable/api/layers/#Normalisation-layers","content":" Normalisation layers"},{"id":1000,"pagetitle":"Layers","title":"Metalhead.Layers.ChannelLayerNorm","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.ChannelLayerNorm","content":" Metalhead.Layers.ChannelLayerNorm  —  Type ChannelLayerNorm(sz::Integer, λ = identity; eps = 1.0f-6) A variant of LayerNorm where the input is normalised along the channel dimension. The input is expected to have channel dimension with size  sz . It also applies a learnable shift and rescaling after the normalization. Note that this is specifically for inputs with 4 dimensions in the format (H, W, C, N) where H, W are the height and width of the input, C is the number of channels, and N is the batch size. source"},{"id":1001,"pagetitle":"Layers","title":"Metalhead.Layers.LayerNormV2","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.LayerNormV2","content":" Metalhead.Layers.LayerNormV2  —  Type LayerNormV2(size..., λ=identity; affine=true, eps=1f-5) Same as Flux's LayerNorm but eps is added before taking the square root in the denominator. Therefore, LayerNormV2 matches pytorch's LayerNorm. source"},{"id":1002,"pagetitle":"Layers","title":"Metalhead.Layers.LayerScale","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.LayerScale","content":" Metalhead.Layers.LayerScale  —  Function LayerScale(planes::Integer, λ) Creates a  Flux.Scale  layer that performs \" LayerScale \" ( reference ). Arguments planes : Size of channel dimension in the input. λ : initialisation value for the learnable diagonal matrix. source"},{"id":1003,"pagetitle":"Layers","title":"Dropout layers","ref":"/metalhead/stable/api/layers/#Dropout-layers","content":" Dropout layers"},{"id":1004,"pagetitle":"Layers","title":"Metalhead.Layers.DropBlock","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.DropBlock","content":" Metalhead.Layers.DropBlock  —  Type DropBlock(drop_block_prob = 0.1, block_size = 7, gamma_scale = 1.0, [rng]) The  DropBlock  layer. While training, it zeroes out continguous regions of size  block_size  in the input. During inference, it simply returns the input  x . It can be used in two ways: either with all blocks having the same survival probability or with a linear scaling rule across the blocks. This is performed only at training time. At test time, the  DropBlock  layer is equivalent to  identity . ( reference ) Arguments drop_block_prob : probability of dropping a block. If  nothing  is passed, it returns  identity . Note that some literature uses the term \"survival probability\" instead, which is equivalent to  1 - drop_block_prob . block_size : size of the block to drop gamma_scale : multiplicative factor for  gamma  used. For the calculation of gamma, refer to  the paper . rng : can be used to pass in a custom RNG instead of the default. Custom RNGs are only supported on the CPU. source"},{"id":1005,"pagetitle":"Layers","title":"Metalhead.Layers.dropblock","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.dropblock","content":" Metalhead.Layers.dropblock  —  Function dropblock([rng], x::AbstractArray{T, 4}, drop_block_prob, block_size,\n          gamma_scale, active::Bool = true) The dropblock function. If  active  is  true , for each input, it zeroes out continguous regions of size  block_size  in the input. Otherwise, it simply returns the input  x . Arguments rng : can be used to pass in a custom RNG instead of the default. Custom RNGs are only supported on the CPU. x : input array drop_block_prob : probability of dropping a block. If  nothing  is passed, it returns  identity . block_size : size of the block to drop gamma_scale : multiplicative factor for  gamma  used. For the calculations, refer to  the paper . If you are not a package developer, you most likely do not want this function. Use  DropBlock  instead. source"},{"id":1006,"pagetitle":"Layers","title":"Metalhead.Layers.StochasticDepth","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.StochasticDepth","content":" Metalhead.Layers.StochasticDepth  —  Function StochasticDepth(p, mode = :row; [rng]) Implements Stochastic Depth. This is a  Dropout  layer from Flux that drops values with probability  p . ( reference ) This layer can be used to drop certain blocks in a residual structure and allow them to propagate completely through the skip connection. It can be used in two ways: either with all blocks having the same survival probability or with a linear scaling rule across the blocks. This is performed only at training time. At test time, the  StochasticDepth  layer is equivalent to  identity . Arguments p : probability of Stochastic Depth. Note that some literature uses the term \"survival probability\" instead, which is equivalent to  1 - p . mode : Either  :batch  or  :row .  :batch  randomly zeroes the entire input,  row  zeroes randomly selected rows from the batch. The default is  :row . rng : can be used to pass in a custom RNG instead of the default. See  Flux.Dropout  for more information on the behaviour of this argument. Custom RNGs are only supported on the CPU. source"},{"id":1007,"pagetitle":"Layers","title":"Pooling layers","ref":"/metalhead/stable/api/layers/#Pooling-layers","content":" Pooling layers"},{"id":1008,"pagetitle":"Layers","title":"Metalhead.Layers.AdaptiveMeanMaxPool","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.AdaptiveMeanMaxPool","content":" Metalhead.Layers.AdaptiveMeanMaxPool  —  Function AdaptiveMeanMaxPool([connection = +], output_size::Tuple = (1, 1)) A type of adaptive pooling layer which uses both mean and max pooling and combines them to produce a single output. Note that this is equivalent to  Parallel(connection, AdaptiveMeanPool(output_size), AdaptiveMaxPool(output_size)) . When  connection  is not specified, it defaults to  + . Arguments connection : The connection type to use. output_size : The size of the output after pooling. source"},{"id":1009,"pagetitle":"Layers","title":"Classifier creation","ref":"/metalhead/stable/api/layers/#Classifier-creation","content":" Classifier creation Metalhead provides a function to create a classifier for neural network models that is quite flexible, and is used by the library extensively to create the classifier \"head\" for networks."},{"id":1010,"pagetitle":"Layers","title":"Metalhead.Layers.create_classifier","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.create_classifier","content":" Metalhead.Layers.create_classifier  —  Function create_classifier(inplanes::Integer, nclasses::Integer, activation = identity;\n                  use_conv::Bool = false, pool_layer = AdaptiveMeanPool((1, 1)), \n                  dropout_prob = nothing) Creates a classifier head to be used for models. Arguments inplanes : number of input feature maps nclasses : number of output classes activation : activation function to use use_conv : whether to use a 1x1 convolutional layer instead of a  Dense  layer. pool_layer : pooling layer to use. This is passed in with the layer instantiated with any arguments that are needed i.e. as  AdaptiveMeanPool((1, 1)) , for example. dropout_prob : dropout probability used in the classifier head. Set to  nothing  to disable dropout. source create_classifier(inplanes::Integer, hidden_planes::Integer, nclasses::Integer,\n                  activations::NTuple{2} = (relu, identity);\n                  use_conv::NTuple{2, Bool} = (false, false),\n                  pool_layer = AdaptiveMeanPool((1, 1)), dropout_prob = nothing) Creates a classifier head to be used for models with an extra hidden layer. Arguments inplanes : number of input feature maps hidden_planes : number of hidden feature maps nclasses : number of output classes activations : activation functions to use for the hidden and output layers. This is a tuple of two elements, the first being the activation function for the hidden layer and the second for the output layer. use_conv : whether to use a 1x1 convolutional layer instead of a  Dense  layer. This is a tuple of two booleans, the first for the hidden layer and the second for the output layer. pool_layer : pooling layer to use. This is passed in with the layer instantiated with any arguments that are needed i.e. as  AdaptiveMeanPool((1, 1)) , for example. dropout_prob : dropout probability used in the classifier head. Set to  nothing  to disable dropout. source"},{"id":1011,"pagetitle":"Layers","title":"Vision transformer-related layers","ref":"/metalhead/stable/api/layers/#Vision-transformer-related-layers","content":" Vision transformer-related layers The  Layers  module contains specific layers that are used to build vision transformer (ViT)-inspired models:"},{"id":1012,"pagetitle":"Layers","title":"Metalhead.Layers.MultiHeadSelfAttention","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.MultiHeadSelfAttention","content":" Metalhead.Layers.MultiHeadSelfAttention  —  Type MultiHeadSelfAttention(planes::Integer, nheads::Integer = 8; qkv_bias::Bool = false, \n            attn_dropout_prob = 0., proj_dropout_prob = 0.) Multi-head self-attention layer. Arguments planes : number of input channels nheads : number of heads qkv_bias : whether to use bias in the layer to get the query, key and value attn_dropout_prob : dropout probability after the self-attention layer proj_dropout_prob : dropout probability after the projection layer source"},{"id":1013,"pagetitle":"Layers","title":"Metalhead.Layers.ClassTokens","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.ClassTokens","content":" Metalhead.Layers.ClassTokens  —  Type ClassTokens(planes::Integer; init = Flux.zeros32) Appends class tokens to an input with embedding dimension  planes  for use in many vision transformer models. source"},{"id":1014,"pagetitle":"Layers","title":"Metalhead.Layers.ViPosEmbedding","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.ViPosEmbedding","content":" Metalhead.Layers.ViPosEmbedding  —  Type ViPosEmbedding(embedsize::Integer, npatches::Integer; \n               init = (dims::Dims{2}) -> rand(Float32, dims)) Positional embedding layer used by many vision transformer-like models. source"},{"id":1015,"pagetitle":"Layers","title":"Metalhead.Layers.PatchEmbedding","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.PatchEmbedding","content":" Metalhead.Layers.PatchEmbedding  —  Function PatchEmbedding(imsize::Dims{2} = (224, 224); inchannels::Integer = 3,\n               patch_size::Dims{2} = (16, 16), embedplanes = 768,\n               norm_layer = planes -> identity, flatten = true) Patch embedding layer used by many vision transformer-like models to split the input image into patches. Arguments imsize : the size of the input image inchannels : number of input channels patch_size : the size of the patches embedplanes : the number of channels in the embedding norm_layer : the normalization layer - by default the identity function but otherwise takes a single argument constructor for a normalization layer like LayerNorm or BatchNorm flatten : set true to flatten the input spatial dimensions after the embedding source"},{"id":1016,"pagetitle":"Layers","title":"MLPMixer-related blocks","ref":"/metalhead/stable/api/layers/#MLPMixer-related-blocks","content":" MLPMixer-related blocks Apart from this, the  Layers  module also contains certain blocks used in MLPMixer-style models:"},{"id":1017,"pagetitle":"Layers","title":"Metalhead.Layers.gated_mlp_block","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.gated_mlp_block","content":" Metalhead.Layers.gated_mlp_block  —  Function gated_mlp(gate_layer, inplanes::Integer, hidden_planes::Integer, \n          outplanes::Integer = inplanes; dropout_prob = 0.0, activation = gelu) Feedforward block based on the implementation in the paper \"Pay Attention to MLPs\". ( reference ) Arguments gate_layer : Layer to use for the gating. inplanes : Number of dimensions in the input. hidden_planes : Number of dimensions in the intermediate layer. outplanes : Number of dimensions in the output - by default it is the same as  inplanes . dropout_prob : Dropout probability. activation : Activation function to use. source"},{"id":1018,"pagetitle":"Layers","title":"Metalhead.Layers.mlp_block","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.mlp_block","content":" Metalhead.Layers.mlp_block  —  Function mlp_block(inplanes::Integer, hidden_planes::Integer, outplanes::Integer = inplanes; \n          dropout_prob = 0., activation = gelu) Feedforward block used in many MLPMixer-like and vision-transformer models. Arguments inplanes : Number of dimensions in the input. hidden_planes : Number of dimensions in the intermediate layer. outplanes : Number of dimensions in the output - by default it is the same as  inplanes . dropout_prob : Dropout probability. activation : Activation function to use. source"},{"id":1019,"pagetitle":"Layers","title":"Utilities for layers","ref":"/metalhead/stable/api/layers/#Utilities-for-layers","content":" Utilities for layers These are some miscellaneous utilities present in the  Layers  module, and are used with other custom/inbuilt layers to make certain common operations in neural networks easier."},{"id":1020,"pagetitle":"Layers","title":"Metalhead.Layers.inputscale","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.inputscale","content":" Metalhead.Layers.inputscale  —  Function inputscale(λ; activation = identity) Scale the input by a scalar  λ  and applies an activation function to it. Equivalent to  activation.(λ .* x) . source"},{"id":1021,"pagetitle":"Layers","title":"Metalhead.Layers.actadd","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.actadd","content":" Metalhead.Layers.actadd  —  Function actadd(activation = relu, xs...) Convenience function for adding input arrays after applying an activation function to them. Useful as the  connection  argument for the block function in  resnet . source"},{"id":1022,"pagetitle":"Layers","title":"Metalhead.Layers.addact","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.addact","content":" Metalhead.Layers.addact  —  Function addact(activation = relu, xs...) Convenience function for applying an activation function to the output after summing up the input arrays. Useful as the  connection  argument for the block function in  resnet . source"},{"id":1023,"pagetitle":"Layers","title":"Metalhead.Layers.cat_channels","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.cat_channels","content":" Metalhead.Layers.cat_channels  —  Function cat_channels(x, y, zs...) Concatenate  x  and  y  (and any  z s) along the channel dimension (third dimension). Equivalent to  cat(x, y, zs...; dims=3) . Convenient reduction operator for use with  Parallel . source"},{"id":1024,"pagetitle":"Layers","title":"Metalhead.Layers.flatten_chains","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.flatten_chains","content":" Metalhead.Layers.flatten_chains  —  Function flatten_chains(m::Chain)\nflatten_chains(m) Convenience function for traversing nested layers of a Chain object and flatten them  into a single iterator. source"},{"id":1025,"pagetitle":"Layers","title":"Metalhead.Layers.linear_scheduler","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.linear_scheduler","content":" Metalhead.Layers.linear_scheduler  —  Function linear_scheduler(drop_prob = 0.0; start_value = 0.0, depth)\nlinear_scheduler(drop_prob::Nothing; depth::Integer) Returns the dropout probabilities for a given depth using the linear scaling rule. Note that this returns evenly spaced values between  start_value  and  drop_prob , not including  drop_prob . If  drop_prob  is  nothing , it returns a  Vector  of length  depth  with all values equal to  nothing . source"},{"id":1026,"pagetitle":"Layers","title":"Metalhead.Layers.swapdims","ref":"/metalhead/stable/api/layers/#Metalhead.Layers.swapdims","content":" Metalhead.Layers.swapdims  —  Function swapdims(perm) Convenience function that returns a closure which permutes the dimensions of an array.  perm  is a vector or tuple specifying a permutation of the input dimensions. Equivalent to  permutedims(x, perm) . source"},{"id":1029,"pagetitle":"MLPMixer-like models","title":"MLPMixer-like models","ref":"/metalhead/stable/api/mixers/#MLPMixer-like-models","content":" MLPMixer-like models This is the API reference for the MLPMixer-like models supported by Metalhead.jl."},{"id":1030,"pagetitle":"MLPMixer-like models","title":"The higher-level model constructors","ref":"/metalhead/stable/api/mixers/#The-higher-level-model-constructors","content":" The higher-level model constructors"},{"id":1031,"pagetitle":"MLPMixer-like models","title":"Metalhead.MLPMixer","ref":"/metalhead/stable/api/mixers/#Metalhead.MLPMixer","content":" Metalhead.MLPMixer  —  Type MLPMixer(config::Symbol; patch_size::Dims{2} = (16, 16), imsize::Dims{2} = (224, 224),\n         inchannels::Integer = 3, nclasses::Integer = 1000) Creates a model with the MLPMixer architecture. ( reference ). Arguments config : the size of the model - one of  :small ,  :base ,  :large  or  :huge patch_size : the size of the patches imsize : the size of the input image stochastic_depth_prob : Stochastic depth probability inchannels : the number of input channels nclasses : number of output classes See also  Metalhead.mlpmixer . source"},{"id":1032,"pagetitle":"MLPMixer-like models","title":"Metalhead.ResMLP","ref":"/metalhead/stable/api/mixers/#Metalhead.ResMLP","content":" Metalhead.ResMLP  —  Type ResMLP(config::Symbol; patch_size::Dims{2} = (16, 16), imsize::Dims{2} = (224, 224),\n       inchannels::Integer = 3, nclasses::Integer = 1000) Creates a model with the ResMLP architecture. ( reference ). Arguments config : the size of the model - one of  :small ,  :base ,  :large  or  :huge patch_size : the size of the patches imsize : the size of the input image inchannels : the number of input channels nclasses : number of output classes See also  Metalhead.mlpmixer . source"},{"id":1033,"pagetitle":"MLPMixer-like models","title":"Metalhead.gMLP","ref":"/metalhead/stable/api/mixers/#Metalhead.gMLP","content":" Metalhead.gMLP  —  Type gMLP(config::Symbol; patch_size::Dims{2} = (16, 16), imsize::Dims{2} = (224, 224),\n     inchannels::Integer = 3, nclasses::Integer = 1000) Creates a model with the gMLP architecture. ( reference ). Arguments config : the size of the model - one of  :small ,  :base ,  :large  or  :huge patch_size : the size of the patches imsize : the size of the input image inchannels : the number of input channels nclasses : number of output classes See also  Metalhead.mlpmixer . source"},{"id":1034,"pagetitle":"MLPMixer-like models","title":"The core MLPMixer function","ref":"/metalhead/stable/api/mixers/#The-core-MLPMixer-function","content":" The core MLPMixer function"},{"id":1035,"pagetitle":"MLPMixer-like models","title":"Metalhead.mlpmixer","ref":"/metalhead/stable/api/mixers/#Metalhead.mlpmixer","content":" Metalhead.mlpmixer  —  Function mlpmixer(block, imsize::Dims{2} = (224, 224); inchannels::Integer = 3, norm_layer = LayerNorm,\n         patch_size::Dims{2} = (16, 16), embedplanes = 512, stochastic_depth_prob = 0.,\n         depth::Integer = 12, nclasses::Integer = 1000, kwargs...) Creates a model with the MLPMixer architecture. ( reference ). Arguments block : the type of mixer block to use in the model - architecture dependent (a constructor of the form  block(embedplanes, npatches; stochastic_depth_prob, kwargs...) ) imsize : the size of the input image inchannels : the number of input channels norm_layer : the normalization layer to use in the model patch_size : the size of the patches embedplanes : the number of channels after the patch embedding (denotes the hidden dimension) stochastic_depth_prob : Stochastic depth probability depth : the number of blocks in the model nclasses : number of output classes kwargs : additional arguments (if any) to pass to the mixer block. Will use the defaults if not specified. source"},{"id":1036,"pagetitle":"MLPMixer-like models","title":"The block functions","ref":"/metalhead/stable/api/mixers/#The-block-functions","content":" The block functions"},{"id":1037,"pagetitle":"MLPMixer-like models","title":"Metalhead.mixerblock","ref":"/metalhead/stable/api/mixers/#Metalhead.mixerblock","content":" Metalhead.mixerblock  —  Function mixerblock(planes::Integer, npatches::Integer; mlp_layer = mlp_block,\n           mlp_ratio = (0.5, 4.0), dropout_prob = 0.0, stochastic_depth_prob = 0.0,\n           activation = gelu) Creates a feedforward block for the MLPMixer architecture. ( reference ) Arguments planes : the number of planes in the block npatches : the number of patches of the input mlp_ratio : number(s) that determine(s) the number of hidden channels in the token mixing MLP and/or the channel mixing MLP as a ratio to the number of planes in the block. mlp_layer : the MLP layer to use in the block dropout_prob : the dropout probability to use in the MLP blocks stochastic_depth_prob : Stochastic depth probability activation : the activation function to use in the MLP blocks source"},{"id":1038,"pagetitle":"MLPMixer-like models","title":"Metalhead.resmixerblock","ref":"/metalhead/stable/api/mixers/#Metalhead.resmixerblock","content":" Metalhead.resmixerblock  —  Function resmixerblock(planes, npatches; dropout_prob = 0., stochastic_depth_prob = 0., mlp_ratio = 4.0,\n              activation = gelu, layerscale_init = 1e-4) Creates a block for the ResMixer architecture. ( reference ). Arguments planes : the number of planes in the block npatches : the number of patches of the input mlp_ratio : ratio of the number of hidden channels in the channel mixing MLP to the number of planes in the block mlp_layer : the MLP block to use dropout_prob : the dropout probability to use in the MLP blocks stochastic_depth_prob : Stochastic depth probability activation : the activation function to use in the MLP blocks layerscale_init : initialisation constant for the LayerScale source"},{"id":1039,"pagetitle":"MLPMixer-like models","title":"Metalhead.SpatialGatingUnit","ref":"/metalhead/stable/api/mixers/#Metalhead.SpatialGatingUnit","content":" Metalhead.SpatialGatingUnit  —  Type SpatialGatingUnit(planes::Integer, npatches::Integer; norm_layer = LayerNorm) Creates a spatial gating unit as described in the gMLP paper. ( reference ) Arguments planes : the number of planes in the block npatches : the number of patches of the input norm_layer : the normalisation layer to use source"},{"id":1040,"pagetitle":"MLPMixer-like models","title":"Metalhead.spatialgatingblock","ref":"/metalhead/stable/api/mixers/#Metalhead.spatialgatingblock","content":" Metalhead.spatialgatingblock  —  Function spatialgatingblock(planes::Integer, npatches::Integer; mlp_ratio = 4.0,\n                   norm_layer = LayerNorm, mlp_layer = gated_mlp_block,\n                   dropout_prob = 0.0, stochastic_depth_prob = 0.0,\n                   activation = gelu) Creates a feedforward block based on the gMLP model architecture described in the paper. ( reference ) Arguments planes : the number of planes in the block npatches : the number of patches of the input mlp_ratio : ratio of the number of hidden channels in the channel mixing MLP to the number of planes in the block norm_layer : the normalisation layer to use dropout_prob : the dropout probability to use in the MLP blocks stochastic_depth_prob : Stochastic depth probability activation : the activation function to use in the MLP blocks source"},{"id":1043,"pagetitle":"MobileNet family of models","title":"MobileNet family of models","ref":"/metalhead/stable/api/mobilenet/#MobileNet-family-of-models","content":" MobileNet family of models This is the API reference for the MobileNet family of models supported by Metalhead.jl."},{"id":1044,"pagetitle":"MobileNet family of models","title":"Metalhead.MobileNetv1","ref":"/metalhead/stable/api/mobilenet/#Metalhead.MobileNetv1","content":" Metalhead.MobileNetv1  —  Type MobileNetv1(width_mult::Real = 1; pretrain::Bool = false,\n            inchannels::Integer = 3, nclasses::Integer = 1000) Create a MobileNetv1 model with the baseline configuration ( reference ). Arguments width_mult : Controls the number of output feature maps in each block (with 1 being the default in the paper; this is usually a value between 0.1 and 1.4) pretrain : Whether to load the pre-trained weights for ImageNet inchannels : The number of input channels. nclasses : The number of output classes Warning MobileNetv1  does not currently support pretrained weights. See also  Metalhead.mobilenetv1 . source"},{"id":1045,"pagetitle":"MobileNet family of models","title":"Metalhead.MobileNetv2","ref":"/metalhead/stable/api/mobilenet/#Metalhead.MobileNetv2","content":" Metalhead.MobileNetv2  —  Type MobileNetv2(width_mult = 1.0; inchannels::Integer = 3, pretrain::Bool = false,\n            nclasses::Integer = 1000) Create a MobileNetv2 model with the specified configuration. ( reference ). Arguments width_mult : Controls the number of output feature maps in each block (with 1 being the default in the paper; this is usually a value between 0.1 and 1.4) pretrain : Whether to load the pre-trained weights for ImageNet inchannels : The number of input channels. nclasses : The number of output classes Warning MobileNetv2  does not currently support pretrained weights. See also  Metalhead.mobilenetv2 . source"},{"id":1046,"pagetitle":"MobileNet family of models","title":"Metalhead.MobileNetv3","ref":"/metalhead/stable/api/mobilenet/#Metalhead.MobileNetv3","content":" Metalhead.MobileNetv3  —  Type MobileNetv3(config::Symbol; width_mult::Real = 1, pretrain::Bool = false,\n            inchannels::Integer = 3, nclasses::Integer = 1000) Create a MobileNetv3 model with the specified configuration. ( reference ). Set  pretrain = true  to load the model with pre-trained weights for ImageNet. Arguments config : :small or :large for the size of the model (see paper). width_mult : Controls the number of output feature maps in each block (with 1 being the default in the paper; this is usually a value between 0.1 and 1.4) pretrain : whether to load the pre-trained weights for ImageNet inchannels : number of input channels nclasses : the number of output classes Warning MobileNetv3  does not currently support pretrained weights. See also  Metalhead.mobilenetv3 . source"},{"id":1047,"pagetitle":"MobileNet family of models","title":"Metalhead.MNASNet","ref":"/metalhead/stable/api/mobilenet/#Metalhead.MNASNet","content":" Metalhead.MNASNet  —  Type MNASNet(config::Symbol; width_mult::Real = 1, pretrain::Bool = false,\n        inchannels::Integer = 3, nclasses::Integer = 1000) Creates a MNASNet model with the specified configuration. ( reference ) Arguments config : configuration of the model. One of  B1 ,  A1  or  small .  B1  is without squeeze-and-excite layers,  A1  is with squeeze-and-excite layers, and  small  is a smaller version of  A1 . width_mult : Controls the number of output feature maps in each block (with 1 being the default in the paper; this is usually a value between 0.1 and 1.4) pretrain : Whether to load the pre-trained weights for ImageNet inchannels : The number of input channels. nclasses : The number of output classes Warning MNASNet  does not currently support pretrained weights. See also  Metalhead.mnasnet . source"},{"id":1050,"pagetitle":"Other models","title":"Other models","ref":"/metalhead/stable/api/others/#Other-models","content":" Other models This is the API reference for some of the other models supported by Metalhead.jl that do not fit into the other categories."},{"id":1051,"pagetitle":"Other models","title":"The higher-level model constructors","ref":"/metalhead/stable/api/others/#The-higher-level-model-constructors","content":" The higher-level model constructors"},{"id":1052,"pagetitle":"Other models","title":"Metalhead.AlexNet","ref":"/metalhead/stable/api/others/#Metalhead.AlexNet","content":" Metalhead.AlexNet  —  Type AlexNet(; pretrain::Bool = false, inchannels::Integer = 3,\n        nclasses::Integer = 1000) Create a  AlexNet . ( reference ). Arguments pretrain : set to  true  to load pre-trained weights for ImageNet inchannels : The number of input channels. nclasses : the number of output classes Warning AlexNet  does not currently support pretrained weights. See also  alexnet . source"},{"id":1053,"pagetitle":"Other models","title":"Metalhead.VGG","ref":"/metalhead/stable/api/others/#Metalhead.VGG","content":" Metalhead.VGG  —  Type VGG(imsize::Dims{2}; config, inchannels, batchnorm = false, nclasses, fcsize, dropout_prob) Construct a VGG model with the specified input image size. Typically, the image size is  (224, 224) . Keyword Arguments: config  : VGG convolutional block configuration. It is defined as a vector of tuples  (output_channels, num_convolutions)  for each block inchannels : number of input channels batchnorm : set to  true  to use batch normalization after each convolution nclasses : number of output classes fcsize : intermediate fully connected layer size (see  Metalhead.vgg_classifier_layers ) dropout_prob : dropout level between fully connected layers source"},{"id":1054,"pagetitle":"Other models","title":"Metalhead.SqueezeNet","ref":"/metalhead/stable/api/others/#Metalhead.SqueezeNet","content":" Metalhead.SqueezeNet  —  Type SqueezeNet(; pretrain::Bool = false, inchannels::Integer = 3,\n           nclasses::Integer = 1000) Create a SqueezeNet ( reference ). Arguments pretrain : set to  true  to load the pre-trained weights for ImageNet inchannels : number of input channels. nclasses : the number of output classes. See also  squeezenet . source"},{"id":1055,"pagetitle":"Other models","title":"Metalhead.UNet","ref":"/metalhead/stable/api/others/#Metalhead.UNet","content":" Metalhead.UNet  —  Type UNet(imsize::Dims{2} = (256, 256), inchannels::Integer = 3, outplanes::Integer = 3,\n     encoder_backbone = Metalhead.backbone(DenseNet(121)); pretrain::Bool = false) Creates a UNet model with an encoder built of specified backbone. By default it uses   DenseNet  backbone, but any ResNet-like Metalhead model can be used for the encoder. ( reference ). Arguments imsize : size of input image inchannels : number of channels in input image outplanes : number of output feature planes. encoder_backbone : The backbone layers of specified model to be used as encoder. For example,  Metalhead.backbone(Metalhead.ResNet(18))  can be passed to instantiate a UNet with layers of resnet18 as encoder. pretrain : Whether to load the pre-trained weights for ImageNet Warning UNet  does not currently support pretrained weights. See also  Metalhead.unet . source"},{"id":1056,"pagetitle":"Other models","title":"The mid-level functions","ref":"/metalhead/stable/api/others/#The-mid-level-functions","content":" The mid-level functions"},{"id":1057,"pagetitle":"Other models","title":"Metalhead.alexnet","ref":"/metalhead/stable/api/others/#Metalhead.alexnet","content":" Metalhead.alexnet  —  Function alexnet(; dropout_prob = 0.5, inchannels::Integer = 3, nclasses::Integer = 1000) Create an AlexNet model ( reference ). Arguments dropout_prob : dropout probability for the classifier inchannels : The number of input channels. nclasses : the number of output classes source"},{"id":1058,"pagetitle":"Other models","title":"Metalhead.vgg","ref":"/metalhead/stable/api/others/#Metalhead.vgg","content":" Metalhead.vgg  —  Function vgg(imsize; config, inchannels, batchnorm = false, nclasses, fcsize, dropout_prob) Create a VGG model ( reference ). Arguments imsize : input image width and height as a tuple config : the configuration for the convolution layers (see  Metalhead.vgg_convolutional_layers ) inchannels : number of input channels batchnorm : set to  true  to use batch normalization after each convolution nclasses : number of output classes fcsize : intermediate fully connected layer size (see  Metalhead.vgg_classifier_layers ) dropout_prob : dropout level between fully connected layers source"},{"id":1059,"pagetitle":"Other models","title":"Metalhead.squeezenet","ref":"/metalhead/stable/api/others/#Metalhead.squeezenet","content":" Metalhead.squeezenet  —  Function squeezenet(; dropout_prob = 0.5, inchannels::Integer = 3, nclasses::Integer = 1000) Create a SqueezeNet model. ( reference ). Arguments dropout_prob : dropout probability for the classifier head. Set to  nothing  to disable dropout. inchannels : number of input channels. nclasses : the number of output classes. source"},{"id":1060,"pagetitle":"Other models","title":"Metalhead.unet","ref":"/metalhead/stable/api/others/#Metalhead.unet","content":" Metalhead.unet  —  Function unet(encoder_backbone, imgdims, outplanes::Integer, final::Any = unet_final_block,\n     fdownscale::Integer = 0) Creates a UNet model with specified convolutional backbone.  Backbone of any Metalhead ResNet-like model can be used as encoder  ( reference ). Arguments - `encoder_backbone`: The backbone layers of specified model to be used as encoder.\n\tFor example, `Metalhead.backbone(Metalhead.ResNet(18))` can be passed \n\tto instantiate a UNet with layers of resnet18 as encoder.\n- `inputsize`: size of input image\n- `outplanes`: number of output feature planes\n- `final`: final block as described in original paper\n- `fdownscale`: downscale factor source"},{"id":1063,"pagetitle":"ResNet-like models","title":"ResNet-like models","ref":"/metalhead/stable/api/resnet/#ResNet-like-models","content":" ResNet-like models This is the API reference for the ResNet inspired model structures present in Metalhead.jl."},{"id":1064,"pagetitle":"ResNet-like models","title":"The higher-level model constructors","ref":"/metalhead/stable/api/resnet/#The-higher-level-model-constructors","content":" The higher-level model constructors"},{"id":1065,"pagetitle":"ResNet-like models","title":"Metalhead.ResNet","ref":"/metalhead/stable/api/resnet/#Metalhead.ResNet","content":" Metalhead.ResNet  —  Type ResNet(depth::Integer; pretrain::Bool = false, inchannels::Integer = 3, nclasses::Integer = 1000) Creates a ResNet model with the specified depth. ( reference ) Arguments depth : one of  [18, 34, 50, 101, 152] . The depth of the ResNet model. pretrain : set to  true  to load the model with pre-trained weights for ImageNet inchannels : The number of input channels. nclasses : the number of output classes Advanced users who want more configuration options will be better served by using  resnet . source"},{"id":1066,"pagetitle":"ResNet-like models","title":"Metalhead.WideResNet","ref":"/metalhead/stable/api/resnet/#Metalhead.WideResNet","content":" Metalhead.WideResNet  —  Type WideResNet(depth::Integer; pretrain::Bool = false, inchannels::Integer = 3, nclasses::Integer = 1000) Creates a Wide ResNet model with the specified depth. The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same. ( reference ) Arguments depth : one of  [18, 34, 50, 101, 152] . The depth of the Wide ResNet model. pretrain : set to  true  to load the model with pre-trained weights for ImageNet inchannels : The number of input channels. nclasses : The number of output classes Advanced users who want more configuration options will be better served by using  resnet . source"},{"id":1067,"pagetitle":"ResNet-like models","title":"Metalhead.ResNeXt","ref":"/metalhead/stable/api/resnet/#Metalhead.ResNeXt","content":" Metalhead.ResNeXt  —  Type ResNeXt(depth::Integer; pretrain::Bool = false, cardinality::Integer = 32,\n        base_width::Integer = 4, inchannels::Integer = 3, nclasses::Integer = 1000) Creates a ResNeXt model with the specified depth, cardinality, and base width. ( reference ) Arguments depth : one of  [50, 101, 152] . The depth of the ResNet model. pretrain : set to  true  to load the model with pre-trained weights for ImageNet. Supported configurations are: depth 50, cardinality of 32 and base width of 4. depth 101, cardinality of 32 and base width of 8. depth 101, cardinality of 64 and base width of 4. cardinality : the number of groups to be used in the 3x3 convolution in each block. base_width : the number of feature maps in each group. inchannels : the number of input channels. nclasses : the number of output classes Advanced users who want more configuration options will be better served by using  resnet . source"},{"id":1068,"pagetitle":"ResNet-like models","title":"Metalhead.SEResNet","ref":"/metalhead/stable/api/resnet/#Metalhead.SEResNet","content":" Metalhead.SEResNet  —  Type SEResNet(depth::Integer; pretrain::Bool = false, inchannels::Integer = 3, nclasses::Integer = 1000) Creates a SEResNet model with the specified depth. ( reference ) Arguments depth : one of  [18, 34, 50, 101, 152] . The depth of the ResNet model. pretrain : set to  true  to load the model with pre-trained weights for ImageNet inchannels : the number of input channels. nclasses : the number of output classes Warning SEResNet  does not currently support pretrained weights. Advanced users who want more configuration options will be better served by using  resnet . source"},{"id":1069,"pagetitle":"ResNet-like models","title":"Metalhead.SEResNeXt","ref":"/metalhead/stable/api/resnet/#Metalhead.SEResNeXt","content":" Metalhead.SEResNeXt  —  Type SEResNeXt(depth::Integer; pretrain::Bool = false, cardinality::Integer = 32,\n          base_width::Integer = 4, inchannels::Integer = 3, nclasses::Integer = 1000) Creates a SEResNeXt model with the specified depth, cardinality, and base width. ( reference ) Arguments depth : one of  [50, 101, 152] . The depth of the ResNet model. pretrain : set to  true  to load the model with pre-trained weights for ImageNet cardinality : the number of groups to be used in the 3x3 convolution in each block. base_width : the number of feature maps in each group. inchannels : the number of input channels nclasses : the number of output classes Warning SEResNeXt  does not currently support pretrained weights. Advanced users who want more configuration options will be better served by using  resnet . source"},{"id":1070,"pagetitle":"ResNet-like models","title":"Metalhead.Res2Net","ref":"/metalhead/stable/api/resnet/#Metalhead.Res2Net","content":" Metalhead.Res2Net  —  Type Res2Net(depth::Integer; pretrain::Bool = false, scale::Integer = 4,\n        base_width::Integer = 26, inchannels::Integer = 3,\n        nclasses::Integer = 1000) Creates a Res2Net model with the specified depth, scale, and base width. ( reference ) Arguments depth : one of  [50, 101, 152] . The depth of the Res2Net model. pretrain : set to  true  to load the model with pre-trained weights for ImageNet scale : the number of feature groups in the block. See the  paper  for more details. base_width : the number of feature maps in each group. inchannels : the number of input channels. nclasses : the number of output classes Warning Res2Net  does not currently support pretrained weights. Advanced users who want more configuration options will be better served by using  resnet . source"},{"id":1071,"pagetitle":"ResNet-like models","title":"Metalhead.Res2NeXt","ref":"/metalhead/stable/api/resnet/#Metalhead.Res2NeXt","content":" Metalhead.Res2NeXt  —  Type Res2NeXt(depth::Integer; pretrain::Bool = false, scale::Integer = 4,\n         base_width::Integer = 4, cardinality::Integer = 8,\n         inchannels::Integer = 3, nclasses::Integer = 1000) Creates a Res2NeXt model with the specified depth, scale, base width and cardinality. ( reference ) Arguments depth : one of  [50, 101, 152] . The depth of the Res2Net model. pretrain : set to  true  to load the model with pre-trained weights for ImageNet scale : the number of feature groups in the block. See the  paper  for more details. base_width : the number of feature maps in each group. cardinality : the number of groups in the 3x3 convolutions. inchannels : the number of input channels. nclasses : the number of output classes Warning Res2NeXt  does not currently support pretrained weights. Advanced users who want more configuration options will be better served by using  resnet . source"},{"id":1072,"pagetitle":"ResNet-like models","title":"The mid-level function","ref":"/metalhead/stable/api/resnet/#The-mid-level-function","content":" The mid-level function"},{"id":1073,"pagetitle":"ResNet-like models","title":"Metalhead.resnet","ref":"/metalhead/stable/api/resnet/#Metalhead.resnet","content":" Metalhead.resnet  —  Function resnet(block_type, block_repeats::AbstractVector{<:Integer},\n       downsample_opt::NTuple{2, Any} = (downsample_conv, downsample_identity);\n       cardinality::Integer = 1, base_width::Integer = 64,\n       inplanes::Integer = 64, reduction_factor::Integer = 1,\n       connection = addact, activation = relu,\n       norm_layer = BatchNorm, revnorm::Bool = false,\n       attn_fn = planes -> identity, pool_layer = AdaptiveMeanPool((1, 1)),\n       use_conv::Bool = false, dropblock_prob = nothing,\n       stochastic_depth_prob = nothing, dropout_prob = nothing,\n       imsize::Dims{2} = (256, 256), inchannels::Integer = 3,\n       nclasses::Integer = 1000, kwargs...) Creates a generic ResNet-like model that is used to create The higher-level model constructors like ResNet, Wide ResNet, ResNeXt and Res2Net. For an  even  more generic model API, see  Metalhead.build_resnet . Arguments block_type : The type of block to be used in the model. This can be one of  Metalhead.basicblock ,  Metalhead.bottleneck  and  Metalhead.bottle2neck .  basicblock  is used in the original ResNet paper for ResNet-18 and ResNet-34, and  bottleneck  is used in the original ResNet-50 and ResNet-101 models, as well as for the Wide ResNet and ResNeXt models.  bottle2neck  is introduced in the  Res2Net  paper. block_repeats : A  Vector  of integers specifying the number of times each block is repeated in each stage of the ResNet model. For example,  [3, 4, 6, 3]  is the configuration used in ResNet-50, which has 3 blocks in the first stage, 4 blocks in the second stage, 6 blocks in the third stage and 3 blocks in the fourth stage. downsample_opt : A  NTuple  of two callbacks that are used to determine the downsampling operation to be used in the model. The first callback is used to determine the convolutional operation to be used in the downsampling operation and the second callback is used to determine the identity operation to be used in the downsampling operation. cardinality : The number of groups to be used in the 3x3 convolutional layer in the bottleneck block. This is usually modified from the default value of  1  in the ResNet models to  32  or  64  in the  ResNeXt  models. base_width : The base width of the convolutional layer in the blocks of the model. inplanes : The number of input channels in the first convolutional layer. reduction_factor : The reduction factor used in the model. connection : This is a function that determines the residual connection in the model. For  resnets , either of  Metalhead.addact  or  Metalhead.actadd  is recommended. These decide whether the residual connection is added before or after the activation function. norm_layer : The normalisation layer to be used in the model. revnorm : set to  true  to place the normalisation layers before the convolutions attn_fn : A callback that is used to determine the attention function to be used in the model. See  Metalhead.Layers.squeeze_excite  for an example. pool_layer : A fully-instantiated pooling layer passed in to be used by the classifier head. For example,  AdaptiveMeanPool((1, 1))  is used in the ResNet family by default, but something like  MeanPool((3, 3))  should also work provided the dimensions after applying the pooling layer are compatible with the rest of the classifier head. use_conv : Set to true to use convolutions instead of identity operations in the model. dropblock_prob :  DropBlock  probability to be used in the model. Set to  nothing  to disable DropBlock. See  Metalhead.DropBlock  for more details. stochastic_depth_prob :  StochasticDepth  probability to be used in the model. Set to  nothing  to disable StochasticDepth. See  Metalhead.StochasticDepth  for more details. dropout_prob :  Dropout  probability to be used in the classifier head. Set to  nothing  to disable Dropout. imsize : The size of the input (height, width). inchannels : The number of input channels. nclasses : The number of output classes. kwargs : Additional keyword arguments to be passed to the block builder (note: ignore this argument if you are not sure what it does. To know more about how this works, check out the section of the documentation that talks about builders in Metalhead and specifically for the ResNet block functions). source"},{"id":1074,"pagetitle":"ResNet-like models","title":"Lower-level functions and builders","ref":"/metalhead/stable/api/resnet/#Lower-level-functions-and-builders","content":" Lower-level functions and builders"},{"id":1075,"pagetitle":"ResNet-like models","title":"Block functions","ref":"/metalhead/stable/api/resnet/#Block-functions","content":" Block functions"},{"id":1076,"pagetitle":"ResNet-like models","title":"Metalhead.basicblock","ref":"/metalhead/stable/api/resnet/#Metalhead.basicblock","content":" Metalhead.basicblock  —  Function basicblock(inplanes::Integer, planes::Integer; stride::Integer = 1,\n           reduction_factor::Integer = 1, activation = relu,\n           norm_layer = BatchNorm, revnorm::Bool = false,\n           drop_block = identity, drop_path = identity,\n           attn_fn = planes -> identity) Creates a basic residual block (see  reference ). This function creates the layers. For more configuration options and to see the function used to build the block for the model, see  Metalhead.basicblock_builder . Arguments inplanes : number of input feature maps planes : number of feature maps for the block stride : the stride of the block reduction_factor : the factor by which the input feature maps are reduced before the first convolution. activation : the activation function to use. norm_layer : the normalization layer to use. revnorm : set to  true  to place the normalisation layer before the convolution drop_block : the drop block layer drop_path : the drop path layer attn_fn : the attention function to use. See  squeeze_excite  for an example. source"},{"id":1077,"pagetitle":"ResNet-like models","title":"Metalhead.bottleneck","ref":"/metalhead/stable/api/resnet/#Metalhead.bottleneck","content":" Metalhead.bottleneck  —  Function bottleneck(inplanes::Integer, planes::Integer; stride::Integer,\n           cardinality::Integer = 1, base_width::Integer = 64,\n           reduction_factor::Integer = 1, activation = relu,\n           norm_layer = BatchNorm, revnorm::Bool = false,\n           drop_block = identity, drop_path = identity,\n           attn_fn = planes -> identity) Creates a bottleneck residual block (see  reference ). This function creates the layers. For more configuration options and to see the function used to build the block for the model, see  Metalhead.bottleneck_builder . Arguments inplanes : number of input feature maps planes : number of feature maps for the block stride : the stride of the block cardinality : the number of groups in the convolution. base_width : the number of output feature maps for each convolutional group. reduction_factor : the factor by which the input feature maps are reduced before the first convolution. activation : the activation function to use. norm_layer : the normalization layer to use. revnorm : set to  true  to place the normalisation layer before the convolution drop_block : the drop block layer drop_path : the drop path layer attn_fn : the attention function to use. See  squeeze_excite  for an example. source"},{"id":1078,"pagetitle":"ResNet-like models","title":"Metalhead.bottle2neck","ref":"/metalhead/stable/api/resnet/#Metalhead.bottle2neck","content":" Metalhead.bottle2neck  —  Function bottle2neck(inplanes::Integer, planes::Integer; stride::Integer = 1,\n            cardinality::Integer = 1, base_width::Integer = 26,\n            scale::Integer = 4, activation = relu, norm_layer = BatchNorm,\n            revnorm::Bool = false, attn_fn = planes -> identity) Creates a bottleneck block as described in the Res2Net paper. ( reference ) This function creates the layers. For more configuration options and to see the function used to build the block for the model, see  Metalhead.bottle2neck_builder . Arguments inplanes : number of input feature maps planes : number of feature maps for the block stride : the stride of the block cardinality : the number of groups in the 3x3 convolutions. base_width : the number of output feature maps for each convolutional group. scale : the number of feature groups in the block. See the  paper  for more details. activation : the activation function to use. norm_layer : the normalization layer to use. revnorm : set to  true  to place the batch norm before the convolution attn_fn : the attention function to use. See  squeeze_excite  for an example. source"},{"id":1079,"pagetitle":"ResNet-like models","title":"Downsampling functions","ref":"/metalhead/stable/api/resnet/#Downsampling-functions","content":" Downsampling functions"},{"id":1080,"pagetitle":"ResNet-like models","title":"Metalhead.downsample_identity","ref":"/metalhead/stable/api/resnet/#Metalhead.downsample_identity","content":" Metalhead.downsample_identity  —  Function downsample_identity(inplanes::Integer, outplanes::Integer; kwargs...) Creates an identity downsample layer. This returns  identity  if  inplanes == outplanes . If  outplanes > inplanes , it maps the input to  outplanes  channels using a 1x1 max pooling layer and zero padding. Warning This does not currently support the scenario where  inplanes > outplanes . Arguments inplanes : number of input feature maps outplanes : number of output feature maps Note that kwargs are ignored and only included for compatibility with other downsample layers. source"},{"id":1081,"pagetitle":"ResNet-like models","title":"Metalhead.downsample_conv","ref":"/metalhead/stable/api/resnet/#Metalhead.downsample_conv","content":" Metalhead.downsample_conv  —  Function downsample_conv(inplanes::Integer, outplanes::Integer; stride::Integer = 1,\n                norm_layer = BatchNorm, revnorm::Bool = false) Creates a 1x1 convolutional downsample layer as used in ResNet. Arguments inplanes : number of input feature maps outplanes : number of output feature maps stride : the stride of the convolution norm_layer : the normalization layer to use. revnorm : set to  true  to place the normalisation layer before the convolution source"},{"id":1082,"pagetitle":"ResNet-like models","title":"Metalhead.downsample_pool","ref":"/metalhead/stable/api/resnet/#Metalhead.downsample_pool","content":" Metalhead.downsample_pool  —  Function downsample_pool(inplanes::Integer, outplanes::Integer; stride::Integer = 1,\n                norm_layer = BatchNorm, revnorm::Bool = false) Creates a pooling-based downsample layer as described in the  Bag of Tricks  paper. This adds an average pooling layer of size  (2, 2)  with  stride  followed by a 1x1 convolution. Arguments inplanes : number of input feature maps outplanes : number of output feature maps stride : the stride of the convolution norm_layer : the normalization layer to use. revnorm : set to  true  to place the normalisation layer before the convolution source"},{"id":1083,"pagetitle":"ResNet-like models","title":"Block builders","ref":"/metalhead/stable/api/resnet/#Block-builders","content":" Block builders"},{"id":1084,"pagetitle":"ResNet-like models","title":"Metalhead.basicblock_builder","ref":"/metalhead/stable/api/resnet/#Metalhead.basicblock_builder","content":" Metalhead.basicblock_builder  —  Function basicblock_builder(block_repeats::AbstractVector{<:Integer};\n                   inplanes::Integer = 64, reduction_factor::Integer = 1,\n                   expansion::Integer = 1, norm_layer = BatchNorm,\n                   revnorm::Bool = false, activation = relu,\n                   attn_fn = planes -> identity,\n                   dropblock_prob = nothing, stochastic_depth_prob = nothing,\n                   stride_fn = resnet_stride, planes_fn = resnet_planes,\n                   downsample_tuple = (downsample_conv, downsample_identity)) Builder for creating a basic block for a ResNet model. ( reference ) Arguments block_repeats : number of repeats of a block in each stage inplanes : number of input channels reduction_factor : reduction factor for the number of channels in each stage expansion : expansion factor for the number of channels for the block norm_layer : normalization layer to use revnorm : set to  true  to place normalization layer before the convolution activation : activation function to use attn_fn : attention function to use dropblock_prob : dropblock probability. Set to  nothing  to disable  DropBlock stochastic_depth_prob : stochastic depth probability. Set to  nothing  to disable  StochasticDepth stride_fn : callback for computing the stride of the block planes_fn : callback for computing the number of channels in each block downsample_tuple : two-element tuple of downsample functions to use. The first one is used when the number of channels changes in the block, the second one is used when the number of channels stays the same. source"},{"id":1085,"pagetitle":"ResNet-like models","title":"Metalhead.bottleneck_builder","ref":"/metalhead/stable/api/resnet/#Metalhead.bottleneck_builder","content":" Metalhead.bottleneck_builder  —  Function bottleneck_builder(block_repeats::AbstractVector{<:Integer};\n                   inplanes::Integer = 64, cardinality::Integer = 1,\n                   base_width::Integer = 64, reduction_factor::Integer = 1,\n                   expansion::Integer = 4, norm_layer = BatchNorm,\n                   revnorm::Bool = false, activation = relu,\n                   attn_fn = planes -> identity, dropblock_prob = nothing,\n                   stochastic_depth_prob = nothing, stride_fn = resnet_stride,\n                   planes_fn = resnet_planes,\n                   downsample_tuple = (downsample_conv, downsample_identity)) Builder for creating a bottleneck block for a ResNet/ResNeXt model. ( reference ) Arguments block_repeats : number of repeats of a block in each stage inplanes : number of input channels cardinality : number of groups for the convolutional layer base_width : base width for the convolutional layer reduction_factor : reduction factor for the number of channels in each stage expansion : expansion factor for the number of channels for the block norm_layer : normalization layer to use revnorm : set to  true  to place normalization layer before the convolution activation : activation function to use attn_fn : attention function to use dropblock_prob : dropblock probability. Set to  nothing  to disable  DropBlock stochastic_depth_prob : stochastic depth probability. Set to  nothing  to disable  StochasticDepth stride_fn : callback for computing the stride of the block planes_fn : callback for computing the number of channels in each block downsample_tuple : two-element tuple of downsample functions to use. The first one is used when the number of channels changes in the block, the second one is used when the number of channels stays the same. source"},{"id":1086,"pagetitle":"ResNet-like models","title":"Metalhead.bottle2neck_builder","ref":"/metalhead/stable/api/resnet/#Metalhead.bottle2neck_builder","content":" Metalhead.bottle2neck_builder  —  Function bottle2neck_builder(block_repeats::AbstractVector{<:Integer};\n                    inplanes::Integer = 64, cardinality::Integer = 1,\n                    base_width::Integer = 26, scale::Integer = 4,\n                    expansion::Integer = 4, norm_layer = BatchNorm,\n                    revnorm::Bool = false, activation = relu,\n                    attn_fn = planes -> identity, stride_fn = resnet_stride,\n                    planes_fn = resnet_planes,\n                    downsample_tuple = (downsample_conv, downsample_identity)) Builder for creating a bottle2neck block for a Res2Net model. ( reference ) Arguments block_repeats : number of repeats of a block in each stage inplanes : number of input channels cardinality : number of groups for the convolutional layer base_width : base width for the convolutional layer scale : scale for the number of channels in each block expansion : expansion factor for the number of channels for the block norm_layer : normalization layer to use revnorm : set to  true  to place normalization layer before the convolution activation : activation function to use attn_fn : attention function to use stride_fn : callback for computing the stride of the block planes_fn : callback for computing the number of channels in each block downsample_tuple : two-element tuple of downsample functions to use. The first one is used when the number of channels changes in the block, the second one is used when the number of channels stays the same. source"},{"id":1087,"pagetitle":"ResNet-like models","title":"Generic ResNet model builder","ref":"/metalhead/stable/api/resnet/#Generic-ResNet-model-builder","content":" Generic ResNet model builder"},{"id":1088,"pagetitle":"ResNet-like models","title":"Metalhead.build_resnet","ref":"/metalhead/stable/api/resnet/#Metalhead.build_resnet","content":" Metalhead.build_resnet  —  Function build_resnet(img_dims, stem, get_layers, block_repeats::AbstractVector{<:Integer},\n             connection, classifier_fn) Creates a generic ResNet-like model. Info This is a very generic, flexible but low level function that can be used to create any of the ResNet variants. For a more user friendly function, see  Metalhead.resnet . Arguments img_dims : The dimensions of the input image. This is used to determine the number of feature maps to be passed to the classifier. This should be a tuple of the form  (height, width, channels) . stem : The stem of the ResNet model. The stem should be created outside of this function and passed in as an argument. This is done to allow for more flexibility in creating the stem.  resnet_stem  is a helper function that Metalhead provides which is recommended for creating the stem. get_layers  is a function that takes in two inputs - the  stage_idx , or the index of the stage, and the  block_idx , or the index of the block within the stage. It returns a tuple of layers. If the tuple returned by  get_layers  has more than one element, then  connection  is used to splat this tuple into  Parallel  - if not, then the only element of the tuple is directly inserted into the network.  get_layers  is a very specific function and should not be created on its own. Instead, use one of the builders provided by Metalhead to create it. block_repeats : This is a  Vector  of integers that specifies the number of repeats of each block in each stage. connection : This is a function that determines the residual connection in the model. For  resnets , either of  Metalhead.addact  or  Metalhead.actadd  is recommended. classifier_fn : This is a function that takes in the number of feature maps and returns a classifier. This is usually built as a closure using a function like  Metalhead.create_classifier . For example, if the number of output classes is  nclasses , then the function can be defined as  channels -> create_classifier(channels, nclasses) . source"},{"id":1089,"pagetitle":"ResNet-like models","title":"Utility callbacks","ref":"/metalhead/stable/api/resnet/#Utility-callbacks","content":" Utility callbacks"},{"id":1090,"pagetitle":"ResNet-like models","title":"Metalhead.resnet_planes","ref":"/metalhead/stable/api/resnet/#Metalhead.resnet_planes","content":" Metalhead.resnet_planes  —  Function resnet_planes(block_repeats::AbstractVector{<:Integer}) Default callback for determining the number of channels in each block in a ResNet model. Arguments block_repeats : A  Vector  of integers specifying the number of times each block is repeated in each stage of the ResNet model. For example,  [3, 4, 6, 3]  is the configuration used in ResNet-50, which has 3 blocks in the first stage, 4 blocks in the second stage, 6 blocks in the third stage and 3 blocks in the fourth stage. source"},{"id":1091,"pagetitle":"ResNet-like models","title":"Metalhead.resnet_stride","ref":"/metalhead/stable/api/resnet/#Metalhead.resnet_stride","content":" Metalhead.resnet_stride  —  Function resnet_stride(stage_idx::Integer, block_idx::Integer) Default callback for determining the stride of a block in a ResNet model. Returns  2  for the first block in every stage except the first stage and  1  for all other blocks. Arguments stage_idx : The index of the stage in the ResNet model. block_idx : The index of the block in the stage. source"},{"id":1092,"pagetitle":"ResNet-like models","title":"Metalhead.resnet_stem","ref":"/metalhead/stable/api/resnet/#Metalhead.resnet_stem","content":" Metalhead.resnet_stem  —  Function resnet_stem(; stem_type = :default, inchannels::Integer = 3, replace_stem_pool = false,\n              norm_layer = BatchNorm, activation = relu) Builds a stem to be used in a ResNet model. See the  stem  argument of  resnet  for details on how to use this function. Arguments stem_type : The type of stem to be built. One of  [:default, :deep, :deep_tiered] . :default : Builds a stem based on the default ResNet stem, which consists of a single 7x7 convolution with stride 2 and a normalisation layer followed by a 3x3 max pooling layer with stride 2. :deep : This borrows ideas from other papers ( InceptionResNetv2 , for example) in using a deeper stem with 3 successive 3x3 convolutions having normalisation layers after each one. This is followed by a 3x3 max pooling layer with stride 2. :deep_tiered : A variant of the  :deep  stem that has a larger width in the second convolution. This is an experimental variant from the  timm  library in Python that shows peformance improvements over the  :deep  stem in some cases. inchannels : number of input channels replace_pool : Set to true to replace the max pooling layers with a 3x3 convolution + normalization with a stride of two. norm_layer : The normalisation layer used in the stem. activation : The activation function used in the stem. source"},{"id":1095,"pagetitle":"Model Utilities","title":"Model utilities","ref":"/metalhead/stable/api/utilities/#Model-utilities","content":" Model utilities Metalhead provides some utility functions for making it easier to work with the models inside the library or to build new ones. The API reference for these is documented below."},{"id":1096,"pagetitle":"Model Utilities","title":"Metalhead.backbone","ref":"/metalhead/stable/api/utilities/#Metalhead.backbone","content":" Metalhead.backbone  —  Function backbone(model) This function returns the backbone of a model that can be used for feature extraction. A  Flux.Chain  is returned, which can be indexed/sliced into to get the desired layer(s). Note that the model used here as input must be the \"camel-cased\" version of the model, e.g.  ResNet  instead of  resnet . source"},{"id":1097,"pagetitle":"Model Utilities","title":"Metalhead.classifier","ref":"/metalhead/stable/api/utilities/#Metalhead.classifier","content":" Metalhead.classifier  —  Function classifier(model) This function returns the classifier head of a model. This is sometimes useful for fine-tuning a model on a different dataset. A  Flux.Chain  is returned, which can be indexed/sliced into to get the desired layer(s). Note that the model used here as input must be the \"camel-cased\" version of the model, e.g.  ResNet  instead of  resnet . source"},{"id":1100,"pagetitle":"Vision Transformer models","title":"Vision Transformer models","ref":"/metalhead/stable/api/vit/#Vision-Transformer-models","content":" Vision Transformer models This is the API reference for the Vision Transformer models supported by Metalhead.jl."},{"id":1101,"pagetitle":"Vision Transformer models","title":"The higher-level model constructors","ref":"/metalhead/stable/api/vit/#The-higher-level-model-constructors","content":" The higher-level model constructors"},{"id":1102,"pagetitle":"Vision Transformer models","title":"Metalhead.ViT","ref":"/metalhead/stable/api/vit/#Metalhead.ViT","content":" Metalhead.ViT  —  Type ViT(config::Symbol = base; imsize::Dims{2} = (224, 224), inchannels::Integer = 3,\n    patch_size::Dims{2} = (16, 16), pool = :class, nclasses::Integer = 1000) Creates a Vision Transformer (ViT) model. ( reference ). Arguments config : the model configuration, one of  [:tiny, :small, :base, :large, :huge, :giant, :gigantic] imsize : image size inchannels : number of input channels patch_size : size of the patches pool : pooling type, either :class or :mean nclasses : number of classes in the output See also  Metalhead.vit . source"},{"id":1103,"pagetitle":"Vision Transformer models","title":"The mid-level functions","ref":"/metalhead/stable/api/vit/#The-mid-level-functions","content":" The mid-level functions"},{"id":1104,"pagetitle":"Vision Transformer models","title":"Metalhead.vit","ref":"/metalhead/stable/api/vit/#Metalhead.vit","content":" Metalhead.vit  —  Function vit(imsize::Dims{2} = (256, 256); inchannels::Integer = 3, patch_size::Dims{2} = (16, 16),\n    embedplanes = 768, depth = 6, nheads = 16, mlp_ratio = 4.0, dropout_prob = 0.1,\n    emb_dropout_prob = 0.1, pool = :class, nclasses::Integer = 1000) Creates a Vision Transformer (ViT) model. ( reference ). Arguments imsize : image size inchannels : number of input channels patch_size : size of the patches embedplanes : the number of channels after the patch embedding depth : number of blocks in the transformer nheads : number of attention heads in the transformer mlpplanes : number of hidden channels in the MLP block in the transformer dropout_prob : dropout probability emb_dropout : dropout probability for the positional embedding layer pool : pooling type, either :class or :mean nclasses : number of classes in the output source"},{"id":1107,"pagetitle":"Contributing to Metalhead","title":"Contribute to Metalhead.jl","ref":"/metalhead/stable/contributing/#contributing","content":" Contribute to Metalhead.jl We welcome contributions from anyone to Metalhead.jl! Thank you for taking the time to make our ecosystem better. You can contribute by fixing bugs, adding new models, or adding pre-trained weights. If you aren't ready to write some code, but you think you found a bug or have a feature request, please  post an issue . Before continuing, make sure you read the  FluxML contributing guide  for general guidelines and tips."},{"id":1108,"pagetitle":"Contributing to Metalhead","title":"Fixing bugs","ref":"/metalhead/stable/contributing/#Fixing-bugs","content":" Fixing bugs To fix a bug in Metalhead.jl, you can  open a PR . It would be helpful to file an issue first so that we can confirm the bug."},{"id":1109,"pagetitle":"Contributing to Metalhead","title":"Adding models","ref":"/metalhead/stable/contributing/#Adding-models","content":" Adding models To add a new model architecture to Metalhead.jl, you can  open a PR . Keep in mind a few guiding principles for how this package is designed: reuse layers from Flux as much as possible (e.g. use  Parallel  before defining a  Bottleneck  struct) adhere as closely as possible to a reference such as a published paper (i.e. the structure of your model should follow intuitively from the paper) use generic functional builders (e.g.  Metalhead.resnet  is the underlying function that builds \"ResNet-like\" models) use multiple dispatch to add convenience constructors that wrap your functional builder When in doubt, just open a PR! We are more than happy to help review your code to help it align with the rest of the library. After adding a model, you might consider adding some pre-trained weights (see below)."},{"id":1110,"pagetitle":"Contributing to Metalhead","title":"Adding pre-trained weights","ref":"/metalhead/stable/contributing/#Adding-pre-trained-weights","content":" Adding pre-trained weights To add pre-trained weights for an existing model or new model, you can  open a PR . Below, we describe the steps you should follow to get there. All Metalhead.jl model artifacts are hosted on HuggingFace. You can find the FluxML account  here . This  documentation from HuggingFace  will provide you with an introduction to their ModelHub. In short, the Model Hub is a collection of Git repositories, similar to Julia packages on GitHub. This means you can  make a pull request to our HuggingFace repositories  to upload updated weight artifacts just like you would make a PR on GitHub to upload code. Train your model or port the weights from another framework. Save the model state using  BSON.jl  with  BSON.@save \"modelname.bson\" model_state=Flux.state(model) . It is important that your model is saved under the key  model_state . Compress the saved model as a tarball using  tar -cvzf modelname.tar.gz modelname.bson . Obtain the SHAs (see the  Pkg docs ). Edit the  Artifacts.toml  file in the Metalhead.jl repository and add entry for your model. You can leave the URL empty for now. Open a PR on Metalhead.jl. Be sure to ping a maintainer (e.g.  @darsnack  or  @theabhirath ) to let us know that you are adding a pre-trained weight. We will create a model repository on HuggingFace if it does not already exist. Open a PR to the  corresponding HuggingFace repo . Do this by going to the \"Community\" tab in the HuggingFace repository. PRs and discussions are shown as the same thing in the HuggingFace web app. You can use your local Git program to make clone the repo and make PRs if you wish. Check out the  guide on PRs to HuggingFace  for more information. Copy the download URL for the model file that you added to HuggingFace. Make sure to grab the URL for a specific commit and not for the  main  branch. Update your Metalhead.jl PR by adding the URL to the Artifacts.toml. If the tests pass for your weights, we will merge your PR! Your model should pass the  acctest  function in the Metalhead.jl test suite. If your model already exists in the repo, then these tests are already in place, and you can add your model configuration to the  PRETRAINED_MODELS  list in the  runtests.jl  file. Please refer to the ResNet tests as an example. If you want to fix existing weights, then you can follow the same set of steps. See the  scripts/  folder in the repo for some helpful scripts that can be used to automate some of these steps."},{"id":1113,"pagetitle":"Using the ResNet model family in Metalhead.jl","title":"Using the ResNet model family in Metalhead.jl","ref":"/metalhead/stable/howto/resnet/#Using-the-ResNet-model-family-in-Metalhead.jl","content":" Using the ResNet model family in Metalhead.jl ResNets are one of the most common convolutional neural network (CNN) models used today. Originally proposed by He et al. in  Deep Residual Learning for Image Recognition , they use a residual structure to learn identity mappings that strengthens gradient propagation, thereby helping to prevent the vanishing gradient problem and allow the advent of truly deep neural networks as used today. Many variants on the original ResNet structure have since become widely used such as  Wide-ResNet ,  ResNeXt ,  SE-ResNet  and  Res2Net . Apart from suggesting modifications to the structure of the residual block, papers have also suggested modifying the stem of the network, adding newer regularisation options in the form of stochastic depth and DropBlock, and changing the downsampling path for the blocks to improve performance. Metalhead provides an extensible, hackable yet powerful interface for working with ResNets that provides built-in toggles for commonly used options in papers and other deep learning libraries, while also allowing the user to build custom model structures if they want very easily."},{"id":1114,"pagetitle":"Using the ResNet model family in Metalhead.jl","title":"Pre-trained models","ref":"/metalhead/stable/howto/resnet/#Pre-trained-models","content":" Pre-trained models Metalhead provides a variety of pretrained models in the ResNet family to allow users to get started quickly with tasks like transfer learning. Pretrained models for  ResNet  with depth 18, 34, 50, 101 and 152 is supported, as is  WideResNet  with depths 50 and 101.  ResNeXt  also supports some configurations of pretrained models - to know more, check out the documentation for the model. This is as easy as setting the  pretrain  keyword to  true  when constructing the model. For example, to load a pretrained  ResNet  with depth 50, you can do the following: using Metalhead\n\nmodel = ResNet(50; pretrain=true) To check out more about using pretrained models, check out the  pretrained models guide ."},{"id":1115,"pagetitle":"Using the ResNet model family in Metalhead.jl","title":"The mid-level function","ref":"/metalhead/stable/howto/resnet/#The-mid-level-function","content":" The mid-level function Metalhead also provides a function for users looking to customise the ResNet family of models further. This function is named  Metalhead.resnet  and has a detailed docstring that describes all the various customisation options. You may want to open the above link in another tab, because we're going to be referring to it extensively to build a ResNet model of our liking. First, let's take a peek at how we would write the vanilla ResNet-18 model using this function. At its core, a residual network is a convolutional network split into stages, where each stage contains a \"residual\" block repeated several times. The Metalhead.jl design reflects this. While there are many keyword arguments that we can configure, there are two required positional arguments–the block type and the number of times a block is repeated in each stage. For all other options, the default values work well. The original ResNet paper suggest using a \"basic block\" type and a block repetition of two. So we can write the ResNet-18 model as follows: resnet18 = Metalhead.resnet(Metalhead.basicblock, [2, 2, 2, 2]) What if we want to customise the number of output classes? That's easy; the model has several keyword arguments, one of which allows this. The docstring tells us that it is  nclasses , and so we can write: resnet18 = Metalhead.resnet(Metalhead.basicblock, [2, 2, 2, 2]; nclasses = 10) Let's try customising this further. Say I want to make a ResNet-50-like model, but with  StochasticDepth  to provide even more regularisation, and also a custom pooling layer such as  AdaptiveMeanMaxPool . Both of these options are provided by Metalhead out of the box, and so we can write: using Metalhead: Layers # AdaptiveMeanMaxPool is in the Layers module in Metalhead\n\ncustom_resnet = Metalhead.resnet(Metalhead.bottleneck, [3, 4, 6, 3];\n                                 pool_layer = Layers.AdaptiveMeanMaxPool((1, 1)),\n                                 stochastic_depth_prob = 0.2) To make this a ResNeXt-like model, all we need to do is configure the cardinality and the  base width: custom_resnet = Metalhead.resnet(Metalhead.bottleneck, [3, 4, 6, 3];\n                                 cardinality = 32, base_width = 4,\n                                 pool_layer = Layers.AdaptiveMeanMaxPool((1, 1)),\n                                 stochastic_depth_prob = 0.2) And we have a custom model, built with minimal effort! The documentation for  Metalhead.resnet  has been written with extensive care and in as much detail as possible to facilitate ease of use. Still, if you find anything difficult to understand, feel free to open an issue and we will be happy to help you out, and to improve the documentation where necessary."},{"id":1120,"pagetitle":"Working with pre-trained models from Metalhead","title":"Working with pre-trained models from Metalhead","ref":"/metalhead/stable/tutorials/pretrained/#pretrained","content":" Working with pre-trained models from Metalhead Using a model from Metalhead is as simple as selecting a model from the table of  available models . For example, below we use the pre-trained ResNet-18 model. using Metalhead\n\nmodel = ResNet(18; pretrain = true); ResNet(\n  Chain(\n    Chain(\n      Chain(\n        Conv((7, 7), 3 => 64, pad=3, stride=2, bias=false),  # 9_408 parameters\n        BatchNorm(64, relu),            # 128 parameters, plus 128\n        MaxPool((3, 3), pad=1, stride=2),\n      ),\n      Chain(\n        Parallel(\n          addact(NNlib.relu, ...),\n          identity,\n          Chain(\n            Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n            BatchNorm(64),              # 128 parameters, plus 128\n            NNlib.relu,\n            Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n            BatchNorm(64),              # 128 parameters, plus 128\n          ),\n        ),\n        Parallel(\n          addact(NNlib.relu, ...),\n          identity,\n          Chain(\n            Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n            BatchNorm(64),              # 128 parameters, plus 128\n            NNlib.relu,\n            Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n            BatchNorm(64),              # 128 parameters, plus 128\n          ),\n        ),\n      ),\n      Chain(\n        Parallel(\n          addact(NNlib.relu, ...),\n          Chain(\n            Conv((1, 1), 64 => 128, stride=2, bias=false),  # 8_192 parameters\n            BatchNorm(128),             # 256 parameters, plus 256\n          ),\n          Chain(\n            Conv((3, 3), 64 => 128, pad=1, stride=2, bias=false),  # 73_728 parameters\n            BatchNorm(128),             # 256 parameters, plus 256\n            NNlib.relu,\n            Conv((3, 3), 128 => 128, pad=1, bias=false),  # 147_456 parameters\n            BatchNorm(128),             # 256 parameters, plus 256\n          ),\n        ),\n        Parallel(\n          addact(NNlib.relu, ...),\n          identity,\n          Chain(\n            Conv((3, 3), 128 => 128, pad=1, bias=false),  # 147_456 parameters\n            BatchNorm(128),             # 256 parameters, plus 256\n            NNlib.relu,\n            Conv((3, 3), 128 => 128, pad=1, bias=false),  # 147_456 parameters\n            BatchNorm(128),             # 256 parameters, plus 256\n          ),\n        ),\n      ),\n      Chain(\n        Parallel(\n          addact(NNlib.relu, ...),\n          Chain(\n            Conv((1, 1), 128 => 256, stride=2, bias=false),  # 32_768 parameters\n            BatchNorm(256),             # 512 parameters, plus 512\n          ),\n          Chain(\n            Conv((3, 3), 128 => 256, pad=1, stride=2, bias=false),  # 294_912 parameters\n            BatchNorm(256),             # 512 parameters, plus 512\n            NNlib.relu,\n            Conv((3, 3), 256 => 256, pad=1, bias=false),  # 589_824 parameters\n            BatchNorm(256),             # 512 parameters, plus 512\n          ),\n        ),\n        Parallel(\n          addact(NNlib.relu, ...),\n          identity,\n          Chain(\n            Conv((3, 3), 256 => 256, pad=1, bias=false),  # 589_824 parameters\n            BatchNorm(256),             # 512 parameters, plus 512\n            NNlib.relu,\n            Conv((3, 3), 256 => 256, pad=1, bias=false),  # 589_824 parameters\n            BatchNorm(256),             # 512 parameters, plus 512\n          ),\n        ),\n      ),\n      Chain(\n        Parallel(\n          addact(NNlib.relu, ...),\n          Chain(\n            Conv((1, 1), 256 => 512, stride=2, bias=false),  # 131_072 parameters\n            BatchNorm(512),             # 1_024 parameters, plus 1_024\n          ),\n          Chain(\n            Conv((3, 3), 256 => 512, pad=1, stride=2, bias=false),  # 1_179_648 parameters\n            BatchNorm(512),             # 1_024 parameters, plus 1_024\n            NNlib.relu,\n            Conv((3, 3), 512 => 512, pad=1, bias=false),  # 2_359_296 parameters\n            BatchNorm(512),             # 1_024 parameters, plus 1_024\n          ),\n        ),\n        Parallel(\n          addact(NNlib.relu, ...),\n          identity,\n          Chain(\n            Conv((3, 3), 512 => 512, pad=1, bias=false),  # 2_359_296 parameters\n            BatchNorm(512),             # 1_024 parameters, plus 1_024\n            NNlib.relu,\n            Conv((3, 3), 512 => 512, pad=1, bias=false),  # 2_359_296 parameters\n            BatchNorm(512),             # 1_024 parameters, plus 1_024\n          ),\n        ),\n      ),\n    ),\n    Chain(\n      AdaptiveMeanPool((1, 1)),\n      MLUtils.flatten,\n      Dense(512 => 1000),               # 513_000 parameters\n    ),\n  ),\n)         # Total: 62 trainable arrays, 11_689_512 parameters,\n          # plus 40 non-trainable, 9_600 parameters, summarysize 44.654 MiB."},{"id":1121,"pagetitle":"Working with pre-trained models from Metalhead","title":"Using pre-trained models as feature extractors","ref":"/metalhead/stable/tutorials/pretrained/#Using-pre-trained-models-as-feature-extractors","content":" Using pre-trained models as feature extractors The  backbone  and  classifier  functions do exactly what their names suggest - they are used to extract the backbone and classifier of a model respectively. For example, to extract the backbone of a pre-trained ResNet-18 model: backbone(model); Chain(\n  Chain(\n    Conv((7, 7), 3 => 64, pad=3, stride=2, bias=false),  # 9_408 parameters\n    BatchNorm(64, relu),                # 128 parameters, plus 128\n    MaxPool((3, 3), pad=1, stride=2),\n  ),\n  Chain(\n    Parallel(\n      addact(NNlib.relu, ...),\n      identity,\n      Chain(\n        Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n        BatchNorm(64),                  # 128 parameters, plus 128\n        NNlib.relu,\n        Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n        BatchNorm(64),                  # 128 parameters, plus 128\n      ),\n    ),\n    Parallel(\n      addact(NNlib.relu, ...),\n      identity,\n      Chain(\n        Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n        BatchNorm(64),                  # 128 parameters, plus 128\n        NNlib.relu,\n        Conv((3, 3), 64 => 64, pad=1, bias=false),  # 36_864 parameters\n        BatchNorm(64),                  # 128 parameters, plus 128\n      ),\n    ),\n  ),\n  Chain(\n    Parallel(\n      addact(NNlib.relu, ...),\n      Chain(\n        Conv((1, 1), 64 => 128, stride=2, bias=false),  # 8_192 parameters\n        BatchNorm(128),                 # 256 parameters, plus 256\n      ),\n      Chain(\n        Conv((3, 3), 64 => 128, pad=1, stride=2, bias=false),  # 73_728 parameters\n        BatchNorm(128),                 # 256 parameters, plus 256\n        NNlib.relu,\n        Conv((3, 3), 128 => 128, pad=1, bias=false),  # 147_456 parameters\n        BatchNorm(128),                 # 256 parameters, plus 256\n      ),\n    ),\n    Parallel(\n      addact(NNlib.relu, ...),\n      identity,\n      Chain(\n        Conv((3, 3), 128 => 128, pad=1, bias=false),  # 147_456 parameters\n        BatchNorm(128),                 # 256 parameters, plus 256\n        NNlib.relu,\n        Conv((3, 3), 128 => 128, pad=1, bias=false),  # 147_456 parameters\n        BatchNorm(128),                 # 256 parameters, plus 256\n      ),\n    ),\n  ),\n  Chain(\n    Parallel(\n      addact(NNlib.relu, ...),\n      Chain(\n        Conv((1, 1), 128 => 256, stride=2, bias=false),  # 32_768 parameters\n        BatchNorm(256),                 # 512 parameters, plus 512\n      ),\n      Chain(\n        Conv((3, 3), 128 => 256, pad=1, stride=2, bias=false),  # 294_912 parameters\n        BatchNorm(256),                 # 512 parameters, plus 512\n        NNlib.relu,\n        Conv((3, 3), 256 => 256, pad=1, bias=false),  # 589_824 parameters\n        BatchNorm(256),                 # 512 parameters, plus 512\n      ),\n    ),\n    Parallel(\n      addact(NNlib.relu, ...),\n      identity,\n      Chain(\n        Conv((3, 3), 256 => 256, pad=1, bias=false),  # 589_824 parameters\n        BatchNorm(256),                 # 512 parameters, plus 512\n        NNlib.relu,\n        Conv((3, 3), 256 => 256, pad=1, bias=false),  # 589_824 parameters\n        BatchNorm(256),                 # 512 parameters, plus 512\n      ),\n    ),\n  ),\n  Chain(\n    Parallel(\n      addact(NNlib.relu, ...),\n      Chain(\n        Conv((1, 1), 256 => 512, stride=2, bias=false),  # 131_072 parameters\n        BatchNorm(512),                 # 1_024 parameters, plus 1_024\n      ),\n      Chain(\n        Conv((3, 3), 256 => 512, pad=1, stride=2, bias=false),  # 1_179_648 parameters\n        BatchNorm(512),                 # 1_024 parameters, plus 1_024\n        NNlib.relu,\n        Conv((3, 3), 512 => 512, pad=1, bias=false),  # 2_359_296 parameters\n        BatchNorm(512),                 # 1_024 parameters, plus 1_024\n      ),\n    ),\n    Parallel(\n      addact(NNlib.relu, ...),\n      identity,\n      Chain(\n        Conv((3, 3), 512 => 512, pad=1, bias=false),  # 2_359_296 parameters\n        BatchNorm(512),                 # 1_024 parameters, plus 1_024\n        NNlib.relu,\n        Conv((3, 3), 512 => 512, pad=1, bias=false),  # 2_359_296 parameters\n        BatchNorm(512),                 # 1_024 parameters, plus 1_024\n      ),\n    ),\n  ),\n)         # Total: 60 trainable arrays, 11_176_512 parameters,\n          # plus 40 non-trainable, 9_600 parameters, summarysize 42.693 MiB. The  backbone  function could also be useful for people looking to just use specific sections of the model for transfer learning. The function returns a  Chain  of the layers of the model, so you can easily index into it to get the layers you want. For example, to get the first five layers of a pre-trained ResNet model, you can just write  backbone(model)[1:5] ."},{"id":1122,"pagetitle":"Working with pre-trained models from Metalhead","title":"Training","ref":"/metalhead/stable/tutorials/pretrained/#Training","content":" Training Now, we can use this model with Flux like any other model. First, let's check the accuracy on a test image from ImageNet. using Images\n\n# test image\nimg = Images.load(download(\"https://cdn.pixabay.com/photo/2015/05/07/11/02/guitar-756326_960_720.jpg\")); We'll use the popular  DataAugmentation.jl  library to crop our input image, convert it to a plain array, and normalize the pixels. using DataAugmentation\nusing Flux\nusing Flux: onecold\n\nDATA_MEAN = (0.485, 0.456, 0.406)\nDATA_STD = (0.229, 0.224, 0.225)\n\naugmentations = CenterCrop((224, 224)) |>\n                ImageToTensor() |>\n                Normalize(DATA_MEAN, DATA_STD)\n\ndata = apply(augmentations, Image(img)) |> itemdata\n\n# ImageNet labels\nlabels = readlines(download(\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"))\n\nprintln(onecold(model(Flux.unsqueeze(data, 4)), labels)) [\"acoustic guitar\"] That is fairly accurate! Below, we train the model on some randomly generated data: using Optimisers\nusing Flux: onehotbatch\nusing Flux.Losses: logitcrossentropy\n\nbatchsize = 1\ndata = [(rand(Float32, 224, 224, 3, batchsize), onehotbatch(rand(1:1000, batchsize), 1:1000))\n        for _ in 1:3]\nopt = Optimisers.Adam()\nstate = Optimisers.setup(rule, model);  # initialise this optimiser's state\nfor (i, (image, y)) in enumerate(data)\n    @info \"Starting batch $i ...\"\n    gs, _ = gradient(model, image) do m, x  # calculate the gradients\n        logitcrossentropy(m(x), y)\n    end;\n    state, model = Optimisers.update(state, model, gs);\nend"},{"id":1125,"pagetitle":"A guide to getting started with Metalhead","title":"A guide to getting started with Metalhead","ref":"/metalhead/stable/tutorials/quickstart/#A-guide-to-getting-started-with-Metalhead","content":" A guide to getting started with Metalhead Metalhead.jl is a library written in Flux.jl that is a collection of image models, layers and utilities for deep learning in computer vision."},{"id":1126,"pagetitle":"A guide to getting started with Metalhead","title":"Model architectures and pre-trained models","ref":"/metalhead/stable/tutorials/quickstart/#Model-architectures-and-pre-trained-models","content":" Model architectures and pre-trained models In Metalhead.jl, camel-cased functions mimicking the naming style followed in the paper such as  ResNet  or  MobileNetv3  are considered the \"higher\" level API for models. These are the functions that end-users who do not want to experiment much with model architectures should use. To use these models, simply call the function of the model: using Metalhead\n\nmodel = ResNet(18); The API reference contains the documentation and options for each model function. These models also support the option for loading pre-trained weights from ImageNet. Note Metalhead is still under active development and thus not all models have pre-trained weights supported. While we are working on expanding the footprint of the pre-trained models, if you would like to help contribute model weights yourself, please check out the  contributing guide  guide. To use a pre-trained model, just instantiate the model with the  pretrain  keyword argument set to  true : using Metalhead\n  \nmodel = ResNet(18; pretrain = true); Refer to the  pretraining guide  for more details on how to use pre-trained models."},{"id":1127,"pagetitle":"A guide to getting started with Metalhead","title":"More model configuration options","ref":"/metalhead/stable/tutorials/quickstart/#More-model-configuration-options","content":" More model configuration options For users who want to use more options for model configuration, Metalhead provides a \"mid-level\" API for models. These are the model functions that are in lowercase such as  resnet  or  mobilenetv3 . End-users who want to experiment with model architectures should use these functions. These models do not support the option for loading pre-trained weights from ImageNet out of the box, although one can always load weights explicitly using the  loadmodel!  function from Flux. To use any of these models, check out the docstrings for the model functions (these are documented in the API reference). Note that these functions typically require more configuration options to be passed in, but offer a lot more flexibility in terms of model architecture. Metalhead defines as many default options as possible so as to make it easier for the user to pick and choose specific options to customise."},{"id":1128,"pagetitle":"A guide to getting started with Metalhead","title":"Builders for the advanced user","ref":"/metalhead/stable/tutorials/quickstart/#Builders-for-the-advanced-user","content":" Builders for the advanced user For users who want the ability to customise their models as much as possible, Metalhead offers a powerful low-level interface. These are known as  builders  and allow the user to hack into the core of models and build them up as per their liking. Most users will not need to use builders since a large number of configuration options are exposed at the mid-level API. However, for package developers and users who want to build customised versions of their own models, the low-level API provides the customisability required while still reducing user code."}]