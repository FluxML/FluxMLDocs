<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/'</script><script charset="utf-8" src="../../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Zygote.jl 📚 (gradient, ...) · Flux</title><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner="" src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/><link href="nothing/flux/stable/training/zygote/" rel="canonical"/><link href="../../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link active nav-item" href="../../../">Flux</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Building Blocks</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a><a class="nav-link nav-item" href="../../../../functors/">Functors</a></ul></div><div class="nav-mega-column"><div class="column-header">Automatic differentiation libraries</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../zygote/">Zygote</a></ul></div><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Training</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Data Wrangling</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mlutils/">MLUtils</a><a class="nav-link nav-item" href="../../../../onehotarrays/">OneHotArrays</a></ul></div><div class="nav-mega-column"><div class="column-header">Data Augmentation</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../dataaugmentation/">DataAugmentation</a></ul></div><div class="nav-mega-column"><div class="column-header">Datasets</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mldatasets/">MLDatasets</a></ul></div><div class="nav-mega-column"><div class="column-header">Schedulers</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../paramschedulers/dev/">ParameterSchedulers</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Models</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Computer Vision</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../metalhead/">Metalhead</a></ul></div><div class="nav-mega-column"><div class="column-header">Natural Language Processing</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../transformers/">Transformers</a></ul></div></div></div><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="Flux logo" class="docs-light-only" src="../../assets/logo.png"/><img alt="Flux logo" class="docs-dark-only" src="../../assets/logo-dark.png"/></a><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><span class="tocitem">Getting Started</span><ul><li><a class="tocitem" href="../../">Welcome</a></li><li><a class="tocitem" href="../../models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../models/basics/">Gradients and Layers</a></li></ul></li><li><span class="tocitem">Building Models</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers 📚</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions 📚</a></li><li><a class="tocitem" href="../../models/nnlib/">NNlib.jl 📚 (<code>softmax</code>, <code>conv</code>, ...)</a></li></ul></li><li><span class="tocitem">Handling Data</span><ul><li><a class="tocitem" href="../../data/mlutils/">MLUtils.jl 📚 (<code>DataLoader</code>, ...)</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl 📚 (<code>onehot</code>, ...)</a></li></ul></li><li><span class="tocitem">Training Models</span><ul><li><a class="tocitem" href="../training/">Training</a></li><li><a class="tocitem" href="../../models/regularisation/">Regularisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions 📚</a></li><li><a class="tocitem" href="../optimisers/">Optimisation Rules 📚</a></li><li><a class="tocitem" href="../callbacks/">Callback Helpers 📚</a></li><li class="is-active"><a class="tocitem" href="">Zygote.jl 📚 (<code>gradient</code>, ...)</a><ul class="internal"><li><a class="tocitem" href="#Implicit-style"><span>Implicit style</span></a></li><li><a class="tocitem" href="#Explicit-style"><span>Explicit style</span></a></li><li><a class="tocitem" href="#ChainRules"><span>ChainRules</span></a></li></ul></li></ul></li><li><span class="tocitem">Model Tools</span><ul><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference 📚</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation 📚</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested 📚</a></li><li><a class="tocitem" href="../../models/functors/">Functors.jl 📚 (<code>fmap</code>, ...)</a></li></ul></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li><li><a class="tocitem" href="../../ecosystem/">Flux's Ecosystem</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../models/advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Training Models</a></li><li class="is-active"><a href="">Zygote.jl 📚 (<code>gradient</code>, ...)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Zygote.jl 📚 (<code>gradient</code>, ...)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/training/zygote.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Automatic-Differentiation-using-Zygote.jl"><a class="docs-heading-anchor" href="#Automatic-Differentiation-using-Zygote.jl">Automatic Differentiation using Zygote.jl</a><a id="Automatic-Differentiation-using-Zygote.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation-using-Zygote.jl" title="Permalink"></a></h1><p>Flux re-exports the <code>gradient</code> from <a href="https://github.com/FluxML/Zygote.jl">Zygote</a>, and uses this function within <a href="../training/#Flux.Optimise.train!"><code>train!</code></a> to differentiate the model. Zygote has its own <a href="https://fluxml.ai/Zygote.jl/dev/">documentation</a>, in particular listing some <a href="https://fluxml.ai/Zygote.jl/dev/limitations/">important limitations</a>.</p><h2 id="Implicit-style"><a class="docs-heading-anchor" href="#Implicit-style">Implicit style</a><a id="Implicit-style-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-style" title="Permalink"></a></h2><p>Flux uses primarily what Zygote calls "implicit" gradients, <a href="https://fluxml.ai/Zygote.jl/dev/#Explicit-and-Implicit-Parameters-1">described here</a> in its documentation. </p><article class="docstring"><header><a class="docstring-binding" href="#Zygote.gradient" id="Zygote.gradient"><code>Zygote.gradient</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gradient(() -&gt; loss(), ps::Params) -&gt; Grads</code></pre><p>Gradient with implicit parameters. Takes a zero-argument function, and returns a dictionary-like container, whose keys are arrays <code>x in ps</code>.</p><pre><code class="language-julia-repl hljs">julia&gt; x = [1 2 3; 4 5 6]; y = [7, 8]; z = [1, 10, 100];

julia&gt; g = gradient(Params([x, y])) do
         sum(x .* y .* z')
       end
Grads(...)

julia&gt; g[x]
2×3 Matrix{Float64}:
 7.0  70.0  700.0
 8.0  80.0  800.0

julia&gt; haskey(g, z)  # only x and y are parameters
false</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#Zygote.Params" id="Zygote.Params"><code>Zygote.Params</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Params([A, B])</code></pre><p>Container for implicit parameters, used when differentiating a zero-argument funtion <code>() -&gt; loss(A, B)</code> with respect to <code>A, B</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#Zygote.Grads" id="Zygote.Grads"><code>Zygote.Grads</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Grads(...)</code></pre><p>Dictionary-like container returned when taking gradients with respect to implicit parameters. For an array <code>W</code>, appearing  within <code>Params([W, A, B...])</code>, the gradient is <code>g[W]</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#Zygote.jacobian-Tuple{Any, Params}" id="Zygote.jacobian-Tuple{Any, Params}"><code>Zygote.jacobian</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">jacobian(loss, ::Params)</code></pre><p>Like <a href="#Zygote.gradient"><code>gradient</code></a> with implicit parameters, this method takes a zero-argument function and returns an <code>IdDict</code>-like object, now containing the Jacobian for each parameter.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; xs = [1 2; 3 4]; ys = [5,7,9];

julia&gt; Jxy = jacobian(() -&gt; ys[1:2] .+ sum(xs.^2), Params([xs, ys]))
Grads(...)

julia&gt; Jxy[ys]
2×3 Matrix{Int64}:
 1  0  0
 0  1  0

julia&gt; Jxy[xs]
2×4 Matrix{Int64}:
 2  6  4  8
 2  6  4  8</code></pre></div></section></article><h2 id="Explicit-style"><a class="docs-heading-anchor" href="#Explicit-style">Explicit style</a><a id="Explicit-style-1"></a><a class="docs-heading-anchor-permalink" href="#Explicit-style" title="Permalink"></a></h2><p>The other way of using Zygote, and using most other AD packages, is to explicitly provide a function and its arguments.</p><article class="docstring"><header><a class="docstring-binding" href="#Zygote.gradient-Tuple{Any, Vararg{Any, N} where N}" id="Zygote.gradient-Tuple{Any, Vararg{Any, N} where N}"><code>Zygote.gradient</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">gradient(f, args...)</code></pre><p>Returns a tuple containing <code>∂f/∂x</code> for each argument <code>x</code>, the derivative (for scalar <code>x</code>) or the gradient.</p><p><code>f(args...)</code> must be a real number, see <a href="#Zygote.jacobian-Tuple{Any, Params}"><code>jacobian</code></a> for array output.</p><p>See also <a href="#Zygote.withgradient"><code>withgradient</code></a> to keep the value <code>f(args...)</code>, and <a href="training/@ref"><code>pullback</code></a> for value and back-propagator.</p><pre><code class="language-julia-repl hljs">julia&gt; gradient(*, 2.0, 3.0, 5.0)
(15.0, 10.0, 6.0)

julia&gt; gradient(x -&gt; sum(abs2,x), [7.0, 11.0, 13.0])
([14.0, 22.0, 26.0],)

julia&gt; gradient([7, 11], 0, 1) do x, y, d
         p = size(x, d)
         sum(x.^p .+ y)
       end
([14.0, 22.0], 2.0, nothing)</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#Zygote.withgradient-Tuple{Any, Vararg{Any, N} where N}" id="Zygote.withgradient-Tuple{Any, Vararg{Any, N} where N}"><code>Zygote.withgradient</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">withgradient(f, args...)
withgradient(f, ::Params)</code></pre><p>Returns both the value of the function and the <a href="#Zygote.gradient"><code>gradient</code></a>, as a named tuple. </p><pre><code class="language-julia-repl hljs">julia&gt; y, ∇ = withgradient(/, 1, 2)
(val = 0.5, grad = (0.5, -0.25))

julia&gt; ∇ == gradient(/, 1, 2)
true</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#Zygote.jacobian-Tuple{Any, Vararg{Any, N} where N}" id="Zygote.jacobian-Tuple{Any, Vararg{Any, N} where N}"><code>Zygote.jacobian</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">jacobian(f, args...) -&gt; Tuple</code></pre><p>For each array <code>a ∈ args</code> this returns a matrix with <code>Ja[k,i] = ∂y[k]/∂a[i]</code> where <code>y = f(args...)</code> is usually a vector. Arrays of higher dimension are treated like <code>vec(a)</code>, or <code>vec(y)</code> for output.</p><p>For scalar <code>x::Number ∈ args</code>, the result is a vector <code>Jx[k] = ∂y[k]/∂x</code>, while for scalar <code>y</code> all results have just one row.</p><p>With any other argument type, no result is produced, even if <a href="#Zygote.gradient"><code>gradient</code></a> would work.</p><p>This reverse-mode Jacobian needs to evaluate the pullback once for each element of <code>y</code>. Doing so is usually only efficient when <code>length(y)</code> is small compared to <code>length(a)</code>, otherwise forward mode is likely to be better.</p><p>See also <a href="training/@ref"><code>withjacobian</code></a>, <a href="training/@ref"><code>hessian</code></a>, <a href="training/@ref"><code>hessian_reverse</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; jacobian(a -&gt; 100*a[1:3].^2, 1:7)[1]  # first index (rows) is output
3×7 Matrix{Int64}:
 200    0    0  0  0  0  0
   0  400    0  0  0  0  0
   0    0  600  0  0  0  0

julia&gt; jacobian((a,x) -&gt; a.^2 .* x, [1,2,3], 1)  # scalar argument has vector jacobian
([2 0 0; 0 4 0; 0 0 6], [1, 4, 9])

julia&gt; jacobian((a,d) -&gt; prod(a, dims=d), [1 2; 3 4; 5 6], 2)
([2 0 … 0 0; 0 4 … 3 0; 0 0 … 0 5], [0, 0, 0])</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>For arguments of any type except <code>Number</code> &amp; <code>AbstractArray</code>, the result is <code>nothing</code>.</p></div></div><pre><code class="nohighlight hljs">julia&gt; jacobian((a,s) -&gt; a.^length(s), [1,2,3], "str")
([3 0 0; 0 12 0; 0 0 27], nothing)

julia&gt; jacobian((a,t) -&gt; sum(a .* t[1]) + t[2], [1,2,3], (4,5))
([4 4 4], nothing)

julia&gt; gradient((a,t) -&gt; sum(a .* t[1]) + t[2], [1,2,3], (4,5))  # gradient undersands the tuple
([4 4 4], (6, 1))</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#Zygote.withgradient" id="Zygote.withgradient"><code>Zygote.withgradient</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">withgradient(f, args...)
withgradient(f, ::Params)</code></pre><p>Returns both the value of the function and the <a href="#Zygote.gradient"><code>gradient</code></a>, as a named tuple. </p><pre><code class="language-julia-repl hljs">julia&gt; y, ∇ = withgradient(/, 1, 2)
(val = 0.5, grad = (0.5, -0.25))

julia&gt; ∇ == gradient(/, 1, 2)
true</code></pre></div></section></article><h2 id="ChainRules"><a class="docs-heading-anchor" href="#ChainRules">ChainRules</a><a id="ChainRules-1"></a><a class="docs-heading-anchor-permalink" href="#ChainRules" title="Permalink"></a></h2><p>Sometimes it is necessary to exclude some code, or a whole function, from automatic differentiation. This can be done using <a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules</a>:</p><article class="docstring"><header><a class="docstring-binding" href="#ChainRulesCore.ignore_derivatives" id="ChainRulesCore.ignore_derivatives"><code>ChainRulesCore.ignore_derivatives</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ignore_derivatives(f::Function)</code></pre><p>Tells the AD system to ignore the gradients of the wrapped closure. The primal computation (forward pass) is executed normally.</p><pre><code class="language-julia hljs">ignore_derivatives() do
    value = rand()
    push!(collection, value)
end</code></pre><p>Using this incorrectly could lead to incorrect gradients. For example, the following function will have zero gradients with respect to its argument:</p><pre><code class="language-julia hljs">function wrong_grads(x)
    y = ones(3)
    ignore_derivatives() do
        push!(y, x)
    end
    return sum(y)
end</code></pre></div></section><section><div><pre><code class="nohighlight hljs">ignore_derivatives(x)</code></pre><p>Tells the AD system to ignore the gradients of the argument. Can be used to avoid unnecessary computation of gradients.</p><pre><code class="language-julia hljs">ignore_derivatives(x) * w</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" href="#ChainRulesCore.@non_differentiable" id="ChainRulesCore.@non_differentiable"><code>ChainRulesCore.@non_differentiable</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@non_differentiable(signature_expression)</code></pre><p>A helper to make it easier to declare that a method is not differentiable. This is a short-hand for defining an <a href="training/@ref"><code>frule</code></a> and <a href="#ChainRulesCore.rrule"><code>rrule</code></a> that return <a href="training/@ref"><code>NoTangent()</code></a> for all partials (even for the function <code>s̄elf</code>-partial itself)</p><p>Keyword arguments should not be included.</p><pre><code class="language-julia-repl hljs">julia&gt; @non_differentiable Base.:(==)(a, b)

julia&gt; _, pullback = rrule(==, 2.0, 3.0);

julia&gt; pullback(1.0)
(NoTangent(), NoTangent(), NoTangent())</code></pre><p>You can place type-constraints in the signature:</p><pre><code class="language-julia-repl hljs">julia&gt; @non_differentiable Base.length(xs::Union{Number, Array})

julia&gt; frule((ZeroTangent(), 1), length, [2.0, 3.0])
(2, NoTangent())</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This helper macro covers only the simple common cases. It does not support <code>where</code>-clauses. For these you can declare the <code>rrule</code> and <code>frule</code> directly</p></div></div></div></section></article><p>To manually supply the gradient for one function, you should define a method of <code>rrule</code>. ChainRules has <a href="https://juliadiff.org/ChainRulesCore.jl/stable/">detailed documentation</a> on how this works.</p><article class="docstring"><header><a class="docstring-binding" href="#ChainRulesCore.rrule" id="ChainRulesCore.rrule"><code>ChainRulesCore.rrule</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rrule([::RuleConfig,] f, x...)</code></pre><p>Expressing <code>x</code> as the tuple <code>(x₁, x₂, ...)</code> and the output tuple of <code>f(x...)</code> as <code>Ω</code>, return the tuple:</p><pre><code class="nohighlight hljs">(Ω, (Ω̄₁, Ω̄₂, ...) -&gt; (s̄elf, x̄₁, x̄₂, ...))</code></pre><p>Where the second return value is the the propagation rule or pullback. It takes in cotangents corresponding to the outputs (<code>x̄₁, x̄₂, ...</code>), and <code>s̄elf</code>, the internal values of the function itself (for closures)</p><p>If no method matching <code>rrule(f, xs...)</code> has been defined, then return <code>nothing</code>.</p><p>Examples:</p><p>unary input, unary output scalar function:</p><pre><code class="language-julia-repl hljs">julia&gt; x = rand();

julia&gt; sinx, sin_pullback = rrule(sin, x);

julia&gt; sinx == sin(x)
true

julia&gt; sin_pullback(1) == (NoTangent(), cos(x))
true</code></pre><p>binary input, unary output scalar function:</p><pre><code class="language-julia-repl hljs">julia&gt; x, y = rand(2);

julia&gt; hypotxy, hypot_pullback = rrule(hypot, x, y);

julia&gt; hypotxy == hypot(x, y)
true

julia&gt; hypot_pullback(1) == (NoTangent(), (x / hypot(x, y)), (y / hypot(x, y)))
true</code></pre><p>The optional <a href="training/@ref"><code>RuleConfig</code></a> option allows specifying rrules only for AD systems that support given features. If not needed, then it can be omitted and the <code>rrule</code> without it will be hit as a fallback. This is the case for most rules.</p><p>See also: <a href="training/@ref"><code>frule</code></a>, <a href="training/@ref"><code>@scalar_rule</code></a>, <a href="training/@ref"><code>RuleConfig</code></a></p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../callbacks/">« Callback Helpers 📚</a><a class="docs-footer-nextpage" href="../../gpu/">GPU Support »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 22 November 2022 05:53">Tuesday 22 November 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>