<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/'</script><script charset="utf-8" src="../../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Fitting a Line · Flux</title><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner="" src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/><link href="nothing/flux/stable/models/overview/" rel="canonical"/><link href="../../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link active nav-item" href="../../../">Flux</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Building Blocks</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a><a class="nav-link nav-item" href="../../../../functors/">Functors</a></ul></div><div class="nav-mega-column"><div class="column-header">Automatic differentiation libraries</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../zygote/">Zygote</a></ul></div><div class="nav-mega-column"><div class="column-header">Neural Network primitives</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../nnlib/">NNlib</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Training</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Data Wrangling</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mlutils/">MLUtils</a><a class="nav-link nav-item" href="../../../../onehotarrays/">OneHotArrays</a></ul></div><div class="nav-mega-column"><div class="column-header">Data Augmentation</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../dataaugmentation/">DataAugmentation</a></ul></div><div class="nav-mega-column"><div class="column-header">Datasets</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../mldatasets/">MLDatasets</a></ul></div><div class="nav-mega-column"><div class="column-header">Schedulers</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../paramschedulers/dev/">ParameterSchedulers</a></ul></div></div></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Models</button><div class="nav-dropdown-container nav-mega-dropdown-container"><div class="nav-mega-column"><div class="column-header">Computer Vision</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../metalhead/">Metalhead</a></ul></div><div class="nav-mega-column"><div class="column-header">Natural Language Processing</div><ul class="column-content"><a class="nav-link nav-item" href="../../../../transformers/">Transformers</a></ul></div></div></div><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="Flux logo" class="docs-light-only" src="../../assets/logo.png"/><img alt="Flux logo" class="docs-dark-only" src="../../assets/logo-dark.png"/></a><form action="../../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li class="is-active"><a class="tocitem" href="">Fitting a Line</a><ul class="internal"><li><a class="tocitem" href="#.-Provide-Training-and-Test-Data"><span>1. Provide Training and Test Data</span></a></li><li><a class="tocitem" href="#.-Build-a-Model-to-Make-Predictions"><span>2. Build a Model to Make Predictions</span></a></li><li><a class="tocitem" href="#.-Improve-the-Prediction"><span>3. Improve the Prediction</span></a></li><li><a class="tocitem" href="#.-Iteratively-Train-the-Model"><span>3+. Iteratively Train the Model</span></a></li><li><a class="tocitem" href="#.-Verify-the-Results"><span>4. Verify the Results</span></a></li></ul></li><li><a class="tocitem" href="../basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../../training/training/">Training</a></li><li><a class="tocitem" href="../recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../layers/">Built-in Layers</a></li><li><a class="tocitem" href="../activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../losses/">Loss Functions</a></li><li><a class="tocitem" href="../../training/reference/">Training API</a></li><li><a class="tocitem" href="../../training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../../training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../../training/zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guide</a></li><li class="is-active"><a href="">Fitting a Line</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Fitting a Line</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/models/overview.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="man-overview"><a class="docs-heading-anchor" href="#man-overview">Flux Overview: Fitting a Straight Line</a><a id="man-overview-1"></a><a class="docs-heading-anchor-permalink" href="#man-overview" title="Permalink"></a></h1><p>Flux is a pure Julia ML stack that allows you to build predictive models. Here are the steps for a typical Flux program:</p><ol><li>Provide training and test data</li><li>Build a model with configurable <em>parameters</em> to make predictions</li><li>Iteratively train the model by tweaking the parameters to improve predictions</li><li>Verify your model</li></ol><p>Under the hood, Flux uses a technique called automatic differentiation to take gradients that help improve predictions. Flux is also fully written in Julia so you can easily replace any layer of Flux with your own code to improve your understanding or satisfy special requirements.</p><p>Here's how you'd use Flux to build and train the most basic of models, step by step.</p><h3 id="A-Trivial-Prediction"><a class="docs-heading-anchor" href="#A-Trivial-Prediction">A Trivial Prediction</a><a id="A-Trivial-Prediction-1"></a><a class="docs-heading-anchor-permalink" href="#A-Trivial-Prediction" title="Permalink"></a></h3><p>This example will predict the output of the function <code>4x + 2</code>. Making such predictions is called "linear regression", and is really too simple to <em>need</em> a neural network. But it's a nice toy example.</p><p>First, import <code>Flux</code> and define the function we want to simulate:</p><pre><code class="language-julia-repl hljs">julia&gt; using Flux

julia&gt; actual(x) = 4x + 2
actual (generic function with 1 method)</code></pre><p>This example will build a model to approximate the <code>actual</code> function.</p><h2 id=".-Provide-Training-and-Test-Data"><a class="docs-heading-anchor" href="#.-Provide-Training-and-Test-Data">1. Provide Training and Test Data</a><a id=".-Provide-Training-and-Test-Data-1"></a><a class="docs-heading-anchor-permalink" href="#.-Provide-Training-and-Test-Data" title="Permalink"></a></h2><p>Use the <code>actual</code> function to build sets of data for training and verification:</p><pre><code class="language-julia-repl hljs">julia&gt; x_train, x_test = hcat(0:5...), hcat(6:10...)
([0 1 … 4 5], [6 7 … 9 10])

julia&gt; y_train, y_test = actual.(x_train), actual.(x_test)
([2 6 … 18 22], [26 30 … 38 42])</code></pre><p>Normally, your training and test data come from real world observations, but here we simulate them.</p><h2 id=".-Build-a-Model-to-Make-Predictions"><a class="docs-heading-anchor" href="#.-Build-a-Model-to-Make-Predictions">2. Build a Model to Make Predictions</a><a id=".-Build-a-Model-to-Make-Predictions-1"></a><a class="docs-heading-anchor-permalink" href="#.-Build-a-Model-to-Make-Predictions" title="Permalink"></a></h2><p>Now, build a model to make predictions with <code>1</code> input and <code>1</code> output:</p><pre><code class="language-julia-repl hljs">julia&gt; model = Dense(1 =&gt; 1)
Dense(1 =&gt; 1)       # 2 parameters

julia&gt; model.weight
1×1 Matrix{Float32}:
 0.95041317

julia&gt; model.bias
1-element Vector{Float32}:
 0.0</code></pre><p>Under the hood, a dense layer is a struct with fields <code>weight</code> and <code>bias</code>. <code>weight</code> represents a weights' matrix and <code>bias</code> represents a bias vector. There's another way to think about a model. In Flux, <em>models are conceptually predictive functions</em>: </p><pre><code class="language-julia-repl hljs">julia&gt; predict = Dense(1 =&gt; 1)
Dense(1 =&gt; 1)       # 2 parameters</code></pre><p><code>Dense(1 =&gt; 1)</code> also implements the function <code>σ(Wx+b)</code> where <code>W</code> and <code>b</code> are the weights and biases. <code>σ</code> is an activation function (more on activations later). Our model has one weight and one bias, but typical models will have many more. Think of weights and biases as knobs and levers Flux can use to tune predictions. Activation functions are transformations that tailor models to your needs. </p><p>This model will already make predictions, though not accurate ones yet:</p><pre><code class="language-julia-repl hljs">julia&gt; predict(x_train)
1×6 Matrix{Float32}:
 0.0  0.906654  1.81331  2.71996  3.62662  4.53327</code></pre><p>In order to make better predictions, you'll need to provide a <em>loss function</em> to tell Flux how to objectively <em>evaluate</em> the quality of a prediction. Loss functions compute the cumulative distance between actual values and predictions. </p><pre><code class="language-julia-repl hljs">julia&gt; using Statistics

julia&gt; loss(model, x, y) = mean(abs2.(model(x) .- y));

julia&gt; loss(predict, x_train, y_train)
122.64734f0</code></pre><p>More accurate predictions will yield a lower loss. You can write your own loss functions or rely on those already provided by Flux. This loss function is called <a href="https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-squared-error/">mean squared error</a> (and built-in as <a href="../losses/#Flux.Losses.mse"><code>mse</code></a>). Flux works by iteratively reducing the loss through <em>training</em>.</p><h2 id=".-Improve-the-Prediction"><a class="docs-heading-anchor" href="#.-Improve-the-Prediction">3. Improve the Prediction</a><a id=".-Improve-the-Prediction-1"></a><a class="docs-heading-anchor-permalink" href="#.-Improve-the-Prediction" title="Permalink"></a></h2><p>Under the hood, the Flux <a href="../../training/reference/#Flux.Optimise.train!-NTuple{4, Any}"><code>Flux.train!</code></a> function uses <em>a loss function</em> and <em>training data</em> to improve the <em>parameters</em> of your model based on a pluggable <a href="../../training/optimisers/"><code>optimiser</code></a>:</p><pre><code class="language-julia-repl hljs">julia&gt; using Flux: train!

julia&gt; opt = Descent()
Descent(0.1)

julia&gt; data = [(x_train, y_train)]
1-element Vector{Tuple{Matrix{Int64}, Matrix{Int64}}}:
 ([0 1 … 4 5], [2 6 … 18 22])</code></pre><p>Now, we have the optimiser and data we'll pass to <code>train!</code>. All that remains are the parameters of the model. Remember, each model is a Julia struct with a function and configurable parameters. Remember, the dense layer has weights and biases that depend on the dimensions of the inputs and outputs: </p><pre><code class="language-julia-repl hljs">julia&gt; predict.weight
1×1 Matrix{Float32}:
 0.9066542

julia&gt; predict.bias
1-element Vector{Float32}:
 0.0</code></pre><p>The dimensions of these model parameters depend on the number of inputs and outputs.</p><p>Flux will adjust predictions by iteratively changing these parameters according to the optimiser.</p><p>This optimiser implements the classic gradient descent strategy. Now improve the parameters of the model with a call to <a href="../../training/reference/#Flux.Optimise.train!-NTuple{4, Any}"><code>Flux.train!</code></a> like this:</p><pre><code class="language-julia-repl hljs">julia&gt; train!(loss, predict, data, opt)</code></pre><p>And check the loss:</p><pre><code class="language-julia-repl hljs">julia&gt; loss(predict, x_train, y_train)
116.38745f0</code></pre><p>It went down. Why? </p><pre><code class="language-julia-repl hljs">julia&gt; predict.weight, predict.bias
(Float32[7.246838;;], Float32[1.748103])</code></pre><p>The parameters have changed. This single step is the essence of machine learning.</p><h2 id=".-Iteratively-Train-the-Model"><a class="docs-heading-anchor" href="#.-Iteratively-Train-the-Model">3+. Iteratively Train the Model</a><a id=".-Iteratively-Train-the-Model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Iteratively-Train-the-Model" title="Permalink"></a></h2><p>In the previous section, we made a single call to <code>train!</code> which iterates over the data we passed in just once. An <em>epoch</em> refers to one pass over the dataset. Typically, we will run the training for multiple epochs to drive the loss down even further. Let's run it a few more times:</p><pre><code class="language-julia-repl hljs">julia&gt; for epoch in 1:200
         train!(loss, predict, data, opt)
       end

julia&gt; loss(predict, x_train, y_train)
0.00339581f0

julia&gt; predict.weight, predict.bias
(Float32[4.0159144;;], Float32[2.004479])</code></pre><p>After 200 training steps, the loss went down, and the parameters are getting close to those in the function the model is built to predict.</p><h2 id=".-Verify-the-Results"><a class="docs-heading-anchor" href="#.-Verify-the-Results">4. Verify the Results</a><a id=".-Verify-the-Results-1"></a><a class="docs-heading-anchor-permalink" href="#.-Verify-the-Results" title="Permalink"></a></h2><p>Now, let's verify the predictions:</p><pre><code class="language-julia-repl hljs">julia&gt; predict(x_test)
1×5 Matrix{Float32}:
 26.1121  30.13  34.1479  38.1657  42.1836

julia&gt; y_test
1×5 Matrix{Int64}:
 26  30  34  38  42</code></pre><p>The predictions are good. Here's how we got there. </p><p>First, we gathered real-world data into the variables <code>x_train</code>, <code>y_train</code>, <code>x_test</code>, and <code>y_test</code>. The <code>x_*</code> data defines inputs, and the <code>y_*</code> data defines outputs. The <code>*_train</code> data is for training the model, and the <code>*_test</code> data is for verifying the model. Our data was based on the function <code>4x + 2</code>.</p><p>Then, we built a single input, single output predictive model, <code>predict = Dense(1 =&gt; 1)</code>. The initial predictions weren't accurate, because we had not trained the model yet.</p><p>After building the model, we trained it with <code>train!(loss, predict, data, opt)</code>. The loss function is first, followed by the model itself, the training data, and the <code>Descent</code> optimiser provided by Flux. We ran the training step once, and observed that the parameters changed and the loss went down. Then, we ran the <code>train!</code> many times to finish the training process.</p><p>After we trained the model, we verified it with the test data to verify the results. </p><p>This overall flow represents how Flux works. Let's drill down a bit to understand what's going on inside the individual layers of Flux.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../quickstart/">« Quick Start</a><a class="docs-footer-nextpage" href="../basics/">Gradients and Layers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Sunday 27 August 2023 19:44">Sunday 27 August 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>